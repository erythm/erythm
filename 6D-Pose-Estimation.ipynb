{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMWItJ1CUp56"
      },
      "source": [
        "# **Configure Google Colab Settings**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "St4iy9xVUvr8",
        "outputId": "bcf72e67-38a8-4279-cdb2-71b677d19be6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Connect to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6e80OV_qUcfQ"
      },
      "outputs": [],
      "source": [
        "# Create Paths\n",
        "!mkdir /content/dataset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7kIVCfM8UwgR",
        "outputId": "89a5560b-ef4a-41c9-d4e5-72e0e42e6808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting 62142 files...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unzipping: 100%|██████████| 62142/62142 [02:38<00:00, 391.92file/s]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Extraction complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/Linemod_preprocessed.zip\"\n",
        "extract_to = \"/content/dataset/\"\n",
        "\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    # Retrieve the list of files in the archive\n",
        "    file_list = zip_ref.infolist()\n",
        "\n",
        "    print(f\"Extracting {len(file_list)} files...\\n\")\n",
        "    for file in tqdm(file_list, desc=\"Unzipping\", unit=\"file\"):\n",
        "        # Extract each file to the target directory\n",
        "        zip_ref.extract(file, extract_to)\n",
        "\n",
        "print(\"\\n Extraction complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rio2kCCbU0rl"
      },
      "source": [
        "# **Configure Github Settings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFNfdbV7BQgc"
      },
      "source": [
        "### Install Git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WskY0S65VGM9",
        "outputId": "b3ce1e85-2142-44b0-c69c-52621ef9a260"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.12).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.12).\n",
            "Calculating upgrade... Done\n",
            "The following packages have been kept back:\n",
            "  libcudnn9-cuda-12 libcudnn9-dev-cuda-12 libldap-2.5-0 libnccl-dev libnccl2\n",
            "The following packages will be upgraded:\n",
            "  base-files binutils binutils-common binutils-x86-64-linux-gnu\n",
            "  cuda-toolkit-12-config-common cuda-toolkit-config-common e2fsprogs\n",
            "  libbinutils libc-bin libcap2 libctf-nobfd0 libctf0 libext2fs2 libgnutls30\n",
            "  libpam-modules libpam-modules-bin libpam-runtime libpam0g libperl5.34\n",
            "  libseccomp2 libss2 libtasn1-6 libudev1 linux-libc-dev logsave openssl perl\n",
            "  perl-base perl-modules-5.34\n",
            "29 upgraded, 0 newly installed, 0 to remove and 5 not upgraded.\n",
            "Need to get 1,298 kB/18.9 MB of archives.\n",
            "After this operation, 133 kB of additional disk space will be used.\n",
            "Ign:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 linux-libc-dev amd64 5.15.0-139.149\n",
            "Err:1 http://security.ubuntu.com/ubuntu jammy-updates/main amd64 linux-libc-dev amd64 5.15.0-139.149\n",
            "  404  Not Found [IP: 185.125.190.81 80]\n",
            "\u001b[1;31mE: \u001b[0mFailed to fetch http://security.ubuntu.com/ubuntu/pool/main/l/linux/linux-libc-dev_5.15.0-139.149_amd64.deb  404  Not Found [IP: 185.125.190.81 80]\u001b[0m\n",
            "\u001b[1;31mE: \u001b[0mUnable to fetch some archives, maybe run apt-get update or try with --fix-missing?\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#Install/Upgrade Git\n",
        "!apt-get install git\n",
        "!apt upgrade git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIXMgN0jVTDp"
      },
      "source": [
        "### Clone project from Github\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlWFu5IEiZHW",
        "outputId": "7caac8da-8cc7-44f5-b398-865192fdbc97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '6D-pose-estimation'...\n",
            "remote: Enumerating objects: 152, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 152 (delta 5), reused 30 (delta 4), pack-reused 119 (from 2)\u001b[K\n",
            "Receiving objects: 100% (152/152), 121.78 MiB | 22.17 MiB/s, done.\n",
            "Resolving deltas: 100% (25/25), done.\n"
          ]
        }
      ],
      "source": [
        "#%cd /content\n",
        "!git clone https://github.com/mldl-team/6D-pose-estimation.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjH76VX3BTow"
      },
      "source": [
        "### Login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "o2MnT90rh5SV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8850ff7-28f7-4a04-9ac8-b76f8e1eebcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/6D-pose-estimation\n"
          ]
        }
      ],
      "source": [
        "%cd /content/6D-pose-estimation\n",
        "#Config Git Before Push\n",
        "!git config --global user.email \"erfan.alerom@gmail.com\"\n",
        "!git config --global user.name \"erythm\"\n",
        "\n",
        "#Authentication\n",
        "!git remote set-url origin https://erythm:@github.com/mldl-team/6D-pose-estimation.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/6D-pose-estimation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6PbW7yq3SDE",
        "outputId": "7d474217-9931-42dc-e9f9-573d9aea950b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/6D-pose-estimation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add ."
      ],
      "metadata": {
        "id": "4TWBYwQVzYSt"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"finish train on object 1 \""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWtkEWbYzeCP",
        "outputId": "9d776163-a149-4b9b-bb10-9a0fd649110a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[erf c972018] finish train on object 1\n",
            " 1 file changed, 0 insertions(+), 0 deletions(-)\n",
            " delete mode 100644 checkpoints/best_model_20250529_150327.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push --force origin erf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCqWmCgO1H-6",
        "outputId": "c8277d2e-d029-4eb4-bc48-d6bb7e1c226d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enumerating objects: 9, done.\n",
            "Counting objects:  11% (1/9)\rCounting objects:  22% (2/9)\rCounting objects:  33% (3/9)\rCounting objects:  44% (4/9)\rCounting objects:  55% (5/9)\rCounting objects:  66% (6/9)\rCounting objects:  77% (7/9)\rCounting objects:  88% (8/9)\rCounting objects: 100% (9/9)\rCounting objects: 100% (9/9), done.\n",
            "Delta compression using up to 8 threads\n",
            "Compressing objects: 100% (5/5), done.\n",
            "Writing objects: 100% (7/7), 557.40 MiB | 60.87 MiB/s, done.\n",
            "Total 7 (delta 2), reused 1 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (2/2), completed with 1 local object.\n",
            "remote: error: Trace: 2cd64276c052dcb217d513e5d554c30d2d9b9e693f0f17d3a1944d662114b808\n",
            "remote: error: See https://gh.io/lfs for more information.\n",
            "remote: error: File checkpoints/best_model_20250529_150327.pth is 608.38 MB; this exceeds GitHub's file size limit of 100.00 MB\n",
            "remote: error: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.\n",
            "To https://github.com/mldl-team/6D-pose-estimation.git\n",
            " ! [remote rejected] erf -> erf (pre-receive hook declined)\n",
            "error: failed to push some refs to 'https://github.com/mldl-team/6D-pose-estimation.git'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jtvb6CJ_VwP0"
      },
      "source": [
        "# **Configure Wandb Settings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mI6oDV8iV4AY",
        "outputId": "6acba1f3-5ae1-4386-d271-93c1448ebfc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.11)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.28.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "#Wandb installation\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5ZYYFDTV4q2",
        "outputId": "d4507ed9-b436-43dd-c01f-7604c088b2d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msina-ghiabi\u001b[0m (\u001b[33merythm-mldl\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "h-piQyhBV6le",
        "outputId": "4b8b84ad-a284-4ae6-dc76-af4f4caddc9b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msina-ghiabi\u001b[0m (\u001b[33merythm-mldl\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250520_215956-3i6cbp0i</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/erythm-mldl/6D/runs/3i6cbp0i' target=\"_blank\">warm-pond-15</a></strong> to <a href='https://wandb.ai/erythm-mldl/6D' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/erythm-mldl/6D' target=\"_blank\">https://wandb.ai/erythm-mldl/6D</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/erythm-mldl/6D/runs/3i6cbp0i' target=\"_blank\">https://wandb.ai/erythm-mldl/6D/runs/3i6cbp0i</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁▅▆▇▄█▅▇</td></tr><tr><td>loss</td><td>█▆▃▄▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.84825</td></tr><tr><td>loss</td><td>0.12104</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">warm-pond-15</strong> at: <a href='https://wandb.ai/erythm-mldl/6D/runs/3i6cbp0i' target=\"_blank\">https://wandb.ai/erythm-mldl/6D/runs/3i6cbp0i</a><br> View project at: <a href='https://wandb.ai/erythm-mldl/6D' target=\"_blank\">https://wandb.ai/erythm-mldl/6D</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250520_215956-3i6cbp0i/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "import wandb\n",
        "\n",
        "# Start a new wandb run to track this script.\n",
        "run = wandb.init(\n",
        "    # Set the wandb entity where your project will be logged (generally your team name).\n",
        "    entity=\"erythm-mldl\",\n",
        "    # Set the wandb project where this run will be logged.\n",
        "    project=\"6D\",\n",
        "    # Track hyperparameters and run metadata.\n",
        "    config={\n",
        "        \"learning_rate\": 0.02,\n",
        "        \"architecture\": \"CNN\",\n",
        "        \"dataset\": \"CIFAR-100\",\n",
        "        \"epochs\": 10,\n",
        "    },\n",
        ")\n",
        "\n",
        "# Simulate training.\n",
        "epochs = 10\n",
        "offset = random.random() / 5\n",
        "for epoch in range(2, epochs):\n",
        "    acc = 1 - 2**-epoch - random.random() / epoch - offset\n",
        "    loss = 2**-epoch + random.random() / epoch + offset\n",
        "\n",
        "    # Log metrics to wandb.\n",
        "    run.log({\"acc\": acc, \"loss\": loss})\n",
        "\n",
        "# Finish the run and upload any remaining data.\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja4g4pbfWAOK"
      },
      "source": [
        "# **Configure Python**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tf4W2glvWGIg",
        "outputId": "76625bc8-13a6-424d-ff04-4500ca2716fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.2.13)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.19.2\n"
          ]
        }
      ],
      "source": [
        "#Install Python\n",
        "!pip install ipython\n",
        "\n",
        "#Usage of the library is to display input or output images\n",
        "from IPython.display import Image, display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpvHIshEiF5O"
      },
      "source": [
        "# **Configure PyTorch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "GL955jklWLg6",
        "outputId": "a0a028ff-2bbe-4d76-b8c2-26b2eef0e4af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m119.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m103.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.6.0+cu124'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "#Install PyTorch\n",
        "!pip install torch\n",
        "!pip install torch torchvision opencv-python matplotlib tqdm pyyaml\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "#Usage of the library is the primary neural network computation framework.\n",
        "import torch\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI7EMHExgdOs"
      },
      "source": [
        "# **Configure YOLO**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mkcXvJosWJG7",
        "outputId": "334e3a3f-1efd-4684-9e96-969818a898f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.145-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.145-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.145 ultralytics-thop-2.0.14\n",
            "/bin/bash: -c: line 2: syntax error: unexpected end of file\n",
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "#Usage of the library is To run Object Detection & Image Classification with YOLO\n",
        "!pip install ultralytics\n",
        "\n",
        "#Check availability of ultralytics\n",
        "!ultralytics.checks()\n",
        "\n",
        "import ultralytics\n",
        "\n",
        "from ultralytics import YOLO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxJlZbFTlShT"
      },
      "source": [
        "# **Configure OpenCV**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dCEtKW0eTrST",
        "outputId": "9baa3d54-c469-4052-8abc-e36249408280"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "#Usage of the library is for real‐time image and video processing\n",
        "!pip install opencv-python\n",
        "\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zvAaOEWyWNu4",
        "outputId": "95e0bf45-855c-4158-8378-6ce125b3184f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu May 29 10:04:42 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#Check if we have an access to nvidia\n",
        "#If \"/bin/bash: line 1: nvidia-smi: command not found\" appeared, change Runtime to GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiEXNi0d1krR"
      },
      "source": [
        "# **Configure Open3D**\n",
        "### **Open3D is used for 3D geometry processing in Pose Estimation, SciPy for scientific computations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "6V3FOZ9H0pF8",
        "outputId": "39ee1529-8552-411c-f1c0-5819dd07b01e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting open3d\n",
            "  Downloading open3d-0.19.0-cp311-cp311-manylinux_2_31_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Collecting dash>=2.6.0 (from open3d)\n",
            "  Downloading dash-3.0.4-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: werkzeug>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.1.3)\n",
            "Requirement already satisfied: flask>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.1.1)\n",
            "Requirement already satisfied: nbformat>=5.7.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (5.10.4)\n",
            "Collecting configargparse (from open3d)\n",
            "  Downloading configargparse-1.7.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting ipywidgets>=8.0.4 (from open3d)\n",
            "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting addict (from open3d)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: pillow>=9.3.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (11.2.1)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.10.0)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (2.2.2)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from open3d) (6.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.11/dist-packages (from open3d) (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from open3d) (4.67.1)\n",
            "Collecting pyquaternion (from open3d)\n",
            "  Downloading pyquaternion-0.9.9-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting flask>=3.0.0 (from open3d)\n",
            "  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting werkzeug>=3.0.0 (from open3d)\n",
            "  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (5.24.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (8.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (4.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (2.32.3)\n",
            "Collecting retrying (from dash>=2.6.0->open3d)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (75.2.0)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (8.2.1)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (1.9.0)\n",
            "Collecting comm>=0.1.3 (from ipywidgets>=8.0.4->open3d)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (5.7.1)\n",
            "Collecting widgetsnbextension~=4.0.14 (from ipywidgets>=8.0.4->open3d)\n",
            "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (3.0.15)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (2.9.0.post0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d) (4.23.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d) (5.7.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->open3d) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->open3d) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21->open3d) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21->open3d) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=3.0.0->open3d) (3.0.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.25.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d) (4.3.8)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (9.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3->open3d) (1.17.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->dash>=2.6.0->open3d) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (2025.4.26)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.13)\n",
            "Downloading open3d-0.19.0-cp311-cp311-manylinux_2_31_x86_64.whl (447.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.7/447.7 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dash-3.0.4-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading configargparse-1.7.1-py3-none-any.whl (25 kB)\n",
            "Downloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n",
            "Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: addict, widgetsnbextension, werkzeug, retrying, pyquaternion, configargparse, comm, flask, ipywidgets, dash, open3d\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 3.6.10\n",
            "    Uninstalling widgetsnbextension-3.6.10:\n",
            "      Successfully uninstalled widgetsnbextension-3.6.10\n",
            "  Attempting uninstall: werkzeug\n",
            "    Found existing installation: Werkzeug 3.1.3\n",
            "    Uninstalling Werkzeug-3.1.3:\n",
            "      Successfully uninstalled Werkzeug-3.1.3\n",
            "  Attempting uninstall: flask\n",
            "    Found existing installation: Flask 3.1.1\n",
            "    Uninstalling Flask-3.1.1:\n",
            "      Successfully uninstalled Flask-3.1.1\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.7.1\n",
            "    Uninstalling ipywidgets-7.7.1:\n",
            "      Successfully uninstalled ipywidgets-7.7.1\n",
            "Successfully installed addict-2.4.0 comm-0.2.2 configargparse-1.7.1 dash-3.0.4 flask-3.0.3 ipywidgets-8.1.7 open3d-0.19.0 pyquaternion-0.9.9 retrying-1.3.4 werkzeug-3.0.6 widgetsnbextension-4.0.14\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (5.29.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.0.6)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.4.2)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (11.2.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.5.21)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (5.29.4)\n",
            "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install open3d scipy numpy\n",
        "!pip install opencv-python tensorboard pyyaml scipy scikit-image\n",
        "!pip install tensorboardX\n",
        "\n",
        "import open3d as o3d\n",
        "from scipy.spatial import distance_matrix\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcrD4hz007gZ"
      },
      "source": [
        "### **Manage file paths with os Library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OiDNB_0YWPhe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "#Examples\n",
        "#img_path = os.path.join('data', 'images', img_name)\n",
        "#images = glob.glob('data/images/*.jpg')\n",
        "#labels = glob.glob('data/labels/*.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZxMTz5eKiig"
      },
      "source": [
        "### **Progress Bar**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvswuY3cKfa_",
        "outputId": "de26b9cd-6760-4355-fce2-6a9a995270f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYVEIJsVY16K"
      },
      "source": [
        "# **Object Detection**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXrj5tdH6FsL"
      },
      "source": [
        "### **Import all the Tools**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JqLkUNE_ZGLC",
        "outputId": "9b398b85-7dc6-4f4a-c76d-b3a68973ea57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21.5M/21.5M [00:00<00:00, 140MB/s] \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "YOLO(\n",
              "  (model): DetectionModel(\n",
              "    (model): Sequential(\n",
              "      (0): Conv(\n",
              "        (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "        (act): SiLU(inplace=True)\n",
              "      )\n",
              "      (1): Conv(\n",
              "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "        (act): SiLU(inplace=True)\n",
              "      )\n",
              "      (2): C2f(\n",
              "        (cv1): Conv(\n",
              "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (cv2): Conv(\n",
              "          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (m): ModuleList(\n",
              "          (0): Bottleneck(\n",
              "            (cv1): Conv(\n",
              "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv2): Conv(\n",
              "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): Conv(\n",
              "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "        (act): SiLU(inplace=True)\n",
              "      )\n",
              "      (4): C2f(\n",
              "        (cv1): Conv(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (cv2): Conv(\n",
              "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (m): ModuleList(\n",
              "          (0-1): 2 x Bottleneck(\n",
              "            (cv1): Conv(\n",
              "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv2): Conv(\n",
              "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): Conv(\n",
              "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "        (act): SiLU(inplace=True)\n",
              "      )\n",
              "      (6): C2f(\n",
              "        (cv1): Conv(\n",
              "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (cv2): Conv(\n",
              "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (m): ModuleList(\n",
              "          (0-1): 2 x Bottleneck(\n",
              "            (cv1): Conv(\n",
              "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv2): Conv(\n",
              "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (7): Conv(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "        (act): SiLU(inplace=True)\n",
              "      )\n",
              "      (8): C2f(\n",
              "        (cv1): Conv(\n",
              "          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (cv2): Conv(\n",
              "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (m): ModuleList(\n",
              "          (0): Bottleneck(\n",
              "            (cv1): Conv(\n",
              "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv2): Conv(\n",
              "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (9): SPPF(\n",
              "        (cv1): Conv(\n",
              "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (cv2): Conv(\n",
              "          (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
              "      )\n",
              "      (10): Upsample(scale_factor=2.0, mode='nearest')\n",
              "      (11): Concat()\n",
              "      (12): C2f(\n",
              "        (cv1): Conv(\n",
              "          (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (cv2): Conv(\n",
              "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (m): ModuleList(\n",
              "          (0): Bottleneck(\n",
              "            (cv1): Conv(\n",
              "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv2): Conv(\n",
              "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (13): Upsample(scale_factor=2.0, mode='nearest')\n",
              "      (14): Concat()\n",
              "      (15): C2f(\n",
              "        (cv1): Conv(\n",
              "          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (cv2): Conv(\n",
              "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (m): ModuleList(\n",
              "          (0): Bottleneck(\n",
              "            (cv1): Conv(\n",
              "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv2): Conv(\n",
              "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (16): Conv(\n",
              "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "        (act): SiLU(inplace=True)\n",
              "      )\n",
              "      (17): Concat()\n",
              "      (18): C2f(\n",
              "        (cv1): Conv(\n",
              "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (cv2): Conv(\n",
              "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (m): ModuleList(\n",
              "          (0): Bottleneck(\n",
              "            (cv1): Conv(\n",
              "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv2): Conv(\n",
              "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (19): Conv(\n",
              "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "        (act): SiLU(inplace=True)\n",
              "      )\n",
              "      (20): Concat()\n",
              "      (21): C2f(\n",
              "        (cv1): Conv(\n",
              "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (cv2): Conv(\n",
              "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (m): ModuleList(\n",
              "          (0): Bottleneck(\n",
              "            (cv1): Conv(\n",
              "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (cv2): Conv(\n",
              "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (22): Detect(\n",
              "        (cv2): ModuleList(\n",
              "          (0): Sequential(\n",
              "            (0): Conv(\n",
              "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv(\n",
              "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (1): Sequential(\n",
              "            (0): Conv(\n",
              "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv(\n",
              "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (2): Sequential(\n",
              "            (0): Conv(\n",
              "              (conv): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv(\n",
              "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (cv3): ModuleList(\n",
              "          (0): Sequential(\n",
              "            (0): Conv(\n",
              "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv(\n",
              "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (1): Sequential(\n",
              "            (0): Conv(\n",
              "              (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv(\n",
              "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (2): Sequential(\n",
              "            (0): Conv(\n",
              "              (conv): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv(\n",
              "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (dfl): DFL(\n",
              "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "\n",
        "#Choose YOLO Model\n",
        "model = YOLO('yolov8s.pt')\n",
        "#Choose Computing Device - CUDA: Compute Unified Device Architecture\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "#Transfer Model to Computing Device\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00BexPmO6J9N"
      },
      "source": [
        "### **Make Dataset Ready**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LX6Kpu2tdb9r",
        "outputId": "5b28b097-5d56-47a3-9541-de8961c1ce99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Class distribution:\n",
            "Class  1: 1236 samples\n",
            "Class  2: 1214 samples\n",
            "Class  4: 1201 samples\n",
            "Class  5: 1196 samples\n",
            "Class  6: 1179 samples\n",
            "Class  8: 1188 samples\n",
            "Class  9: 1254 samples\n",
            "Class 10: 1253 samples\n",
            "Class 11: 1220 samples\n",
            "Class 12: 1237 samples\n",
            "Class 13: 1152 samples\n",
            "Class 14: 1227 samples\n",
            "Class 15: 1243 samples\n",
            "\n",
            "⚙️ Processing and splitting images...\n",
            "    Train Count  Validation Count\n",
            "0           988               248\n",
            "1           971               243\n",
            "2           960               241\n",
            "3           956               240\n",
            "4           943               236\n",
            "5           950               238\n",
            "6          1003               251\n",
            "7          1002               251\n",
            "8           976               244\n",
            "9           989               248\n",
            "10          921               231\n",
            "11          981               246\n",
            "12          994               249\n",
            "✅ YOLO dataset generated, split, and labeled.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import yaml\n",
        "import random\n",
        "import shutil\n",
        "from PIL import Image\n",
        "from collections import defaultdict, Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "# CONFIGURATION\n",
        "!mkdir /content/dataset/linemod/Linemod_ready\n",
        "root_dir = '/content/dataset/linemod/Linemod_preprocessed/data'\n",
        "output_base = '/content/dataset/linemod/Linemod_ready'  # Base output folder\n",
        "img_out_train = os.path.join(output_base, 'images/train')\n",
        "img_out_val = os.path.join(output_base, 'images/val')\n",
        "label_out_train = os.path.join(output_base, 'labels/train')\n",
        "label_out_val = os.path.join(output_base, 'labels/val')\n",
        "\n",
        "# Create directories if they don't exist\n",
        "for path in [img_out_train, img_out_val, label_out_train, label_out_val]:\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "# Define valid class IDs and their YOLO-mapped indices (0-based)\n",
        "existing_classes = [1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15]\n",
        "#Create {cls_id: idx}: {0:1} - {1:2} - {2:4} - {3:5} ...\n",
        "class_map = {cls_id: idx for idx, cls_id in enumerate(existing_classes)}\n",
        "\n",
        "# ================ STEP 1: GATHER SAMPLES ===================\n",
        "samples_by_class = defaultdict(list)\n",
        "for cls in existing_classes:\n",
        "    folder_path = os.path.join(root_dir, f\"{cls:02d}\")\n",
        "    rgb_folder = os.path.join(folder_path, 'rgb')\n",
        "    gt_file = os.path.join(folder_path, 'gt.yml')\n",
        "\n",
        "    if not os.path.exists(gt_file):\n",
        "        print(f\"⚠️ Class {cls:02d} missing → skipped.\")\n",
        "        continue\n",
        "\n",
        "    with open(gt_file, 'r') as f:\n",
        "        gt_data = yaml.safe_load(f)\n",
        "\n",
        "    for img_id in gt_data:\n",
        "        img_path = os.path.join(rgb_folder, f\"{int(img_id):04d}.png\")\n",
        "        if os.path.exists(img_path):\n",
        "            samples_by_class[cls].append(img_id)\n",
        "\n",
        "# Display class distribution\n",
        "print(\"🔍 Class distribution:\")\n",
        "for cls, samples in samples_by_class.items():\n",
        "    print(f\"Class {cls:>2}: {len(samples)} samples\")\n",
        "\n",
        "# =============== STEP 2: LABEL AND IMAGE PROCESSING =================\n",
        "def save_yolo_labels(sample_list, mode, cls, gt_data, rgb_folder):\n",
        "    label_dir = label_out_train if mode == 'train' else label_out_val\n",
        "    img_dir = img_out_train if mode == 'train' else img_out_val\n",
        "    label_id = class_map[cls]\n",
        "\n",
        "    for img_id in sample_list:\n",
        "        img_path = os.path.join(rgb_folder, f\"{int(img_id):04d}.png\")\n",
        "        img = Image.open(img_path)\n",
        "        w, h = img.size\n",
        "\n",
        "        bbox = gt_data[img_id][0]['obj_bb']\n",
        "        x, y, bw, bh = bbox\n",
        "        x_center = (x + bw / 2) / w\n",
        "        y_center = (y + bh / 2) / h\n",
        "        norm_bw = bw / w\n",
        "        norm_bh = bh / h\n",
        "\n",
        "        # Save label\n",
        "        label_file = f\"{cls:02d}_{int(img_id):04d}.txt\"\n",
        "        with open(os.path.join(label_dir, label_file), 'w') as f:\n",
        "            f.write(f\"{label_id} {x_center:.6f} {y_center:.6f} {norm_bw:.6f} {norm_bh:.6f}\\n\")\n",
        "\n",
        "        # Save image\n",
        "        img_out_path = os.path.join(img_dir, f\"{cls:02d}_{int(img_id):04d}.png\")\n",
        "        shutil.copy(img_path, img_out_path)\n",
        "\n",
        "# Split, save labels and copy images\n",
        "print(\"\\n Processing and splitting images...\")\n",
        "for cls, samples in samples_by_class.items():\n",
        "    random.shuffle(samples)\n",
        "    split_idx = int(0.8 * len(samples))\n",
        "    train_samples = samples[:split_idx]\n",
        "    val_samples = samples[split_idx:]\n",
        "\n",
        "    folder_path = os.path.join(root_dir, f\"{cls:02d}\")\n",
        "    rgb_folder = os.path.join(folder_path, 'rgb')\n",
        "    gt_file = os.path.join(folder_path, 'gt.yml')\n",
        "\n",
        "    with open(gt_file, 'r') as f:\n",
        "        gt_data = yaml.safe_load(f)\n",
        "\n",
        "    save_yolo_labels(train_samples, 'train', cls, gt_data, rgb_folder)\n",
        "    save_yolo_labels(val_samples, 'val', cls, gt_data, rgb_folder)\n",
        "\n",
        "# ============= STEP 3: LABEL DISTRIBUTION STATS ===============\n",
        "def count_labels(label_dir):\n",
        "    counter = Counter()\n",
        "    for filename in os.listdir(label_dir):\n",
        "        if filename.endswith('.txt'):\n",
        "            with open(os.path.join(label_dir, filename), 'r') as f:\n",
        "                for line in f:\n",
        "                    class_id = int(line.strip().split()[0])\n",
        "                    counter[class_id] += 1\n",
        "    return counter\n",
        "\n",
        "train_counts = count_labels(label_out_train)\n",
        "val_counts = count_labels(label_out_val)\n",
        "\n",
        "import pandas as pd\n",
        "df_stats = pd.DataFrame({\n",
        "    \"Train Count\": pd.Series(train_counts),\n",
        "    \"Validation Count\": pd.Series(val_counts)\n",
        "}).fillna(0).astype(int)\n",
        "\n",
        "print(df_stats)\n",
        "\n",
        "print(\" YOLO dataset generated, split, and labeled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5UQxMsZ6RT2"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raqAJg_s6Y8D"
      },
      "source": [
        "### **Train the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWdLRQ-qyHUQ"
      },
      "outputs": [],
      "source": [
        "model.train(\n",
        "    data='/content/project/6D-pose-estimation/configs/linemod_final.yaml',\n",
        "    epochs=15,\n",
        "    imgsz=640,\n",
        "    batch=8,\n",
        "    device=0,\n",
        "    patience=5,\n",
        "    weight_decay=0.0005\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXhj_4zwy4Gt"
      },
      "outputs": [],
      "source": [
        "!cp -r runs/detect/train ~/6d/yolo_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZACuf_m66sGQ"
      },
      "source": [
        "### **Save Trained Model Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YD2UGsFky-pW"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"model.pt\")\n",
        "metrics = model.val(data=\"linemod_final.yaml\", plots=True, save=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z05pObtQAvQb"
      },
      "source": [
        "# **Pose Estimation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLw9Lzbazk4D"
      },
      "source": [
        "### **Prepare File & Folders for RCVPose**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DingMrNW0bPZ"
      },
      "source": [
        "### 1 - Move Objects' Models From models Folder To Class Folders & Rename Them To mesh.ply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0n9I3pMgxT0u",
        "outputId": "8fc68d47-e8ed-4da5-b8f4-cee47b704184"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ obj_11.ply → /content/dataset/Linemod_preprocessed/data/11/mesh.ply\n",
            "✓ obj_01.ply → /content/dataset/Linemod_preprocessed/data/01/mesh.ply\n",
            "✓ obj_09.ply → /content/dataset/Linemod_preprocessed/data/09/mesh.ply\n",
            "✓ obj_02.ply → /content/dataset/Linemod_preprocessed/data/02/mesh.ply\n",
            "✓ obj_12.ply → /content/dataset/Linemod_preprocessed/data/12/mesh.ply\n",
            "✓ obj_14.ply → /content/dataset/Linemod_preprocessed/data/14/mesh.ply\n",
            "✓ obj_08.ply → /content/dataset/Linemod_preprocessed/data/08/mesh.ply\n",
            "✓ obj_06.ply → /content/dataset/Linemod_preprocessed/data/06/mesh.ply\n",
            "Skipping 07: folder not found in data/\n",
            "Skipping 03: folder not found in data/\n",
            "✓ obj_10.ply → /content/dataset/Linemod_preprocessed/data/10/mesh.ply\n",
            "✓ obj_13.ply → /content/dataset/Linemod_preprocessed/data/13/mesh.ply\n",
            "✓ obj_04.ply → /content/dataset/Linemod_preprocessed/data/04/mesh.ply\n",
            "✓ obj_05.ply → /content/dataset/Linemod_preprocessed/data/05/mesh.ply\n",
            "✓ obj_15.ply → /content/dataset/Linemod_preprocessed/data/15/mesh.ply\n",
            "\n",
            " All model files copied and renamed as mesh.ply.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "data_path = \"/content/dataset/Linemod_preprocessed/data\"\n",
        "models_path = \"/content/dataset/Linemod_preprocessed/models\"\n",
        "\n",
        "# === Process Each Model File ===\n",
        "for filename in os.listdir(models_path):\n",
        "    if not (filename.startswith(\"obj_\") and filename.endswith(\".ply\")):\n",
        "        continue\n",
        "\n",
        "    model_id = filename.split(\"_\")[1].split(\".\")[0]\n",
        "    src = os.path.join(models_path, filename)\n",
        "    dst_dir = os.path.join(data_path, model_id)\n",
        "    dst = os.path.join(dst_dir, \"mesh.ply\")\n",
        "\n",
        "    if not os.path.isdir(dst_dir):\n",
        "        print(f\"Skipping {model_id}: folder not found in data/\")\n",
        "        continue\n",
        "\n",
        "    shutil.copy2(src, dst)\n",
        "    print(f\"✓ {filename} → {dst}\")\n",
        "\n",
        "print(\"\\n All model files copied and renamed as mesh.ply.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vunah5M8cYW-"
      },
      "source": [
        "### 2 - Create Outside9.npy From Objects' Models\n",
        "A NumPy file containing a [9, 3] array of 3D keypoint coordinates sampled from the object mesh using Farthest-Point Sampling. These five points are chosen to maximally span the surface of the model. During data preprocessing, each keypoint is used to generate a per-frame radial distance map: for every pixel with valid depth, its Euclidean distance in 3D space to each keypoint is computed and stored. These distance maps serve as ground-truth supervision when training a network to predict 3D distance fields from RGB-D inputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "g0L1ky8r2svp",
        "outputId": "77ba12e9-18c7-499e-b381-f6380699ae8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Sampling keypoints for class 01\n",
            " Saved keypoints → /content/dataset/Linemod_preprocessed/data/01/Outside5.npy (shape: (5, 3))\n",
            " Verified load → shape: (5, 3)\n",
            "\n",
            " Sampling keypoints for class 02\n",
            " Saved keypoints → /content/dataset/Linemod_preprocessed/data/02/Outside5.npy (shape: (5, 3))\n",
            " Verified load → shape: (5, 3)\n",
            "\n",
            " Sampling keypoints for class 04\n",
            " Saved keypoints → /content/dataset/Linemod_preprocessed/data/04/Outside5.npy (shape: (5, 3))\n",
            " Verified load → shape: (5, 3)\n",
            "\n",
            " Sampling keypoints for class 05\n",
            " Saved keypoints → /content/dataset/Linemod_preprocessed/data/05/Outside5.npy (shape: (5, 3))\n",
            " Verified load → shape: (5, 3)\n",
            "\n",
            " Sampling keypoints for class 06\n",
            " Saved keypoints → /content/dataset/Linemod_preprocessed/data/06/Outside5.npy (shape: (5, 3))\n",
            " Verified load → shape: (5, 3)\n",
            "\n",
            " Sampling keypoints for class 08\n",
            " Saved keypoints → /content/dataset/Linemod_preprocessed/data/08/Outside5.npy (shape: (5, 3))\n",
            " Verified load → shape: (5, 3)\n",
            "\n",
            " Sampling keypoints for class 09\n",
            " Saved keypoints → /content/dataset/Linemod_preprocessed/data/09/Outside5.npy (shape: (5, 3))\n",
            " Verified load → shape: (5, 3)\n",
            "\n",
            " Sampling keypoints for class 10\n",
            " Saved keypoints → /content/dataset/Linemod_preprocessed/data/10/Outside5.npy (shape: (5, 3))\n",
            " Verified load → shape: (5, 3)\n",
            "\n",
            " Sampling keypoints for class 11\n",
            " Saved keypoints → /content/dataset/Linemod_preprocessed/data/11/Outside5.npy (shape: (5, 3))\n",
            " Verified load → shape: (5, 3)\n",
            "\n",
            " Sampling keypoints for class 12\n",
            " Saved keypoints → /content/dataset/Linemod_preprocessed/data/12/Outside5.npy (shape: (5, 3))\n",
            " Verified load → shape: (5, 3)\n",
            "\n",
            " Sampling keypoints for class 13\n",
            " Saved keypoints → /content/dataset/Linemod_preprocessed/data/13/Outside5.npy (shape: (5, 3))\n",
            " Verified load → shape: (5, 3)\n",
            "\n",
            " Sampling keypoints for class 14\n",
            " Saved keypoints → /content/dataset/Linemod_preprocessed/data/14/Outside5.npy (shape: (5, 3))\n",
            " Verified load → shape: (5, 3)\n",
            "\n",
            " Sampling keypoints for class 15\n",
            " Saved keypoints → /content/dataset/Linemod_preprocessed/data/15/Outside5.npy (shape: (5, 3))\n",
            " Verified load → shape: (5, 3)\n"
          ]
        }
      ],
      "source": [
        "import open3d as o3d\n",
        "\n",
        "def fps(points: np.ndarray, k: int, seed: int = 0) -> np.ndarray:\n",
        "    \"\"\"Farthest-Point Sampling to select k diverse keypoints from a 3D point cloud.\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    N = points.shape[0]\n",
        "    centroids = np.zeros((k,), dtype=np.int32)\n",
        "    distances = np.full((N,), np.inf)\n",
        "    farthest = np.random.randint(0, N)\n",
        "\n",
        "    for i in range(k):\n",
        "        centroids[i] = farthest\n",
        "        centroid = points[farthest]\n",
        "        dist = np.sum((points - centroid) ** 2, axis=1)\n",
        "        distances = np.minimum(distances, dist)\n",
        "        farthest = np.argmax(distances)\n",
        "\n",
        "    return points[centroids]\n",
        "\n",
        "# Paths and Classes\n",
        "base_dir = \"/content/dataset/Linemod_preprocessed/data\"\n",
        "classes = ['01','02','04','05','06','08','09','10','11','12','13','14','15']\n",
        "\n",
        "for cls in classes:\n",
        "    class_dir = os.path.join(base_dir, cls)\n",
        "    mesh_file = os.path.join(class_dir, \"mesh.ply\")\n",
        "\n",
        "    if not os.path.isfile(mesh_file):\n",
        "        print(f\" Skipping {cls} – mesh.ply not found\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n Sampling keypoints for class {cls}\")\n",
        "\n",
        "    # Load 3D mesh as point cloud\n",
        "    pcd = o3d.io.read_point_cloud(mesh_file)\n",
        "    pts = np.asarray(pcd.points)\n",
        "\n",
        "    if pts.shape[0] < 5:\n",
        "        print(f\" Not enough points in mesh ({pts.shape[0]}), skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Sample 5 diverse keypoints\n",
        "    keypoints = fps(pts, k=5, seed=42)\n",
        "\n",
        "    # Save as .npy file\n",
        "    keypoint_file = os.path.join(class_dir, \"Outside5.npy\")\n",
        "    np.save(keypoint_file, keypoints)\n",
        "    print(f\" Saved keypoints → {keypoint_file} (shape: {keypoints.shape})\")\n",
        "\n",
        "    # Quick check\n",
        "    try:\n",
        "        test = np.load(keypoint_file)\n",
        "        print(f\" Verified load → shape: {test.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\" Load failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXbUbafiiMFD"
      },
      "source": [
        "### It is only Possible to see the content of Outside9.npy using code below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrrof0ashbQ9",
        "outputId": "e6d7c5b6-91f9-458e-9439-7db7dd711cd7",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Inspecting sampled keypoints...\n",
            "\n",
            "\n",
            " Class 01 — Outside5.npy\n",
            " ├─ Shape : (5, 3)\n",
            " ├─ Dtype : float64\n",
            " ├─ Min   : -44.263\n",
            " ├─ Max   : 41.4566\n",
            " ├─ Mean  : -7.6299133333333335\n",
            " ├─ Std   : 24.804431580341088\n",
            " └─ Sample Data:\n",
            "[[-20.3664  19.3442  -7.6361]\n",
            " [ 35.6255 -16.9712 -44.263 ]\n",
            " [  7.5111  -4.4303  41.4566]\n",
            " [ -1.604  -33.9459  -5.4867]\n",
            " [-26.6571 -13.9838 -43.0416]]\n",
            "\n",
            " Class 02 — Outside5.npy\n",
            " ├─ Shape : (5, 3)\n",
            " ├─ Dtype : float64\n",
            " ├─ Min   : -107.092\n",
            " ├─ Max   : 90.5719\n",
            " ├─ Mean  : -7.4556266666666655\n",
            " ├─ Std   : 55.5565135499876\n",
            " └─ Sample Data:\n",
            "[[  12.3025  -17.8971   43.9721]\n",
            " [  90.5719   -1.9537 -107.092 ]\n",
            " [ -70.8535   22.383   -92.6023]\n",
            " [  53.2234   58.4881  -23.509 ]\n",
            " [   7.5829  -29.9273  -56.5234]]\n",
            "\n",
            " Class 04 — Outside5.npy\n",
            " ├─ Shape : (5, 3)\n",
            " ├─ Dtype : float64\n",
            " ├─ Min   : -67.9932\n",
            " ├─ Max   : 67.0034\n",
            " ├─ Mean  : -6.4787\n",
            " ├─ Std   : 41.27224723246038\n",
            " └─ Sample Data:\n",
            "[[-60.7914  62.0638 -19.3466]\n",
            " [ 67.0034 -10.3907  -2.8697]\n",
            " [-54.8219 -67.9932  19.5177]\n",
            " [-10.7869  13.9897  49.5203]\n",
            " [-19.0693 -15.506  -47.6997]]\n",
            "\n",
            " Class 05 — Outside5.npy\n",
            " ├─ Shape : (5, 3)\n",
            " ├─ Dtype : float64\n",
            " ├─ Min   : -79.5385\n",
            " ├─ Max   : 96.397\n",
            " ├─ Mean  : 1.1072000000000002\n",
            " ├─ Std   : 44.33083543431141\n",
            " └─ Sample Data:\n",
            "[[ -8.0273  55.7049 -79.5385]\n",
            " [  1.2705   6.302   96.397 ]\n",
            " [ -6.0933 -70.6847 -20.2746]\n",
            " [ 47.7824  17.2083   1.3454]\n",
            " [-48.5596  10.6845  13.091 ]]\n",
            "\n",
            " Class 06 — Outside5.npy\n",
            " ├─ Shape : (5, 3)\n",
            " ├─ Dtype : float64\n",
            " ├─ Min   : -61.5181\n",
            " ├─ Max   : 50.4696\n",
            " ├─ Mean  : -6.724186666666666\n",
            " ├─ Std   : 35.240367365506025\n",
            " └─ Sample Data:\n",
            "[[ -9.8392  50.4696  31.5456]\n",
            " [ 21.0611 -61.5181 -54.2087]\n",
            " [-27.4429  20.7841 -56.9346]\n",
            " [  1.3448 -54.5936  25.5383]\n",
            " [ 18.5875   3.4214  -9.0781]]\n",
            "\n",
            " Class 08 — Outside5.npy\n",
            " ├─ Shape : (5, 3)\n",
            " ├─ Dtype : float64\n",
            " ├─ Min   : -102.533\n",
            " ├─ Max   : 114.738\n",
            " ├─ Mean  : -0.5784866666666659\n",
            " ├─ Std   : 62.71586939188376\n",
            " └─ Sample Data:\n",
            "[[ -85.1721   27.1747   55.3571]\n",
            " [  41.817   -21.577  -102.533 ]\n",
            " [ 114.738    -3.9728   89.5442]\n",
            " [  18.887    -2.2339   10.7861]\n",
            " [ -64.6352    9.4542  -96.3116]]\n",
            "\n",
            " Class 09 — Outside5.npy\n",
            " ├─ Shape : (5, 3)\n",
            " ├─ Dtype : float64\n",
            " ├─ Min   : -40.411\n",
            " ├─ Max   : 46.728\n",
            " ├─ Mean  : -1.5423999999999998\n",
            " ├─ Std   : 29.332552360020316\n",
            " └─ Sample Data:\n",
            "[[ -2.0994  11.7574   0.0198]\n",
            " [ 46.728  -13.5908 -40.1141]\n",
            " [-35.4824 -23.6476 -38.1239]\n",
            " [ 44.0857  -6.8009  20.9951]\n",
            " [ 21.6419  31.9062 -40.411 ]]\n",
            "\n",
            " Class 10 — Outside5.npy\n",
            " ├─ Shape : (5, 3)\n",
            " ├─ Dtype : float64\n",
            " ├─ Min   : -73.4333\n",
            " ├─ Max   : 45.2791\n",
            " ├─ Mean  : -6.731120000000001\n",
            " ├─ Std   : 36.24998286131457\n",
            " └─ Sample Data:\n",
            "[[ 39.1349  11.601  -23.9408]\n",
            " [-73.4333 -30.9207  -0.2043]\n",
            " [-29.2001  45.2791  29.0616]\n",
            " [  4.209  -42.7445  30.9551]\n",
            " [-56.7928  29.7865 -33.7575]]\n",
            "\n",
            " Class 11 — Outside5.npy\n",
            " ├─ Shape : (5, 3)\n",
            " ├─ Dtype : float64\n",
            " ├─ Min   : -84.7123\n",
            " ├─ Max   : 85.9639\n",
            " ├─ Mean  : -5.987466666666667\n",
            " ├─ Std   : 40.78579532444544\n",
            " └─ Sample Data:\n",
            "[[ 15.494  -15.955   -7.3843]\n",
            " [  0.3111   5.3904  85.9639]\n",
            " [-12.4399  31.7775 -84.7123]\n",
            " [  7.3067 -37.177  -76.2814]\n",
            " [-11.4441  34.2681 -24.9297]]\n",
            "\n",
            " Class 12 — Outside5.npy\n",
            " ├─ Shape : (5, 3)\n",
            " ├─ Dtype : float64\n",
            " ├─ Min   : -43.2355\n",
            " ├─ Max   : 48.1457\n",
            " ├─ Mean  : -1.113106666666666\n",
            " ├─ Std   : 37.376659589936004\n",
            " └─ Sample Data:\n",
            "[[  1.2851 -26.6466  13.895 ]\n",
            " [ 44.2851  48.1457 -37.705 ]\n",
            " [-43.2355  29.6682 -42.005 ]\n",
            " [ 42.1351  36.1182  40.9671]\n",
            " [-42.4672 -41.2818 -39.855 ]]\n",
            "\n",
            " Class 13 — Outside5.npy\n",
            " ├─ Shape : (5, 3)\n",
            " ├─ Dtype : float64\n",
            " ├─ Min   : -127.028\n",
            " ├─ Max   : 127.495\n",
            " ├─ Mean  : -2.744773333333334\n",
            " ├─ Std   : 66.53620530312267\n",
            " └─ Sample Data:\n",
            "[[ -11.3506  -37.9226  -11.2949]\n",
            " [ 127.495    -0.2632  -67.9833]\n",
            " [-127.028    44.9615  -37.3693]\n",
            " [-107.902    -1.4071   69.7612]\n",
            " [  80.0632    0.7081   38.3604]]\n",
            "\n",
            " Class 14 — Outside5.npy\n",
            " ├─ Shape : (5, 3)\n",
            " ├─ Dtype : float64\n",
            " ├─ Min   : -101.751\n",
            " ├─ Max   : 82.3728\n",
            " ├─ Mean  : -6.0128933333333325\n",
            " ├─ Std   : 64.4755434749587\n",
            " └─ Sample Data:\n",
            "[[  43.2177   52.4112   54.0458]\n",
            " [ -99.9506  -36.8233 -101.751 ]\n",
            " [  32.046    34.081   -98.0899]\n",
            " [ -70.1905  -24.798    76.0068]\n",
            " [  82.3728  -56.4652   23.6938]]\n",
            "\n",
            " Class 15 — Outside5.npy\n",
            " ├─ Shape : (5, 3)\n",
            " ├─ Dtype : float64\n",
            " ├─ Min   : -81.7832\n",
            " ├─ Max   : 91.5867\n",
            " ├─ Mean  : -9.378386666666668\n",
            " ├─ Std   : 49.41984019317028\n",
            " └─ Sample Data:\n",
            "[[-33.3298 -51.9169 -81.7832]\n",
            " [-10.1223 -27.1229  91.5867]\n",
            " [ -3.8379  72.9849 -24.3855]\n",
            " [ 46.3641   7.2564 -79.3277]\n",
            " [ 10.0339 -60.7272   3.6516]]\n",
            "\n",
            " All keypoints verified.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Full array printing settings\n",
        "np.set_printoptions(precision=4, suppress=True, threshold=np.inf)\n",
        "\n",
        "# Base path for keypoints\n",
        "base_path = \"/content/dataset/Linemod_preprocessed/data\"\n",
        "classes = ['01','02','04','05','06','08','09','10','11','12','13','14','15']\n",
        "\n",
        "print(\" Inspecting sampled keypoints...\\n\")\n",
        "\n",
        "for cls in classes:\n",
        "    keypoint_file = os.path.join(base_path, cls, \"Outside5.npy\")\n",
        "\n",
        "    if not os.path.isfile(keypoint_file):\n",
        "        print(f\" Missing keypoints for class {cls} → {keypoint_file}\")\n",
        "        continue\n",
        "\n",
        "    keypoints = np.load(keypoint_file)\n",
        "    stats = {\n",
        "        \"shape\": keypoints.shape,\n",
        "        \"dtype\": keypoints.dtype,\n",
        "        \"min\": keypoints.min(),\n",
        "        \"max\": keypoints.max(),\n",
        "        \"mean\": keypoints.mean(),\n",
        "        \"std\": keypoints.std()\n",
        "    }\n",
        "\n",
        "    print(f\"\\n Class {cls} — {os.path.basename(keypoint_file)}\")\n",
        "    for k, v in stats.items():\n",
        "        print(f\" ├─ {k.capitalize():<6}: {v}\")\n",
        "    print(\" └─ Sample Data:\")\n",
        "    print(keypoints)\n",
        "\n",
        "print(\"\\n All keypoints verified.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDddF5Sy-XbT"
      },
      "source": [
        "### **3 - Generate Pose For Each Picture Using gt.yml**\n",
        "\n",
        "Purpose: This script generates pose files (Rotation & Translation matrices) for each image in the dataset.\n",
        "\n",
        "How it works:\n",
        "1. Reads the depth image and removes the background using the mask.\n",
        "2. Converts the depth into a point cloud (scene point cloud).\n",
        "3. Loads the 3D model (`mesh.ply`) and samples it into a point cloud.\n",
        "4. Uses ICP to align the model point cloud with the scene point cloud.\n",
        "5. Saves the resulting [R|t] matrix as a `.npy` file named `poseXXXXXX.npy`.\n",
        "\n",
        "Output:\n",
        "Each frame will have a file like `pose000123.npy` in the `pose/` folder.\n",
        "It contains a 3×4 RT matrix with:\n",
        "- R: rotation (3×3)\n",
        "- t: translation (3×1)\n",
        "\n",
        "Use in RCVPose:\n",
        "These pose files are used to:\n",
        "- Project 3D keypoints onto 2D image space\n",
        "- Generate 3D radius maps\n",
        "- Serve as ground truth during training\n",
        "\n",
        "Requirements:\n",
        "- `.dpt` depth images\n",
        "- `.png` binary masks\n",
        "- 3D model in `.ply` format\n",
        "- Camera intrinsics `K`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj1gZdE7-9j4",
        "outputId": "59f300c5-de34-4037-ce6e-fb52829f2c34",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Extracting ground-truth poses...\n",
            "\n",
            "\n",
            " Class 01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Class 01: 100%|██████████| 1236/1236 [00:00<00:00, 10495.57img/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Class 02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Class 02: 100%|██████████| 1214/1214 [00:00<00:00, 9835.02img/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Image 000000 has multiple objects — using second.\n",
            " Image 000001 has multiple objects — using second.\n",
            " Image 000002 has multiple objects — using second.\n",
            " Image 000003 has multiple objects — using second.\n",
            " Image 000004 has multiple objects — using second.\n",
            " Image 000005 has multiple objects — using second.\n",
            " Image 000006 has multiple objects — using second.\n",
            " Image 000007 has multiple objects — using second.\n",
            " Image 000008 has multiple objects — using second.\n",
            " Image 000009 has multiple objects — using second.\n",
            " Image 000010 has multiple objects — using second.\n",
            " Image 000011 has multiple objects — using second.\n",
            " Image 000012 has multiple objects — using second.\n",
            " Image 000013 has multiple objects — using second.\n",
            " Image 000014 has multiple objects — using second.\n",
            " Image 000015 has multiple objects — using second.\n",
            " Image 000016 has multiple objects — using second.\n",
            " Image 000017 has multiple objects — using second.\n",
            " Image 000018 has multiple objects — using second.\n",
            " Image 000019 has multiple objects — using second.\n",
            " Image 000020 has multiple objects — using second.\n",
            " Image 000021 has multiple objects — using second.\n",
            " Image 000022 has multiple objects — using second.\n",
            " Image 000023 has multiple objects — using second.\n",
            " Image 000024 has multiple objects — using second.\n",
            " Image 000025 has multiple objects — using second.\n",
            " Image 000026 has multiple objects — using second.\n",
            " Image 000027 has multiple objects — using second.\n",
            " Image 000028 has multiple objects — using second.\n",
            " Image 000029 has multiple objects — using second.\n",
            " Image 000030 has multiple objects — using second.\n",
            " Image 000031 has multiple objects — using second.\n",
            " Image 000032 has multiple objects — using second.\n",
            " Image 000033 has multiple objects — using second.\n",
            " Image 000034 has multiple objects — using second.\n",
            " Image 000035 has multiple objects — using second.\n",
            " Image 000036 has multiple objects — using second.\n",
            " Image 000037 has multiple objects — using second.\n",
            " Image 000038 has multiple objects — using second.\n",
            " Image 000039 has multiple objects — using second.\n",
            " Image 000040 has multiple objects — using second.\n",
            " Image 000041 has multiple objects — using second.\n",
            " Image 000042 has multiple objects — using second.\n",
            " Image 000043 has multiple objects — using second.\n",
            " Image 000044 has multiple objects — using second.\n",
            " Image 000045 has multiple objects — using second.\n",
            " Image 000046 has multiple objects — using second.\n",
            " Image 000047 has multiple objects — using second.\n",
            " Image 000048 has multiple objects — using second.\n",
            " Image 000049 has multiple objects — using second.\n",
            " Image 000050 has multiple objects — using second.\n",
            " Image 000051 has multiple objects — using second.\n",
            " Image 000052 has multiple objects — using second.\n",
            " Image 000053 has multiple objects — using second.\n",
            " Image 000054 has multiple objects — using second.\n",
            " Image 000055 has multiple objects — using second.\n",
            " Image 000056 has multiple objects — using second.\n",
            " Image 000057 has multiple objects — using second.\n",
            " Image 000058 has multiple objects — using second.\n",
            " Image 000059 has multiple objects — using second.\n",
            " Image 000060 has multiple objects — using second.\n",
            " Image 000061 has multiple objects — using second.\n",
            " Image 000062 has multiple objects — using second.\n",
            " Image 000063 has multiple objects — using second.\n",
            " Image 000064 has multiple objects — using second.\n",
            " Image 000065 has multiple objects — using second.\n",
            " Image 000066 has multiple objects — using second.\n",
            " Image 000067 has multiple objects — using second.\n",
            " Image 000068 has multiple objects — using second.\n",
            " Image 000069 has multiple objects — using second.\n",
            " Image 000070 has multiple objects — using second.\n",
            " Image 000071 has multiple objects — using second.\n",
            " Image 000072 has multiple objects — using second.\n",
            " Image 000073 has multiple objects — using second.\n",
            " Image 000074 has multiple objects — using second.\n",
            " Image 000075 has multiple objects — using second.\n",
            " Image 000076 has multiple objects — using second.\n",
            " Image 000077 has multiple objects — using second.\n",
            " Image 000078 has multiple objects — using second.\n",
            " Image 000079 has multiple objects — using second.\n",
            " Image 000080 has multiple objects — using second.\n",
            " Image 000081 has multiple objects — using second.\n",
            " Image 000082 has multiple objects — using second.\n",
            " Image 000083 has multiple objects — using second.\n",
            " Image 000084 has multiple objects — using second.\n",
            " Image 000085 has multiple objects — using second.\n",
            " Image 000086 has multiple objects — using second.\n",
            " Image 000087 has multiple objects — using second.\n",
            " Image 000088 has multiple objects — using second.\n",
            " Image 000089 has multiple objects — using second.\n",
            " Image 000090 has multiple objects — using second.\n",
            " Image 000091 has multiple objects — using second.\n",
            " Image 000092 has multiple objects — using second.\n",
            " Image 000093 has multiple objects — using second.\n",
            " Image 000094 has multiple objects — using second.\n",
            " Image 000095 has multiple objects — using second.\n",
            " Image 000096 has multiple objects — using second.\n",
            " Image 000097 has multiple objects — using second.\n",
            " Image 000098 has multiple objects — using second.\n",
            " Image 000099 has multiple objects — using second.\n",
            " Image 000100 has multiple objects — using second.\n",
            " Image 000101 has multiple objects — using second.\n",
            " Image 000102 has multiple objects — using second.\n",
            " Image 000103 has multiple objects — using second.\n",
            " Image 000104 has multiple objects — using second.\n",
            " Image 000105 has multiple objects — using second.\n",
            " Image 000106 has multiple objects — using second.\n",
            " Image 000107 has multiple objects — using second.\n",
            " Image 000108 has multiple objects — using second.\n",
            " Image 000109 has multiple objects — using second.\n",
            " Image 000110 has multiple objects — using second.\n",
            " Image 000111 has multiple objects — using second.\n",
            " Image 000112 has multiple objects — using second.\n",
            " Image 000113 has multiple objects — using second.\n",
            " Image 000114 has multiple objects — using second.\n",
            " Image 000115 has multiple objects — using second.\n",
            " Image 000116 has multiple objects — using second.\n",
            " Image 000117 has multiple objects — using second.\n",
            " Image 000118 has multiple objects — using second.\n",
            " Image 000119 has multiple objects — using second.\n",
            " Image 000120 has multiple objects — using second.\n",
            " Image 000121 has multiple objects — using second.\n",
            " Image 000122 has multiple objects — using second.\n",
            " Image 000123 has multiple objects — using second.\n",
            " Image 000124 has multiple objects — using second.\n",
            " Image 000125 has multiple objects — using second.\n",
            " Image 000126 has multiple objects — using second.\n",
            " Image 000127 has multiple objects — using second.\n",
            " Image 000128 has multiple objects — using second.\n",
            " Image 000129 has multiple objects — using second.\n",
            " Image 000130 has multiple objects — using second.\n",
            " Image 000131 has multiple objects — using second.\n",
            " Image 000132 has multiple objects — using second.\n",
            " Image 000133 has multiple objects — using second.\n",
            " Image 000134 has multiple objects — using second.\n",
            " Image 000135 has multiple objects — using second.\n",
            " Image 000136 has multiple objects — using second.\n",
            " Image 000137 has multiple objects — using second.\n",
            " Image 000138 has multiple objects — using second.\n",
            " Image 000139 has multiple objects — using second.\n",
            " Image 000140 has multiple objects — using second.\n",
            " Image 000141 has multiple objects — using second.\n",
            " Image 000142 has multiple objects — using second.\n",
            " Image 000143 has multiple objects — using second.\n",
            " Image 000144 has multiple objects — using second.\n",
            " Image 000145 has multiple objects — using second.\n",
            " Image 000146 has multiple objects — using second.\n",
            " Image 000147 has multiple objects — using second.\n",
            " Image 000148 has multiple objects — using second.\n",
            " Image 000149 has multiple objects — using second.\n",
            " Image 000150 has multiple objects — using second.\n",
            " Image 000151 has multiple objects — using second.\n",
            " Image 000152 has multiple objects — using second.\n",
            " Image 000153 has multiple objects — using second.\n",
            " Image 000154 has multiple objects — using second.\n",
            " Image 000155 has multiple objects — using second.\n",
            " Image 000156 has multiple objects — using second.\n",
            " Image 000157 has multiple objects — using second.\n",
            " Image 000158 has multiple objects — using second.\n",
            " Image 000159 has multiple objects — using second.\n",
            " Image 000160 has multiple objects — using second.\n",
            " Image 000161 has multiple objects — using second.\n",
            " Image 000162 has multiple objects — using second.\n",
            " Image 000163 has multiple objects — using second.\n",
            " Image 000164 has multiple objects — using second.\n",
            " Image 000165 has multiple objects — using second.\n",
            " Image 000166 has multiple objects — using second.\n",
            " Image 000167 has multiple objects — using second.\n",
            " Image 000168 has multiple objects — using second.\n",
            " Image 000169 has multiple objects — using second.\n",
            " Image 000170 has multiple objects — using second.\n",
            " Image 000171 has multiple objects — using second.\n",
            " Image 000172 has multiple objects — using second.\n",
            " Image 000173 has multiple objects — using second.\n",
            " Image 000174 has multiple objects — using second.\n",
            " Image 000175 has multiple objects — using second.\n",
            " Image 000176 has multiple objects — using second.\n",
            " Image 000177 has multiple objects — using second.\n",
            " Image 000178 has multiple objects — using second.\n",
            " Image 000179 has multiple objects — using second.\n",
            " Image 000180 has multiple objects — using second.\n",
            " Image 000181 has multiple objects — using second.\n",
            " Image 000182 has multiple objects — using second.\n",
            " Image 000183 has multiple objects — using second.\n",
            " Image 000184 has multiple objects — using second.\n",
            " Image 000185 has multiple objects — using second.\n",
            " Image 000186 has multiple objects — using second.\n",
            " Image 000187 has multiple objects — using second.\n",
            " Image 000188 has multiple objects — using second.\n",
            " Image 000189 has multiple objects — using second.\n",
            " Image 000190 has multiple objects — using second.\n",
            " Image 000191 has multiple objects — using second.\n",
            " Image 000192 has multiple objects — using second.\n",
            " Image 000193 has multiple objects — using second.\n",
            " Image 000194 has multiple objects — using second.\n",
            " Image 000195 has multiple objects — using second.\n",
            " Image 000196 has multiple objects — using second.\n",
            " Image 000197 has multiple objects — using second.\n",
            " Image 000198 has multiple objects — using second.\n",
            " Image 000199 has multiple objects — using second.\n",
            " Image 000200 has multiple objects — using second.\n",
            " Image 000201 has multiple objects — using second.\n",
            " Image 000202 has multiple objects — using second.\n",
            " Image 000203 has multiple objects — using second.\n",
            " Image 000204 has multiple objects — using second.\n",
            " Image 000205 has multiple objects — using second.\n",
            " Image 000206 has multiple objects — using second.\n",
            " Image 000207 has multiple objects — using second.\n",
            " Image 000208 has multiple objects — using second.\n",
            " Image 000209 has multiple objects — using second.\n",
            " Image 000210 has multiple objects — using second.\n",
            " Image 000211 has multiple objects — using second.\n",
            " Image 000212 has multiple objects — using second.\n",
            " Image 000213 has multiple objects — using second.\n",
            " Image 000214 has multiple objects — using second.\n",
            " Image 000215 has multiple objects — using second.\n",
            " Image 000216 has multiple objects — using second.\n",
            " Image 000217 has multiple objects — using second.\n",
            " Image 000218 has multiple objects — using second.\n",
            " Image 000219 has multiple objects — using second.\n",
            " Image 000220 has multiple objects — using second.\n",
            " Image 000221 has multiple objects — using second.\n",
            " Image 000222 has multiple objects — using second.\n",
            " Image 000223 has multiple objects — using second.\n",
            " Image 000224 has multiple objects — using second.\n",
            " Image 000225 has multiple objects — using second.\n",
            " Image 000226 has multiple objects — using second.\n",
            " Image 000227 has multiple objects — using second.\n",
            " Image 000228 has multiple objects — using second.\n",
            " Image 000229 has multiple objects — using second.\n",
            " Image 000230 has multiple objects — using second.\n",
            " Image 000231 has multiple objects — using second.\n",
            " Image 000232 has multiple objects — using second.\n",
            " Image 000233 has multiple objects — using second.\n",
            " Image 000234 has multiple objects — using second.\n",
            " Image 000235 has multiple objects — using second.\n",
            " Image 000236 has multiple objects — using second.\n",
            " Image 000237 has multiple objects — using second.\n",
            " Image 000238 has multiple objects — using second.\n",
            " Image 000239 has multiple objects — using second.\n",
            " Image 000240 has multiple objects — using second.\n",
            " Image 000241 has multiple objects — using second.\n",
            " Image 000242 has multiple objects — using second.\n",
            " Image 000243 has multiple objects — using second.\n",
            " Image 000244 has multiple objects — using second.\n",
            " Image 000245 has multiple objects — using second.\n",
            " Image 000246 has multiple objects — using second.\n",
            " Image 000247 has multiple objects — using second.\n",
            " Image 000248 has multiple objects — using second.\n",
            " Image 000249 has multiple objects — using second.\n",
            " Image 000250 has multiple objects — using second.\n",
            " Image 000251 has multiple objects — using second.\n",
            " Image 000252 has multiple objects — using second.\n",
            " Image 000253 has multiple objects — using second.\n",
            " Image 000254 has multiple objects — using second.\n",
            " Image 000255 has multiple objects — using second.\n",
            " Image 000256 has multiple objects — using second.\n",
            " Image 000257 has multiple objects — using second.\n",
            " Image 000258 has multiple objects — using second.\n",
            " Image 000259 has multiple objects — using second.\n",
            " Image 000260 has multiple objects — using second.\n",
            " Image 000261 has multiple objects — using second.\n",
            " Image 000262 has multiple objects — using second.\n",
            " Image 000263 has multiple objects — using second.\n",
            " Image 000264 has multiple objects — using second.\n",
            " Image 000265 has multiple objects — using second.\n",
            " Image 000266 has multiple objects — using second.\n",
            " Image 000267 has multiple objects — using second.\n",
            " Image 000268 has multiple objects — using second.\n",
            " Image 000269 has multiple objects — using second.\n",
            " Image 000270 has multiple objects — using second.\n",
            " Image 000271 has multiple objects — using second.\n",
            " Image 000272 has multiple objects — using second.\n",
            " Image 000273 has multiple objects — using second.\n",
            " Image 000274 has multiple objects — using second.\n",
            " Image 000275 has multiple objects — using second.\n",
            " Image 000276 has multiple objects — using second.\n",
            " Image 000277 has multiple objects — using second.\n",
            " Image 000278 has multiple objects — using second.\n",
            " Image 000279 has multiple objects — using second.\n",
            " Image 000280 has multiple objects — using second.\n",
            " Image 000281 has multiple objects — using second.\n",
            " Image 000282 has multiple objects — using second.\n",
            " Image 000283 has multiple objects — using second.\n",
            " Image 000284 has multiple objects — using second.\n",
            " Image 000285 has multiple objects — using second.\n",
            " Image 000286 has multiple objects — using second.\n",
            " Image 000287 has multiple objects — using second.\n",
            " Image 000288 has multiple objects — using second.\n",
            " Image 000289 has multiple objects — using second.\n",
            " Image 000290 has multiple objects — using second.\n",
            " Image 000291 has multiple objects — using second.\n",
            " Image 000292 has multiple objects — using second.\n",
            " Image 000293 has multiple objects — using second.\n",
            " Image 000294 has multiple objects — using second.\n",
            " Image 000295 has multiple objects — using second.\n",
            " Image 000296 has multiple objects — using second.\n",
            " Image 000297 has multiple objects — using second.\n",
            " Image 000298 has multiple objects — using second.\n",
            " Image 000299 has multiple objects — using second.\n",
            " Image 000300 has multiple objects — using second.\n",
            " Image 000301 has multiple objects — using second.\n",
            " Image 000302 has multiple objects — using second.\n",
            " Image 000303 has multiple objects — using second.\n",
            " Image 000304 has multiple objects — using second.\n",
            " Image 000305 has multiple objects — using second.\n",
            " Image 000306 has multiple objects — using second.\n",
            " Image 000307 has multiple objects — using second.\n",
            " Image 000308 has multiple objects — using second.\n",
            " Image 000309 has multiple objects — using second.\n",
            " Image 000310 has multiple objects — using second.\n",
            " Image 000311 has multiple objects — using second.\n",
            " Image 000312 has multiple objects — using second.\n",
            " Image 000313 has multiple objects — using second.\n",
            " Image 000314 has multiple objects — using second.\n",
            " Image 000315 has multiple objects — using second.\n",
            " Image 000316 has multiple objects — using second.\n",
            " Image 000317 has multiple objects — using second.\n",
            " Image 000318 has multiple objects — using second.\n",
            " Image 000319 has multiple objects — using second.\n",
            " Image 000320 has multiple objects — using second.\n",
            " Image 000321 has multiple objects — using second.\n",
            " Image 000322 has multiple objects — using second.\n",
            " Image 000323 has multiple objects — using second.\n",
            " Image 000324 has multiple objects — using second.\n",
            " Image 000325 has multiple objects — using second.\n",
            " Image 000326 has multiple objects — using second.\n",
            " Image 000327 has multiple objects — using second.\n",
            " Image 000328 has multiple objects — using second.\n",
            " Image 000329 has multiple objects — using second.\n",
            " Image 000330 has multiple objects — using second.\n",
            " Image 000331 has multiple objects — using second.\n",
            " Image 000332 has multiple objects — using second.\n",
            " Image 000333 has multiple objects — using second.\n",
            " Image 000334 has multiple objects — using second.\n",
            " Image 000335 has multiple objects — using second.\n",
            " Image 000336 has multiple objects — using second.\n",
            " Image 000337 has multiple objects — using second.\n",
            " Image 000338 has multiple objects — using second.\n",
            " Image 000339 has multiple objects — using second.\n",
            " Image 000340 has multiple objects — using second.\n",
            " Image 000341 has multiple objects — using second.\n",
            " Image 000342 has multiple objects — using second.\n",
            " Image 000343 has multiple objects — using second.\n",
            " Image 000344 has multiple objects — using second.\n",
            " Image 000345 has multiple objects — using second.\n",
            " Image 000346 has multiple objects — using second.\n",
            " Image 000347 has multiple objects — using second.\n",
            " Image 000348 has multiple objects — using second.\n",
            " Image 000349 has multiple objects — using second.\n",
            " Image 000350 has multiple objects — using second.\n",
            " Image 000351 has multiple objects — using second.\n",
            " Image 000352 has multiple objects — using second.\n",
            " Image 000353 has multiple objects — using second.\n",
            " Image 000354 has multiple objects — using second.\n",
            " Image 000355 has multiple objects — using second.\n",
            " Image 000356 has multiple objects — using second.\n",
            " Image 000357 has multiple objects — using second.\n",
            " Image 000358 has multiple objects — using second.\n",
            " Image 000359 has multiple objects — using second.\n",
            " Image 000360 has multiple objects — using second.\n",
            " Image 000361 has multiple objects — using second.\n",
            " Image 000362 has multiple objects — using second.\n",
            " Image 000363 has multiple objects — using second.\n",
            " Image 000364 has multiple objects — using second.\n",
            " Image 000365 has multiple objects — using second.\n",
            " Image 000366 has multiple objects — using second.\n",
            " Image 000367 has multiple objects — using second.\n",
            " Image 000368 has multiple objects — using second.\n",
            " Image 000369 has multiple objects — using second.\n",
            " Image 000370 has multiple objects — using second.\n",
            " Image 000371 has multiple objects — using second.\n",
            " Image 000372 has multiple objects — using second.\n",
            " Image 000373 has multiple objects — using second.\n",
            " Image 000374 has multiple objects — using second.\n",
            " Image 000375 has multiple objects — using second.\n",
            " Image 000376 has multiple objects — using second.\n",
            " Image 000377 has multiple objects — using second.\n",
            " Image 000378 has multiple objects — using second.\n",
            " Image 000379 has multiple objects — using second.\n",
            " Image 000380 has multiple objects — using second.\n",
            " Image 000381 has multiple objects — using second.\n",
            " Image 000382 has multiple objects — using second.\n",
            " Image 000383 has multiple objects — using second.\n",
            " Image 000384 has multiple objects — using second.\n",
            " Image 000385 has multiple objects — using second.\n",
            " Image 000386 has multiple objects — using second.\n",
            " Image 000387 has multiple objects — using second.\n",
            " Image 000388 has multiple objects — using second.\n",
            " Image 000389 has multiple objects — using second.\n",
            " Image 000390 has multiple objects — using second.\n",
            " Image 000391 has multiple objects — using second.\n",
            " Image 000392 has multiple objects — using second.\n",
            " Image 000393 has multiple objects — using second.\n",
            " Image 000394 has multiple objects — using second.\n",
            " Image 000395 has multiple objects — using second.\n",
            " Image 000396 has multiple objects — using second.\n",
            " Image 000397 has multiple objects — using second.\n",
            " Image 000398 has multiple objects — using second.\n",
            " Image 000399 has multiple objects — using second.\n",
            " Image 000400 has multiple objects — using second.\n",
            " Image 000401 has multiple objects — using second.\n",
            " Image 000402 has multiple objects — using second.\n",
            " Image 000403 has multiple objects — using second.\n",
            " Image 000404 has multiple objects — using second.\n",
            " Image 000405 has multiple objects — using second.\n",
            " Image 000406 has multiple objects — using second.\n",
            " Image 000407 has multiple objects — using second.\n",
            " Image 000408 has multiple objects — using second.\n",
            " Image 000409 has multiple objects — using second.\n",
            " Image 000410 has multiple objects — using second.\n",
            " Image 000411 has multiple objects — using second.\n",
            " Image 000412 has multiple objects — using second.\n",
            " Image 000413 has multiple objects — using second.\n",
            " Image 000414 has multiple objects — using second.\n",
            " Image 000415 has multiple objects — using second.\n",
            " Image 000416 has multiple objects — using second.\n",
            " Image 000417 has multiple objects — using second.\n",
            " Image 000418 has multiple objects — using second.\n",
            " Image 000419 has multiple objects — using second.\n",
            " Image 000420 has multiple objects — using second.\n",
            " Image 000421 has multiple objects — using second.\n",
            " Image 000422 has multiple objects — using second.\n",
            " Image 000423 has multiple objects — using second.\n",
            " Image 000424 has multiple objects — using second.\n",
            " Image 000425 has multiple objects — using second.\n",
            " Image 000426 has multiple objects — using second.\n",
            " Image 000427 has multiple objects — using second.\n",
            " Image 000428 has multiple objects — using second.\n",
            " Image 000429 has multiple objects — using second.\n",
            " Image 000430 has multiple objects — using second.\n",
            " Image 000431 has multiple objects — using second.\n",
            " Image 000432 has multiple objects — using second.\n",
            " Image 000433 has multiple objects — using second.\n",
            " Image 000434 has multiple objects — using second.\n",
            " Image 000435 has multiple objects — using second.\n",
            " Image 000436 has multiple objects — using second.\n",
            " Image 000437 has multiple objects — using second.\n",
            " Image 000438 has multiple objects — using second.\n",
            " Image 000439 has multiple objects — using second.\n",
            " Image 000440 has multiple objects — using second.\n",
            " Image 000441 has multiple objects — using second.\n",
            " Image 000442 has multiple objects — using second.\n",
            " Image 000443 has multiple objects — using second.\n",
            " Image 000444 has multiple objects — using second.\n",
            " Image 000445 has multiple objects — using second.\n",
            " Image 000446 has multiple objects — using second.\n",
            " Image 000447 has multiple objects — using second.\n",
            " Image 000448 has multiple objects — using second.\n",
            " Image 000449 has multiple objects — using second.\n",
            " Image 000450 has multiple objects — using second.\n",
            " Image 000451 has multiple objects — using second.\n",
            " Image 000452 has multiple objects — using second.\n",
            " Image 000453 has multiple objects — using second.\n",
            " Image 000454 has multiple objects — using second.\n",
            " Image 000455 has multiple objects — using second.\n",
            " Image 000456 has multiple objects — using second.\n",
            " Image 000457 has multiple objects — using second.\n",
            " Image 000458 has multiple objects — using second.\n",
            " Image 000459 has multiple objects — using second.\n",
            " Image 000460 has multiple objects — using second.\n",
            " Image 000461 has multiple objects — using second.\n",
            " Image 000462 has multiple objects — using second.\n",
            " Image 000463 has multiple objects — using second.\n",
            " Image 000464 has multiple objects — using second.\n",
            " Image 000465 has multiple objects — using second.\n",
            " Image 000466 has multiple objects — using second.\n",
            " Image 000467 has multiple objects — using second.\n",
            " Image 000468 has multiple objects — using second.\n",
            " Image 000469 has multiple objects — using second.\n",
            " Image 000470 has multiple objects — using second.\n",
            " Image 000471 has multiple objects — using second.\n",
            " Image 000472 has multiple objects — using second.\n",
            " Image 000473 has multiple objects — using second.\n",
            " Image 000474 has multiple objects — using second.\n",
            " Image 000475 has multiple objects — using second.\n",
            " Image 000476 has multiple objects — using second.\n",
            " Image 000477 has multiple objects — using second.\n",
            " Image 000478 has multiple objects — using second.\n",
            " Image 000479 has multiple objects — using second.\n",
            " Image 000480 has multiple objects — using second.\n",
            " Image 000481 has multiple objects — using second.\n",
            " Image 000482 has multiple objects — using second.\n",
            " Image 000483 has multiple objects — using second.\n",
            " Image 000484 has multiple objects — using second.\n",
            " Image 000485 has multiple objects — using second.\n",
            " Image 000486 has multiple objects — using second.\n",
            " Image 000487 has multiple objects — using second.\n",
            " Image 000488 has multiple objects — using second.\n",
            " Image 000489 has multiple objects — using second.\n",
            " Image 000490 has multiple objects — using second.\n",
            " Image 000491 has multiple objects — using second.\n",
            " Image 000492 has multiple objects — using second.\n",
            " Image 000493 has multiple objects — using second.\n",
            " Image 000494 has multiple objects — using second.\n",
            " Image 000495 has multiple objects — using second.\n",
            " Image 000496 has multiple objects — using second.\n",
            " Image 000497 has multiple objects — using second.\n",
            " Image 000498 has multiple objects — using second.\n",
            " Image 000499 has multiple objects — using second.\n",
            " Image 000500 has multiple objects — using second.\n",
            " Image 000501 has multiple objects — using second.\n",
            " Image 000502 has multiple objects — using second.\n",
            " Image 000503 has multiple objects — using second.\n",
            " Image 000504 has multiple objects — using second.\n",
            " Image 000505 has multiple objects — using second.\n",
            " Image 000506 has multiple objects — using second.\n",
            " Image 000507 has multiple objects — using second.\n",
            " Image 000508 has multiple objects — using second.\n",
            " Image 000509 has multiple objects — using second.\n",
            " Image 000510 has multiple objects — using second.\n",
            " Image 000511 has multiple objects — using second.\n",
            " Image 000512 has multiple objects — using second.\n",
            " Image 000513 has multiple objects — using second.\n",
            " Image 000514 has multiple objects — using second.\n",
            " Image 000515 has multiple objects — using second.\n",
            " Image 000516 has multiple objects — using second.\n",
            " Image 000517 has multiple objects — using second.\n",
            " Image 000518 has multiple objects — using second.\n",
            " Image 000519 has multiple objects — using second.\n",
            " Image 000520 has multiple objects — using second.\n",
            " Image 000521 has multiple objects — using second.\n",
            " Image 000522 has multiple objects — using second.\n",
            " Image 000523 has multiple objects — using second.\n",
            " Image 000524 has multiple objects — using second.\n",
            " Image 000525 has multiple objects — using second.\n",
            " Image 000526 has multiple objects — using second.\n",
            " Image 000527 has multiple objects — using second.\n",
            " Image 000528 has multiple objects — using second.\n",
            " Image 000529 has multiple objects — using second.\n",
            " Image 000530 has multiple objects — using second.\n",
            " Image 000531 has multiple objects — using second.\n",
            " Image 000532 has multiple objects — using second.\n",
            " Image 000533 has multiple objects — using second.\n",
            " Image 000534 has multiple objects — using second.\n",
            " Image 000535 has multiple objects — using second.\n",
            " Image 000536 has multiple objects — using second.\n",
            " Image 000537 has multiple objects — using second.\n",
            " Image 000538 has multiple objects — using second.\n",
            " Image 000539 has multiple objects — using second.\n",
            " Image 000540 has multiple objects — using second.\n",
            " Image 000541 has multiple objects — using second.\n",
            " Image 000542 has multiple objects — using second.\n",
            " Image 000543 has multiple objects — using second.\n",
            " Image 000544 has multiple objects — using second.\n",
            " Image 000545 has multiple objects — using second.\n",
            " Image 000546 has multiple objects — using second.\n",
            " Image 000547 has multiple objects — using second.\n",
            " Image 000548 has multiple objects — using second.\n",
            " Image 000549 has multiple objects — using second.\n",
            " Image 000550 has multiple objects — using second.\n",
            " Image 000551 has multiple objects — using second.\n",
            " Image 000552 has multiple objects — using second.\n",
            " Image 000553 has multiple objects — using second.\n",
            " Image 000554 has multiple objects — using second.\n",
            " Image 000555 has multiple objects — using second.\n",
            " Image 000556 has multiple objects — using second.\n",
            " Image 000557 has multiple objects — using second.\n",
            " Image 000558 has multiple objects — using second.\n",
            " Image 000559 has multiple objects — using second.\n",
            " Image 000560 has multiple objects — using second.\n",
            " Image 000561 has multiple objects — using second.\n",
            " Image 000562 has multiple objects — using second.\n",
            " Image 000563 has multiple objects — using second.\n",
            " Image 000564 has multiple objects — using second.\n",
            " Image 000565 has multiple objects — using second.\n",
            " Image 000566 has multiple objects — using second.\n",
            " Image 000567 has multiple objects — using second.\n",
            " Image 000568 has multiple objects — using second.\n",
            " Image 000569 has multiple objects — using second.\n",
            " Image 000570 has multiple objects — using second.\n",
            " Image 000571 has multiple objects — using second.\n",
            " Image 000572 has multiple objects — using second.\n",
            " Image 000573 has multiple objects — using second.\n",
            " Image 000574 has multiple objects — using second.\n",
            " Image 000575 has multiple objects — using second.\n",
            " Image 000576 has multiple objects — using second.\n",
            " Image 000577 has multiple objects — using second.\n",
            " Image 000578 has multiple objects — using second.\n",
            " Image 000579 has multiple objects — using second.\n",
            " Image 000580 has multiple objects — using second.\n",
            " Image 000581 has multiple objects — using second.\n",
            " Image 000582 has multiple objects — using second.\n",
            " Image 000583 has multiple objects — using second.\n",
            " Image 000584 has multiple objects — using second.\n",
            " Image 000585 has multiple objects — using second.\n",
            " Image 000586 has multiple objects — using second.\n",
            " Image 000587 has multiple objects — using second.\n",
            " Image 000588 has multiple objects — using second.\n",
            " Image 000589 has multiple objects — using second.\n",
            " Image 000590 has multiple objects — using second.\n",
            " Image 000591 has multiple objects — using second.\n",
            " Image 000592 has multiple objects — using second.\n",
            " Image 000593 has multiple objects — using second.\n",
            " Image 000594 has multiple objects — using second.\n",
            " Image 000595 has multiple objects — using second.\n",
            " Image 000596 has multiple objects — using second.\n",
            " Image 000597 has multiple objects — using second.\n",
            " Image 000598 has multiple objects — using second.\n",
            " Image 000599 has multiple objects — using second.\n",
            " Image 000600 has multiple objects — using second.\n",
            " Image 000601 has multiple objects — using second.\n",
            " Image 000602 has multiple objects — using second.\n",
            " Image 000603 has multiple objects — using second.\n",
            " Image 000604 has multiple objects — using second.\n",
            " Image 000605 has multiple objects — using second.\n",
            " Image 000606 has multiple objects — using second.\n",
            " Image 000607 has multiple objects — using second.\n",
            " Image 000608 has multiple objects — using second.\n",
            " Image 000609 has multiple objects — using second.\n",
            " Image 000610 has multiple objects — using second.\n",
            " Image 000611 has multiple objects — using second.\n",
            " Image 000612 has multiple objects — using second.\n",
            " Image 000613 has multiple objects — using second.\n",
            " Image 000614 has multiple objects — using second.\n",
            " Image 000615 has multiple objects — using second.\n",
            " Image 000616 has multiple objects — using second.\n",
            " Image 000617 has multiple objects — using second.\n",
            " Image 000618 has multiple objects — using second.\n",
            " Image 000619 has multiple objects — using second.\n",
            " Image 000620 has multiple objects — using second.\n",
            " Image 000621 has multiple objects — using second.\n",
            " Image 000622 has multiple objects — using second.\n",
            " Image 000623 has multiple objects — using second.\n",
            " Image 000624 has multiple objects — using second.\n",
            " Image 000625 has multiple objects — using second.\n",
            " Image 000626 has multiple objects — using second.\n",
            " Image 000627 has multiple objects — using second.\n",
            " Image 000628 has multiple objects — using second.\n",
            " Image 000629 has multiple objects — using second.\n",
            " Image 000630 has multiple objects — using second.\n",
            " Image 000631 has multiple objects — using second.\n",
            " Image 000632 has multiple objects — using second.\n",
            " Image 000633 has multiple objects — using second.\n",
            " Image 000634 has multiple objects — using second.\n",
            " Image 000635 has multiple objects — using second.\n",
            " Image 000636 has multiple objects — using second.\n",
            " Image 000637 has multiple objects — using second.\n",
            " Image 000638 has multiple objects — using second.\n",
            " Image 000639 has multiple objects — using second.\n",
            " Image 000640 has multiple objects — using second.\n",
            " Image 000641 has multiple objects — using second.\n",
            " Image 000642 has multiple objects — using second.\n",
            " Image 000643 has multiple objects — using second.\n",
            " Image 000644 has multiple objects — using second.\n",
            " Image 000645 has multiple objects — using second.\n",
            " Image 000646 has multiple objects — using second.\n",
            " Image 000647 has multiple objects — using second.\n",
            " Image 000648 has multiple objects — using second.\n",
            " Image 000649 has multiple objects — using second.\n",
            " Image 000650 has multiple objects — using second.\n",
            " Image 000651 has multiple objects — using second.\n",
            " Image 000652 has multiple objects — using second.\n",
            " Image 000653 has multiple objects — using second.\n",
            " Image 000654 has multiple objects — using second.\n",
            " Image 000655 has multiple objects — using second.\n",
            " Image 000656 has multiple objects — using second.\n",
            " Image 000657 has multiple objects — using second.\n",
            " Image 000658 has multiple objects — using second.\n",
            " Image 000659 has multiple objects — using second.\n",
            " Image 000660 has multiple objects — using second.\n",
            " Image 000661 has multiple objects — using second.\n",
            " Image 000662 has multiple objects — using second.\n",
            " Image 000663 has multiple objects — using second.\n",
            " Image 000664 has multiple objects — using second.\n",
            " Image 000665 has multiple objects — using second.\n",
            " Image 000666 has multiple objects — using second.\n",
            " Image 000667 has multiple objects — using second.\n",
            " Image 000668 has multiple objects — using second.\n",
            " Image 000669 has multiple objects — using second.\n",
            " Image 000670 has multiple objects — using second.\n",
            " Image 000671 has multiple objects — using second.\n",
            " Image 000672 has multiple objects — using second.\n",
            " Image 000673 has multiple objects — using second.\n",
            " Image 000674 has multiple objects — using second.\n",
            " Image 000675 has multiple objects — using second.\n",
            " Image 000676 has multiple objects — using second.\n",
            " Image 000677 has multiple objects — using second.\n",
            " Image 000678 has multiple objects — using second.\n",
            " Image 000679 has multiple objects — using second.\n",
            " Image 000680 has multiple objects — using second.\n",
            " Image 000681 has multiple objects — using second.\n",
            " Image 000682 has multiple objects — using second.\n",
            " Image 000683 has multiple objects — using second.\n",
            " Image 000684 has multiple objects — using second.\n",
            " Image 000685 has multiple objects — using second.\n",
            " Image 000686 has multiple objects — using second.\n",
            " Image 000687 has multiple objects — using second.\n",
            " Image 000688 has multiple objects — using second.\n",
            " Image 000689 has multiple objects — using second.\n",
            " Image 000690 has multiple objects — using second.\n",
            " Image 000691 has multiple objects — using second.\n",
            " Image 000692 has multiple objects — using second.\n",
            " Image 000693 has multiple objects — using second.\n",
            " Image 000694 has multiple objects — using second.\n",
            " Image 000695 has multiple objects — using second.\n",
            " Image 000696 has multiple objects — using second.\n",
            " Image 000697 has multiple objects — using second.\n",
            " Image 000698 has multiple objects — using second.\n",
            " Image 000699 has multiple objects — using second.\n",
            " Image 000700 has multiple objects — using second.\n",
            " Image 000701 has multiple objects — using second.\n",
            " Image 000702 has multiple objects — using second.\n",
            " Image 000703 has multiple objects — using second.\n",
            " Image 000704 has multiple objects — using second.\n",
            " Image 000705 has multiple objects — using second.\n",
            " Image 000706 has multiple objects — using second.\n",
            " Image 000707 has multiple objects — using second.\n",
            " Image 000708 has multiple objects — using second.\n",
            " Image 000709 has multiple objects — using second.\n",
            " Image 000710 has multiple objects — using second.\n",
            " Image 000711 has multiple objects — using second.\n",
            " Image 000712 has multiple objects — using second.\n",
            " Image 000713 has multiple objects — using second.\n",
            " Image 000714 has multiple objects — using second.\n",
            " Image 000715 has multiple objects — using second.\n",
            " Image 000716 has multiple objects — using second.\n",
            " Image 000717 has multiple objects — using second.\n",
            " Image 000718 has multiple objects — using second.\n",
            " Image 000719 has multiple objects — using second.\n",
            " Image 000720 has multiple objects — using second.\n",
            " Image 000721 has multiple objects — using second.\n",
            " Image 000722 has multiple objects — using second.\n",
            " Image 000723 has multiple objects — using second.\n",
            " Image 000724 has multiple objects — using second.\n",
            " Image 000725 has multiple objects — using second.\n",
            " Image 000726 has multiple objects — using second.\n",
            " Image 000727 has multiple objects — using second.\n",
            " Image 000728 has multiple objects — using second.\n",
            " Image 000729 has multiple objects — using second.\n",
            " Image 000730 has multiple objects — using second.\n",
            " Image 000731 has multiple objects — using second.\n",
            " Image 000732 has multiple objects — using second.\n",
            " Image 000733 has multiple objects — using second.\n",
            " Image 000734 has multiple objects — using second.\n",
            " Image 000735 has multiple objects — using second.\n",
            " Image 000736 has multiple objects — using second.\n",
            " Image 000737 has multiple objects — using second.\n",
            " Image 000738 has multiple objects — using second.\n",
            " Image 000739 has multiple objects — using second.\n",
            " Image 000740 has multiple objects — using second.\n",
            " Image 000741 has multiple objects — using second.\n",
            " Image 000742 has multiple objects — using second.\n",
            " Image 000743 has multiple objects — using second.\n",
            " Image 000744 has multiple objects — using second.\n",
            " Image 000745 has multiple objects — using second.\n",
            " Image 000746 has multiple objects — using second.\n",
            " Image 000747 has multiple objects — using second.\n",
            " Image 000748 has multiple objects — using second.\n",
            " Image 000749 has multiple objects — using second.\n",
            " Image 000750 has multiple objects — using second.\n",
            " Image 000751 has multiple objects — using second.\n",
            " Image 000752 has multiple objects — using second.\n",
            " Image 000753 has multiple objects — using second.\n",
            " Image 000754 has multiple objects — using second.\n",
            " Image 000755 has multiple objects — using second.\n",
            " Image 000756 has multiple objects — using second.\n",
            " Image 000757 has multiple objects — using second.\n",
            " Image 000758 has multiple objects — using second.\n",
            " Image 000759 has multiple objects — using second.\n",
            " Image 000760 has multiple objects — using second.\n",
            " Image 000761 has multiple objects — using second.\n",
            " Image 000762 has multiple objects — using second.\n",
            " Image 000763 has multiple objects — using second.\n",
            " Image 000764 has multiple objects — using second.\n",
            " Image 000765 has multiple objects — using second.\n",
            " Image 000766 has multiple objects — using second.\n",
            " Image 000767 has multiple objects — using second.\n",
            " Image 000768 has multiple objects — using second.\n",
            " Image 000769 has multiple objects — using second.\n",
            " Image 000770 has multiple objects — using second.\n",
            " Image 000771 has multiple objects — using second.\n",
            " Image 000772 has multiple objects — using second.\n",
            " Image 000773 has multiple objects — using second.\n",
            " Image 000774 has multiple objects — using second.\n",
            " Image 000775 has multiple objects — using second.\n",
            " Image 000776 has multiple objects — using second.\n",
            " Image 000777 has multiple objects — using second.\n",
            " Image 000778 has multiple objects — using second.\n",
            " Image 000779 has multiple objects — using second.\n",
            " Image 000780 has multiple objects — using second.\n",
            " Image 000781 has multiple objects — using second.\n",
            " Image 000782 has multiple objects — using second.\n",
            " Image 000783 has multiple objects — using second.\n",
            " Image 000784 has multiple objects — using second.\n",
            " Image 000785 has multiple objects — using second.\n",
            " Image 000786 has multiple objects — using second.\n",
            " Image 000787 has multiple objects — using second.\n",
            " Image 000788 has multiple objects — using second.\n",
            " Image 000789 has multiple objects — using second.\n",
            " Image 000790 has multiple objects — using second.\n",
            " Image 000791 has multiple objects — using second.\n",
            " Image 000792 has multiple objects — using second.\n",
            " Image 000793 has multiple objects — using second.\n",
            " Image 000794 has multiple objects — using second.\n",
            " Image 000795 has multiple objects — using second.\n",
            " Image 000796 has multiple objects — using second.\n",
            " Image 000797 has multiple objects — using second.\n",
            " Image 000798 has multiple objects — using second.\n",
            " Image 000799 has multiple objects — using second.\n",
            " Image 000800 has multiple objects — using second.\n",
            " Image 000801 has multiple objects — using second.\n",
            " Image 000802 has multiple objects — using second.\n",
            " Image 000803 has multiple objects — using second.\n",
            " Image 000804 has multiple objects — using second.\n",
            " Image 000805 has multiple objects — using second.\n",
            " Image 000806 has multiple objects — using second.\n",
            " Image 000807 has multiple objects — using second.\n",
            " Image 000808 has multiple objects — using second.\n",
            " Image 000809 has multiple objects — using second.\n",
            " Image 000810 has multiple objects — using second.\n",
            " Image 000811 has multiple objects — using second.\n",
            " Image 000812 has multiple objects — using second.\n",
            " Image 000813 has multiple objects — using second.\n",
            " Image 000814 has multiple objects — using second.\n",
            " Image 000815 has multiple objects — using second.\n",
            " Image 000816 has multiple objects — using second.\n",
            " Image 000817 has multiple objects — using second.\n",
            " Image 000818 has multiple objects — using second.\n",
            " Image 000819 has multiple objects — using second.\n",
            " Image 000820 has multiple objects — using second.\n",
            " Image 000821 has multiple objects — using second.\n",
            " Image 000822 has multiple objects — using second.\n",
            " Image 000823 has multiple objects — using second.\n",
            " Image 000824 has multiple objects — using second.\n",
            " Image 000825 has multiple objects — using second.\n",
            " Image 000826 has multiple objects — using second.\n",
            " Image 000827 has multiple objects — using second.\n",
            " Image 000828 has multiple objects — using second.\n",
            " Image 000829 has multiple objects — using second.\n",
            " Image 000830 has multiple objects — using second.\n",
            " Image 000831 has multiple objects — using second.\n",
            " Image 000832 has multiple objects — using second.\n",
            " Image 000833 has multiple objects — using second.\n",
            " Image 000834 has multiple objects — using second.\n",
            " Image 000835 has multiple objects — using second.\n",
            " Image 000836 has multiple objects — using second.\n",
            " Image 000837 has multiple objects — using second.\n",
            " Image 000838 has multiple objects — using second.\n",
            " Image 000839 has multiple objects — using second.\n",
            " Image 000840 has multiple objects — using second.\n",
            " Image 000841 has multiple objects — using second.\n",
            " Image 000842 has multiple objects — using second.\n",
            " Image 000843 has multiple objects — using second.\n",
            " Image 000844 has multiple objects — using second.\n",
            " Image 000845 has multiple objects — using second.\n",
            " Image 000846 has multiple objects — using second.\n",
            " Image 000847 has multiple objects — using second.\n",
            " Image 000848 has multiple objects — using second.\n",
            " Image 000849 has multiple objects — using second.\n",
            " Image 000850 has multiple objects — using second.\n",
            " Image 000851 has multiple objects — using second.\n",
            " Image 000852 has multiple objects — using second.\n",
            " Image 000853 has multiple objects — using second.\n",
            " Image 000854 has multiple objects — using second.\n",
            " Image 000855 has multiple objects — using second.\n",
            " Image 000856 has multiple objects — using second.\n",
            " Image 000857 has multiple objects — using second.\n",
            " Image 000858 has multiple objects — using second.\n",
            " Image 000859 has multiple objects — using second.\n",
            " Image 000860 has multiple objects — using second.\n",
            " Image 000861 has multiple objects — using second.\n",
            " Image 000862 has multiple objects — using second.\n",
            " Image 000863 has multiple objects — using second.\n",
            " Image 000864 has multiple objects — using second.\n",
            " Image 000865 has multiple objects — using second.\n",
            " Image 000866 has multiple objects — using second.\n",
            " Image 000867 has multiple objects — using second.\n",
            " Image 000868 has multiple objects — using second.\n",
            " Image 000869 has multiple objects — using second.\n",
            " Image 000870 has multiple objects — using second.\n",
            " Image 000871 has multiple objects — using second.\n",
            " Image 000872 has multiple objects — using second.\n",
            " Image 000873 has multiple objects — using second.\n",
            " Image 000874 has multiple objects — using second.\n",
            " Image 000875 has multiple objects — using second.\n",
            " Image 000876 has multiple objects — using second.\n",
            " Image 000877 has multiple objects — using second.\n",
            " Image 000878 has multiple objects — using second.\n",
            " Image 000879 has multiple objects — using second.\n",
            " Image 000880 has multiple objects — using second.\n",
            " Image 000881 has multiple objects — using second.\n",
            " Image 000882 has multiple objects — using second.\n",
            " Image 000883 has multiple objects — using second.\n",
            " Image 000884 has multiple objects — using second.\n",
            " Image 000885 has multiple objects — using second.\n",
            " Image 000886 has multiple objects — using second.\n",
            " Image 000887 has multiple objects — using second.\n",
            " Image 000888 has multiple objects — using second.\n",
            " Image 000889 has multiple objects — using second.\n",
            " Image 000890 has multiple objects — using second.\n",
            " Image 000891 has multiple objects — using second.\n",
            " Image 000892 has multiple objects — using second.\n",
            " Image 000893 has multiple objects — using second.\n",
            " Image 000894 has multiple objects — using second.\n",
            " Image 000895 has multiple objects — using second.\n",
            " Image 000896 has multiple objects — using second.\n",
            " Image 000897 has multiple objects — using second.\n",
            " Image 000898 has multiple objects — using second.\n",
            " Image 000899 has multiple objects — using second.\n",
            " Image 000900 has multiple objects — using second.\n",
            " Image 000901 has multiple objects — using second.\n",
            " Image 000902 has multiple objects — using second.\n",
            " Image 000903 has multiple objects — using second.\n",
            " Image 000904 has multiple objects — using second.\n",
            " Image 000905 has multiple objects — using second.\n",
            " Image 000906 has multiple objects — using second.\n",
            " Image 000907 has multiple objects — using second.\n",
            " Image 000908 has multiple objects — using second.\n",
            " Image 000909 has multiple objects — using second.\n",
            " Image 000910 has multiple objects — using second.\n",
            " Image 000911 has multiple objects — using second.\n",
            " Image 000912 has multiple objects — using second.\n",
            " Image 000913 has multiple objects — using second.\n",
            " Image 000914 has multiple objects — using second.\n",
            " Image 000915 has multiple objects — using second.\n",
            " Image 000916 has multiple objects — using second.\n",
            " Image 000917 has multiple objects — using second.\n",
            " Image 000918 has multiple objects — using second.\n",
            " Image 000919 has multiple objects — using second.\n",
            " Image 000920 has multiple objects — using second.\n",
            " Image 000921 has multiple objects — using second.\n",
            " Image 000922 has multiple objects — using second.\n",
            " Image 000923 has multiple objects — using second.\n",
            " Image 000924 has multiple objects — using second.\n",
            " Image 000925 has multiple objects — using second.\n",
            " Image 000926 has multiple objects — using second.\n",
            " Image 000927 has multiple objects — using second.\n",
            " Image 000928 has multiple objects — using second.\n",
            " Image 000929 has multiple objects — using second.\n",
            " Image 000930 has multiple objects — using second.\n",
            " Image 000931 has multiple objects — using second.\n",
            " Image 000932 has multiple objects — using second.\n",
            " Image 000933 has multiple objects — using second.\n",
            " Image 000934 has multiple objects — using second.\n",
            " Image 000935 has multiple objects — using second.\n",
            " Image 000936 has multiple objects — using second.\n",
            " Image 000937 has multiple objects — using second.\n",
            " Image 000938 has multiple objects — using second.\n",
            " Image 000939 has multiple objects — using second.\n",
            " Image 000940 has multiple objects — using second.\n",
            " Image 000941 has multiple objects — using second.\n",
            " Image 000942 has multiple objects — using second.\n",
            " Image 000943 has multiple objects — using second.\n",
            " Image 000944 has multiple objects — using second.\n",
            " Image 000945 has multiple objects — using second.\n",
            " Image 000946 has multiple objects — using second.\n",
            " Image 000947 has multiple objects — using second.\n",
            " Image 000948 has multiple objects — using second.\n",
            " Image 000949 has multiple objects — using second.\n",
            " Image 000950 has multiple objects — using second.\n",
            " Image 000951 has multiple objects — using second.\n",
            " Image 000952 has multiple objects — using second.\n",
            " Image 000953 has multiple objects — using second.\n",
            " Image 000954 has multiple objects — using second.\n",
            " Image 000955 has multiple objects — using second.\n",
            " Image 000956 has multiple objects — using second.\n",
            " Image 000957 has multiple objects — using second.\n",
            " Image 000958 has multiple objects — using second.\n",
            " Image 000959 has multiple objects — using second.\n",
            " Image 000960 has multiple objects — using second.\n",
            " Image 000961 has multiple objects — using second.\n",
            " Image 000962 has multiple objects — using second.\n",
            " Image 000963 has multiple objects — using second.\n",
            " Image 000964 has multiple objects — using second.\n",
            " Image 000965 has multiple objects — using second.\n",
            " Image 000966 has multiple objects — using second.\n",
            " Image 000967 has multiple objects — using second.\n",
            " Image 000968 has multiple objects — using second.\n",
            " Image 000969 has multiple objects — using second.\n",
            " Image 000970 has multiple objects — using second.\n",
            " Image 000971 has multiple objects — using second.\n",
            " Image 000972 has multiple objects — using second.\n",
            " Image 000973 has multiple objects — using second.\n",
            " Image 000974 has multiple objects — using second.\n",
            " Image 000975 has multiple objects — using second.\n",
            " Image 000976 has multiple objects — using second.\n",
            " Image 000977 has multiple objects — using second.\n",
            " Image 000978 has multiple objects — using second.\n",
            " Image 000979 has multiple objects — using second.\n",
            " Image 000980 has multiple objects — using second.\n",
            " Image 000981 has multiple objects — using second.\n",
            " Image 000982 has multiple objects — using second.\n",
            " Image 000983 has multiple objects — using second.\n",
            " Image 000984 has multiple objects — using second.\n",
            " Image 000985 has multiple objects — using second.\n",
            " Image 000986 has multiple objects — using second.\n",
            " Image 000987 has multiple objects — using second.\n",
            " Image 000988 has multiple objects — using second.\n",
            " Image 000989 has multiple objects — using second.\n",
            " Image 000990 has multiple objects — using second.\n",
            " Image 000991 has multiple objects — using second.\n",
            " Image 000992 has multiple objects — using second.\n",
            " Image 000993 has multiple objects — using second.\n",
            " Image 000994 has multiple objects — using second.\n",
            " Image 000995 has multiple objects — using second.\n",
            " Image 000996 has multiple objects — using second.\n",
            " Image 000997 has multiple objects — using second.\n",
            " Image 000998 has multiple objects — using second.\n",
            " Image 000999 has multiple objects — using second.\n",
            " Image 001000 has multiple objects — using second.\n",
            " Image 001001 has multiple objects — using second.\n",
            " Image 001002 has multiple objects — using second.\n",
            " Image 001003 has multiple objects — using second.\n",
            " Image 001004 has multiple objects — using second.\n",
            " Image 001005 has multiple objects — using second.\n",
            " Image 001006 has multiple objects — using second.\n",
            " Image 001007 has multiple objects — using second.\n",
            " Image 001008 has multiple objects — using second.\n",
            " Image 001009 has multiple objects — using second.\n",
            " Image 001010 has multiple objects — using second.\n",
            " Image 001011 has multiple objects — using second.\n",
            " Image 001012 has multiple objects — using second.\n",
            " Image 001013 has multiple objects — using second.\n",
            " Image 001014 has multiple objects — using second.\n",
            " Image 001015 has multiple objects — using second.\n",
            " Image 001016 has multiple objects — using second.\n",
            " Image 001017 has multiple objects — using second.\n",
            " Image 001018 has multiple objects — using second.\n",
            " Image 001019 has multiple objects — using second.\n",
            " Image 001020 has multiple objects — using second.\n",
            " Image 001021 has multiple objects — using second.\n",
            " Image 001022 has multiple objects — using second.\n",
            " Image 001023 has multiple objects — using second.\n",
            " Image 001024 has multiple objects — using second.\n",
            " Image 001025 has multiple objects — using second.\n",
            " Image 001026 has multiple objects — using second.\n",
            " Image 001027 has multiple objects — using second.\n",
            " Image 001028 has multiple objects — using second.\n",
            " Image 001029 has multiple objects — using second.\n",
            " Image 001030 has multiple objects — using second.\n",
            " Image 001031 has multiple objects — using second.\n",
            " Image 001032 has multiple objects — using second.\n",
            " Image 001033 has multiple objects — using second.\n",
            " Image 001034 has multiple objects — using second.\n",
            " Image 001035 has multiple objects — using second.\n",
            " Image 001036 has multiple objects — using second.\n",
            " Image 001037 has multiple objects — using second.\n",
            " Image 001038 has multiple objects — using second.\n",
            " Image 001039 has multiple objects — using second.\n",
            " Image 001040 has multiple objects — using second.\n",
            " Image 001041 has multiple objects — using second.\n",
            " Image 001042 has multiple objects — using second.\n",
            " Image 001043 has multiple objects — using second.\n",
            " Image 001044 has multiple objects — using second.\n",
            " Image 001045 has multiple objects — using second.\n",
            " Image 001046 has multiple objects — using second.\n",
            " Image 001047 has multiple objects — using second.\n",
            " Image 001048 has multiple objects — using second.\n",
            " Image 001049 has multiple objects — using second.\n",
            " Image 001050 has multiple objects — using second.\n",
            " Image 001051 has multiple objects — using second.\n",
            " Image 001052 has multiple objects — using second.\n",
            " Image 001053 has multiple objects — using second.\n",
            " Image 001054 has multiple objects — using second.\n",
            " Image 001055 has multiple objects — using second.\n",
            " Image 001056 has multiple objects — using second.\n",
            " Image 001057 has multiple objects — using second.\n",
            " Image 001058 has multiple objects — using second.\n",
            " Image 001059 has multiple objects — using second.\n",
            " Image 001060 has multiple objects — using second.\n",
            " Image 001061 has multiple objects — using second.\n",
            " Image 001062 has multiple objects — using second.\n",
            " Image 001063 has multiple objects — using second.\n",
            " Image 001064 has multiple objects — using second.\n",
            " Image 001065 has multiple objects — using second.\n",
            " Image 001066 has multiple objects — using second.\n",
            " Image 001067 has multiple objects — using second.\n",
            " Image 001068 has multiple objects — using second.\n",
            " Image 001069 has multiple objects — using second.\n",
            " Image 001070 has multiple objects — using second.\n",
            " Image 001071 has multiple objects — using second.\n",
            " Image 001072 has multiple objects — using second.\n",
            " Image 001073 has multiple objects — using second.\n",
            " Image 001074 has multiple objects — using second.\n",
            " Image 001075 has multiple objects — using second.\n",
            " Image 001076 has multiple objects — using second.\n",
            " Image 001077 has multiple objects — using second.\n",
            " Image 001078 has multiple objects — using second.\n",
            " Image 001079 has multiple objects — using second.\n",
            " Image 001080 has multiple objects — using second.\n",
            " Image 001081 has multiple objects — using second.\n",
            " Image 001082 has multiple objects — using second.\n",
            " Image 001083 has multiple objects — using second.\n",
            " Image 001084 has multiple objects — using second.\n",
            " Image 001085 has multiple objects — using second.\n",
            " Image 001086 has multiple objects — using second.\n",
            " Image 001087 has multiple objects — using second.\n",
            " Image 001088 has multiple objects — using second.\n",
            " Image 001089 has multiple objects — using second.\n",
            " Image 001090 has multiple objects — using second.\n",
            " Image 001091 has multiple objects — using second.\n",
            " Image 001092 has multiple objects — using second.\n",
            " Image 001093 has multiple objects — using second.\n",
            " Image 001094 has multiple objects — using second.\n",
            " Image 001095 has multiple objects — using second.\n",
            " Image 001096 has multiple objects — using second.\n",
            " Image 001097 has multiple objects — using second.\n",
            " Image 001098 has multiple objects — using second.\n",
            " Image 001099 has multiple objects — using second.\n",
            " Image 001100 has multiple objects — using second.\n",
            " Image 001101 has multiple objects — using second.\n",
            " Image 001102 has multiple objects — using second.\n",
            " Image 001103 has multiple objects — using second.\n",
            " Image 001104 has multiple objects — using second.\n",
            " Image 001105 has multiple objects — using second.\n",
            " Image 001106 has multiple objects — using second.\n",
            " Image 001107 has multiple objects — using second.\n",
            " Image 001108 has multiple objects — using second.\n",
            " Image 001109 has multiple objects — using second.\n",
            " Image 001110 has multiple objects — using second.\n",
            " Image 001111 has multiple objects — using second.\n",
            " Image 001112 has multiple objects — using second.\n",
            " Image 001113 has multiple objects — using second.\n",
            " Image 001114 has multiple objects — using second.\n",
            " Image 001115 has multiple objects — using second.\n",
            " Image 001116 has multiple objects — using second.\n",
            " Image 001117 has multiple objects — using second.\n",
            " Image 001118 has multiple objects — using second.\n",
            " Image 001119 has multiple objects — using second.\n",
            " Image 001120 has multiple objects — using second.\n",
            " Image 001121 has multiple objects — using second.\n",
            " Image 001122 has multiple objects — using second.\n",
            " Image 001123 has multiple objects — using second.\n",
            " Image 001124 has multiple objects — using second.\n",
            " Image 001125 has multiple objects — using second.\n",
            " Image 001126 has multiple objects — using second.\n",
            " Image 001127 has multiple objects — using second.\n",
            " Image 001128 has multiple objects — using second.\n",
            " Image 001129 has multiple objects — using second.\n",
            " Image 001130 has multiple objects — using second.\n",
            " Image 001131 has multiple objects — using second.\n",
            " Image 001132 has multiple objects — using second.\n",
            " Image 001133 has multiple objects — using second.\n",
            " Image 001134 has multiple objects — using second.\n",
            " Image 001135 has multiple objects — using second.\n",
            " Image 001136 has multiple objects — using second.\n",
            " Image 001137 has multiple objects — using second.\n",
            " Image 001138 has multiple objects — using second.\n",
            " Image 001139 has multiple objects — using second.\n",
            " Image 001140 has multiple objects — using second.\n",
            " Image 001141 has multiple objects — using second.\n",
            " Image 001142 has multiple objects — using second.\n",
            " Image 001143 has multiple objects — using second.\n",
            " Image 001144 has multiple objects — using second.\n",
            " Image 001145 has multiple objects — using second.\n",
            " Image 001146 has multiple objects — using second.\n",
            " Image 001147 has multiple objects — using second.\n",
            " Image 001148 has multiple objects — using second.\n",
            " Image 001149 has multiple objects — using second.\n",
            " Image 001150 has multiple objects — using second.\n",
            " Image 001151 has multiple objects — using second.\n",
            " Image 001152 has multiple objects — using second.\n",
            " Image 001153 has multiple objects — using second.\n",
            " Image 001154 has multiple objects — using second.\n",
            " Image 001155 has multiple objects — using second.\n",
            " Image 001156 has multiple objects — using second.\n",
            " Image 001157 has multiple objects — using second.\n",
            " Image 001158 has multiple objects — using second.\n",
            " Image 001159 has multiple objects — using second.\n",
            " Image 001160 has multiple objects — using second.\n",
            " Image 001161 has multiple objects — using second.\n",
            " Image 001162 has multiple objects — using second.\n",
            " Image 001163 has multiple objects — using second.\n",
            " Image 001164 has multiple objects — using second.\n",
            " Image 001165 has multiple objects — using second.\n",
            " Image 001166 has multiple objects — using second.\n",
            " Image 001167 has multiple objects — using second.\n",
            " Image 001168 has multiple objects — using second.\n",
            " Image 001169 has multiple objects — using second.\n",
            " Image 001170 has multiple objects — using second.\n",
            " Image 001171 has multiple objects — using second.\n",
            " Image 001172 has multiple objects — using second.\n",
            " Image 001173 has multiple objects — using second.\n",
            " Image 001174 has multiple objects — using second.\n",
            " Image 001175 has multiple objects — using second.\n",
            " Image 001176 has multiple objects — using second.\n",
            " Image 001177 has multiple objects — using second.\n",
            " Image 001178 has multiple objects — using second.\n",
            " Image 001179 has multiple objects — using second.\n",
            " Image 001180 has multiple objects — using second.\n",
            " Image 001181 has multiple objects — using second.\n",
            " Image 001182 has multiple objects — using second.\n",
            " Image 001183 has multiple objects — using second.\n",
            " Image 001184 has multiple objects — using second.\n",
            " Image 001185 has multiple objects — using second.\n",
            " Image 001186 has multiple objects — using second.\n",
            " Image 001187 has multiple objects — using second.\n",
            " Image 001188 has multiple objects — using second.\n",
            " Image 001189 has multiple objects — using second.\n",
            " Image 001190 has multiple objects — using second.\n",
            " Image 001191 has multiple objects — using second.\n",
            " Image 001192 has multiple objects — using second.\n",
            " Image 001193 has multiple objects — using second.\n",
            " Image 001194 has multiple objects — using second.\n",
            " Image 001195 has multiple objects — using second.\n",
            " Image 001196 has multiple objects — using second.\n",
            " Image 001197 has multiple objects — using second.\n",
            " Image 001198 has multiple objects — using second.\n",
            " Image 001199 has multiple objects — using second.\n",
            " Image 001200 has multiple objects — using second.\n",
            " Image 001201 has multiple objects — using second.\n",
            " Image 001202 has multiple objects — using second.\n",
            " Image 001203 has multiple objects — using second.\n",
            " Image 001204 has multiple objects — using second.\n",
            " Image 001205 has multiple objects — using second.\n",
            " Image 001206 has multiple objects — using second.\n",
            " Image 001207 has multiple objects — using second.\n",
            " Image 001208 has multiple objects — using second.\n",
            " Image 001209 has multiple objects — using second.\n",
            " Image 001210 has multiple objects — using second.\n",
            " Image 001211 has multiple objects — using second.\n",
            " Image 001212 has multiple objects — using second.\n",
            " Image 001213 has multiple objects — using second.\n",
            "\n",
            " Class 04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Class 04: 100%|██████████| 1201/1201 [00:00<00:00, 11681.62img/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Class 05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Class 05: 100%|██████████| 1196/1196 [00:00<00:00, 11431.75img/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Class 06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Class 06: 100%|██████████| 1179/1179 [00:00<00:00, 11022.22img/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Class 08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Class 08: 100%|██████████| 1188/1188 [00:00<00:00, 11509.29img/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Class 09\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Class 09: 100%|██████████| 1254/1254 [00:00<00:00, 11651.49img/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Class 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Class 10: 100%|██████████| 1253/1253 [00:00<00:00, 11445.45img/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Class 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Class 11: 100%|██████████| 1220/1220 [00:00<00:00, 11800.45img/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Class 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Class 12: 100%|██████████| 1237/1237 [00:00<00:00, 11402.55img/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Class 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Class 13: 100%|██████████| 1152/1152 [00:00<00:00, 11908.62img/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Class 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Class 14: 100%|██████████| 1227/1227 [00:00<00:00, 11575.95img/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Class 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Class 15: 100%|██████████| 1243/1243 [00:00<00:00, 11571.43img/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " All pose matrices extracted and saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import yaml\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === Base Paths ===\n",
        "base_root = \"/content/dataset/Linemod_preprocessed/data\"\n",
        "class_ids = ['01', '02', '04', '05', '06', '08', '09', '10', '11', '12', '13', '14', '15']\n",
        "\n",
        "print(\" Extracting ground-truth poses...\\n\")\n",
        "\n",
        "for class_id in class_ids:\n",
        "    print(f\"\\n Class {class_id}\")\n",
        "    class_dir = os.path.join(base_root, class_id)\n",
        "    gt_file = os.path.join(class_dir, \"gt.yml\")\n",
        "    pose_dir = os.path.join(class_dir, \"pose\")\n",
        "\n",
        "    if not os.path.exists(gt_file):\n",
        "        print(f\" Missing gt.yml → skipped.\")\n",
        "        continue\n",
        "\n",
        "    os.makedirs(pose_dir, exist_ok=True)\n",
        "\n",
        "    with open(gt_file, 'r') as f:\n",
        "        gt_data = yaml.safe_load(f)\n",
        "\n",
        "    for img_id, poses in tqdm(gt_data.items(), desc=f\"Class {class_id}\", unit=\"img\"):\n",
        "        img_idx = int(img_id)\n",
        "\n",
        "        if len(poses) > 1:\n",
        "            print(f\" Image {img_idx:06d} has multiple objects — using second.\")\n",
        "            selected = poses[1]\n",
        "        else:\n",
        "            selected = poses[0]\n",
        "\n",
        "        R = np.array(selected['cam_R_m2c'], dtype=np.float32).reshape(3, 3)\n",
        "        t = np.array(selected['cam_t_m2c'], dtype=np.float32).reshape(3, 1)  # in mm\n",
        "        RT = np.hstack((R, t))  # Shape: (3, 4)\n",
        "\n",
        "        save_file = os.path.join(pose_dir, f\"pose{img_idx:06d}.npy\")\n",
        "        np.save(save_file, RT)\n",
        "\n",
        "print(\"\\n All pose matrices extracted and saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yim04eDar2j1"
      },
      "source": [
        "### 4 - Modify RGB, Mask, Depth Files Format From 4 Digits to 6 Digits - Radius Map Needs Files in 6-Digit Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yV8C9kelWjbe",
        "outputId": "f58aa465-0d0f-4dd4-8a87-9a42d6904c33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================ Renaming files for class 01================\n",
            "Folder     │ File Count\n",
            "------------------------------------------------------------\n",
            "🌈 rgb      │       1236 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🛡️ mask     │       1236 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌊 depth    │       1236 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "\n",
            "================ Renaming files for class 02================\n",
            "Folder     │ File Count\n",
            "------------------------------------------------------------\n",
            "🌈 rgb      │       1214 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🛡️ mask     │       1215 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌊 depth    │       1214 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "\n",
            "================ Renaming files for class 04================\n",
            "Folder     │ File Count\n",
            "------------------------------------------------------------\n",
            "🌈 rgb      │       1201 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🛡️ mask     │       1201 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌊 depth    │       1201 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "\n",
            "================ Renaming files for class 05================\n",
            "Folder     │ File Count\n",
            "------------------------------------------------------------\n",
            "🌈 rgb      │       1196 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🛡️ mask     │       1196 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌊 depth    │       1196 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "\n",
            "================ Renaming files for class 06================\n",
            "Folder     │ File Count\n",
            "------------------------------------------------------------\n",
            "🌈 rgb      │       1179 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🛡️ mask     │       1179 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌊 depth    │       1179 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "\n",
            "================ Renaming files for class 08================\n",
            "Folder     │ File Count\n",
            "------------------------------------------------------------\n",
            "🌈 rgb      │       1188 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🛡️ mask     │       1188 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌊 depth    │       1188 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "\n",
            "================ Renaming files for class 09================\n",
            "Folder     │ File Count\n",
            "------------------------------------------------------------\n",
            "🌈 rgb      │       1254 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🛡️ mask     │       1254 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌊 depth    │       1254 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "\n",
            "================ Renaming files for class 10================\n",
            "Folder     │ File Count\n",
            "------------------------------------------------------------\n",
            "🌈 rgb      │       1253 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🛡️ mask     │       1253 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌊 depth    │       1253 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "\n",
            "================ Renaming files for class 11================\n",
            "Folder     │ File Count\n",
            "------------------------------------------------------------\n",
            "🌈 rgb      │       1220 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🛡️ mask     │       1220 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌊 depth    │       1220 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "\n",
            "================ Renaming files for class 12================\n",
            "Folder     │ File Count\n",
            "------------------------------------------------------------\n",
            "🌈 rgb      │       1237 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🛡️ mask     │       1237 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌊 depth    │       1237 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "\n",
            "================ Renaming files for class 13================\n",
            "Folder     │ File Count\n",
            "------------------------------------------------------------\n",
            "🌈 rgb      │       1152 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🛡️ mask     │       1152 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌊 depth    │       1152 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "\n",
            "================ Renaming files for class 14================\n",
            "Folder     │ File Count\n",
            "------------------------------------------------------------\n",
            "🌈 rgb      │       1227 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🛡️ mask     │       1227 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌊 depth    │       1227 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "\n",
            "================ Renaming files for class 15================\n",
            "Folder     │ File Count\n",
            "------------------------------------------------------------\n",
            "🌈 rgb      │       1243 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🛡️ mask     │       1225 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌊 depth    │       1243 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                        "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          \n",
            "  All files renamed to 6-digit format.\n",
            "          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "def rename_files_to_six_digits(base_dir, class_ids):\n",
        "    line_width = 60\n",
        "\n",
        "    for cls in class_ids:\n",
        "        print(\"\\n\" + f\" Renaming files for class {cls}\".center(line_width, \"=\"))\n",
        "        print(f\"{'Folder':<10} │ {'File Count':>10}\")\n",
        "        print(\"-\" * line_width)\n",
        "\n",
        "        for folder in ['rgb', 'mask', 'depth']:\n",
        "            emoji = {'rgb': '🌈', 'mask': '🛡️', 'depth': '🌊'}[folder]\n",
        "            folder_path = os.path.join(base_dir, cls, folder)\n",
        "\n",
        "            if not os.path.isdir(folder_path):\n",
        "                print(f\"{emoji} {folder:<8} │ {'—':>10} (missing)\")\n",
        "                continue\n",
        "\n",
        "            files = [f for f in os.listdir(folder_path) if f.split('.')[0].isdigit()]\n",
        "            print(f\"{emoji} {folder:<8} │ {len(files):>10} files\")\n",
        "\n",
        "            for fname in tqdm(files, desc=f\"{cls}/{folder}\", leave=False, unit=\"file\", ncols=40):\n",
        "                idx = int(os.path.splitext(fname)[0])\n",
        "                new_name = f\"{idx:06d}{os.path.splitext(fname)[1]}\"\n",
        "                os.rename(os.path.join(folder_path, fname), os.path.join(folder_path, new_name))\n",
        "\n",
        "        print(\"=\" * line_width)\n",
        "\n",
        "    print(\"\\n  All files renamed to 6-digit format.\\n\".center(line_width))\n",
        "\n",
        "# Base path and class list\n",
        "base_dir = \"/content/dataset/Linemod_preprocessed/data\"\n",
        "classes = ['01','02','04','05','06','08','09','10','11','12','13','14','15']\n",
        "\n",
        "rename_files_to_six_digits(base_dir, classes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxdBZpOTodXW"
      },
      "source": [
        "###5 -  Convert Depth files from .png to .dpt - Radius Map Needs Depth Files With .dpt Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPZWinGpongz",
        "outputId": "41539fd0-554c-4feb-b374-e70e1c15064d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Converting depth PNGs for class 01...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Class 01: 100%|██████████| 1236/1236 [00:05<00:00, 246.99file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Converting depth PNGs for class 02...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Class 02: 100%|██████████| 1214/1214 [00:07<00:00, 171.15file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Converting depth PNGs for class 04...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Class 04: 100%|██████████| 1201/1201 [00:04<00:00, 295.68file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Converting depth PNGs for class 05...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Class 05: 100%|██████████| 1196/1196 [00:05<00:00, 207.98file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Converting depth PNGs for class 06...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Class 06: 100%|██████████| 1179/1179 [00:05<00:00, 217.91file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Converting depth PNGs for class 08...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Class 08: 100%|██████████| 1188/1188 [00:05<00:00, 216.53file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Converting depth PNGs for class 09...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Class 09: 100%|██████████| 1254/1254 [00:04<00:00, 295.94file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Converting depth PNGs for class 10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Class 10: 100%|██████████| 1253/1253 [00:04<00:00, 266.47file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Converting depth PNGs for class 11...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Class 11: 100%|██████████| 1220/1220 [00:05<00:00, 214.21file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Converting depth PNGs for class 12...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Class 12: 100%|██████████| 1237/1237 [00:06<00:00, 193.31file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Converting depth PNGs for class 13...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Class 13: 100%|██████████| 1152/1152 [00:04<00:00, 283.60file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Converting depth PNGs for class 14...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Class 14: 100%|██████████| 1227/1227 [00:05<00:00, 232.72file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Converting depth PNGs for class 15...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Class 15: 100%|██████████| 1243/1243 [00:05<00:00, 222.58file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " All depth PNGs converted to .dpt format.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import struct\n",
        "\n",
        "def save_dpt_file(png_path, dpt_path, scale=1.0):\n",
        "    \"\"\"\n",
        "    Convert 16-bit PNG depth image to .dpt binary format.\n",
        "    Format:\n",
        "      [uint32 height][uint32 width][uint16 * (H×W)]\n",
        "    \"\"\"\n",
        "    depth_img = np.array(Image.open(png_path)).astype(np.float32)\n",
        "    depth_img = (depth_img * scale).astype(np.uint16)\n",
        "\n",
        "    height, width = depth_img.shape\n",
        "\n",
        "    with open(dpt_path, 'wb') as f:\n",
        "        f.write(struct.pack('I', height))\n",
        "        f.write(struct.pack('I', width))\n",
        "        f.write(depth_img.tobytes())\n",
        "\n",
        "    os.remove(png_path)  # Save space after conversion\n",
        "\n",
        "def convert_all_depth_pngs_to_dpt(base_dir, class_ids, scale=1.0):\n",
        "    \"\"\"\n",
        "    Converts all PNG depth images in class folders to binary .dpt format.\n",
        "    \"\"\"\n",
        "    for cls in class_ids:\n",
        "        depth_folder = os.path.join(base_dir, cls, \"depth\")\n",
        "\n",
        "        if not os.path.isdir(depth_folder):\n",
        "            print(f\" Class {cls}: 'depth/' folder not found — skipping.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n Converting depth PNGs for class {cls}...\")\n",
        "\n",
        "        png_files = sorted([f for f in os.listdir(depth_folder) if f.endswith(\".png\")])\n",
        "        for fname in tqdm(png_files, desc=f\"Class {cls}\", unit=\"file\"):\n",
        "            png_path = os.path.join(depth_folder, fname)\n",
        "            dpt_path = os.path.join(depth_folder, fname.replace(\".png\", \".dpt\"))\n",
        "            save_dpt_file(png_path, dpt_path, scale)\n",
        "\n",
        "    print(\"\\n All depth PNGs converted to .dpt format.\")\n",
        "\n",
        "# Run Conversion\n",
        "convert_all_depth_pngs_to_dpt(base_dir, classes, scale=1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za-C--7N2egf"
      },
      "source": [
        "### 3 - Generate Radius Map\n",
        "\n",
        "We compute the mesh and keypoints to:\n",
        "\n",
        "* Mesh, accurately represent an object’s 3D shape.\n",
        "* Reduce complexity by sampling a few representative points.\n",
        "* Enable fast registration and matching across views.\n",
        "* Support precise pose estimation via 3D–2D point correspondences.\n",
        "* Improve efficiency in real-time or learning-based applications.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ppcG2yKdz_s"
      },
      "source": [
        "### **Generate Radius Map**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import open3d as o3d\n",
        "import os\n",
        "from numba import jit, prange\n",
        "from tqdm import tqdm\n",
        "from scipy.ndimage import gaussian_filter\n",
        "\n",
        "linemod_cls_names = ['02']\n",
        "\n",
        "linemod_K = np.array([\n",
        "    [572.4114,   0.0,     325.2611],\n",
        "    [  0.0,     573.57043, 242.04899],\n",
        "    [  0.0,       0.0,       1.0   ]\n",
        "], dtype=np.float32)\n",
        "\n",
        "linemod_path = \"/content/dataset/Linemod_preprocessed/data/\"\n",
        "original_linemod_path = linemod_path\n",
        "\n",
        "def project(xyz, K, RT):\n",
        "    xyz_cam = xyz @ RT[:, :3].T + RT[:, 3:].T\n",
        "    proj = xyz_cam @ K.T\n",
        "    xy = proj[:, :2] / proj[:, 2:3]\n",
        "    return xy, xyz_cam\n",
        "\n",
        "def rgbd_to_point_cloud(K, depth):\n",
        "    vs, us = depth.nonzero()\n",
        "    zs = depth[vs, us]\n",
        "    xs = (us - K[0,2]) * zs / K[0,0]\n",
        "    ys = (vs - K[1,2]) * zs / K[1,1]\n",
        "    pts = np.stack((xs, ys, zs), axis=1)\n",
        "    return pts, vs, us\n",
        "\n",
        "@jit(nopython=True, parallel=True)\n",
        "def fast_for_map(vs, us, dist_list, radius_map):\n",
        "    for i in prange(len(us)):\n",
        "        radius_map[vs[i], us[i]] = dist_list[i]\n",
        "    return radius_map\n",
        "\n",
        "def read_depth(path):\n",
        "    if path.endswith(\".dpt\"):\n",
        "        with open(path, \"rb\") as f:\n",
        "            h, w = np.fromfile(f, dtype=np.uint32, count=2)\n",
        "            data = np.fromfile(f, dtype=np.uint16, count=h * w)\n",
        "        return data.reshape((h, w)).astype(np.float32)\n",
        "    else:\n",
        "        return np.asarray(Image.open(path)).astype(np.float32)\n",
        "\n",
        "# Generate radius maps\n",
        "for cls in linemod_cls_names:\n",
        "    print(f\"\\n[DEBUG] Starting class {cls}\")\n",
        "    depth_dir = os.path.join(linemod_path, cls, \"depth\")\n",
        "    if not os.path.isdir(depth_dir):\n",
        "        print(f\"[SKIP] {depth_dir} does not exist.\")\n",
        "        continue\n",
        "\n",
        "    dpt_files = [f for f in os.listdir(depth_dir) if f.endswith(\".dpt\")]\n",
        "    print(f\"[DEBUG] Found {len(dpt_files)} .dpt files for class {cls}\")\n",
        "    if not dpt_files:\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        keypts = np.load(os.path.join(linemod_path, cls, \"Outside5.npy\"))\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Couldn't load Outside5.npy for {cls}: {e}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"[DEBUG] Loaded {keypts.shape[0]} keypoints for class {cls}\")\n",
        "\n",
        "    total_steps = len(keypts) * len(dpt_files)\n",
        "    pbar = tqdm(total=total_steps, ncols=80, bar_format=\"{desc} │{bar}| {n_fmt}/{total_fmt}\")\n",
        "\n",
        "    for idx_pt, kp in enumerate(keypts, start=1):\n",
        "        folder_name = f\"Out_pt{idx_pt}_dm\"\n",
        "        out_folder = os.path.join(original_linemod_path, cls, folder_name)\n",
        "        os.makedirs(out_folder, exist_ok=True)\n",
        "\n",
        "        for fname in dpt_files:\n",
        "            base = os.path.splitext(fname)[0]\n",
        "            pbar.set_description(f\"🔄 Class {cls} | 🔑 {folder_name}\")\n",
        "\n",
        "            try:\n",
        "                depth = read_depth(os.path.join(depth_dir, fname))\n",
        "                mask_path = os.path.join(linemod_path, cls, \"mask\", base + \".png\")\n",
        "                if not os.path.exists(mask_path):\n",
        "                    raise FileNotFoundError(f\"Missing mask: {mask_path}\")\n",
        "                mask = np.asarray(Image.open(mask_path).convert(\"L\"))\n",
        "                depth[mask == 0] = 0.0\n",
        "\n",
        "                pose_path = os.path.join(linemod_path, cls, \"pose\", f\"pose{base}.npy\")\n",
        "                if not os.path.exists(pose_path):\n",
        "                    raise FileNotFoundError(f\"Missing pose: {pose_path}\")\n",
        "                RT = np.load(pose_path)\n",
        "\n",
        "                pts, vs, us = rgbd_to_point_cloud(linemod_K, depth)\n",
        "                _, kp_cam_xyz = project(np.array([kp]), linemod_K, RT)\n",
        "                kp_cam = kp_cam_xyz[0]\n",
        "                dist_list = np.log1p(np.linalg.norm(pts - kp_cam, axis=1))\n",
        "                radius_map = np.zeros_like(mask, dtype=np.float32)\n",
        "                radius_map = fast_for_map(vs, us, dist_list, radius_map)\n",
        "                radius_map = gaussian_filter(radius_map, sigma=1)\n",
        "\n",
        "                out_path = os.path.join(out_folder, base + \".npy\")\n",
        "                np.save(out_path, radius_map)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"[ERROR] Class {cls} | Img {base} | KP {idx_pt} → {e}\")\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "print(\"\\n Radius map generation complete with log scaling and Gaussian smoothing.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twPP3t5Gtp8f",
        "outputId": "f6114e5a-23a6-4284-a2b9-6a10dd037a16"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[DEBUG] Starting class 02\n",
            "[DEBUG] Found 1214 .dpt files for class 02\n",
            "[DEBUG] Loaded 5 keypoints for class 02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🔄 Class 02 | 🔑 Out_pt5_dm:  │██████████████████████████████████████| 6070/6070"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Radius map generation complete with log scaling and Gaussian smoothing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx11kUkndrmA"
      },
      "source": [
        "### **Train Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT_d0-U-gkpW"
      },
      "source": [
        "### Split Data: %70: Train - %20 Validation - %10 Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_MLHwzbAvO4m",
        "outputId": "ae36e64f-fa5f-48dc-a99a-5e81be6a1975"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Splitting RGB frames for class 02\n",
            "  train: 849 samples\n",
            "  val  : 243 samples\n",
            "  test : 122 samples\n",
            "\n",
            " Done splitting.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import yaml\n",
        "import random\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "base_dir = \"/content/dataset/Linemod_preprocessed/data\"\n",
        "classes = [\"02\"]\n",
        "random.seed(42)  # Reproducibility\n",
        "\n",
        "for cls in classes:\n",
        "    print(f\"\\n Splitting RGB frames for class {cls}\")\n",
        "\n",
        "    rgb_dir = os.path.join(base_dir, cls, \"rgb\")\n",
        "    gt_path = os.path.join(base_dir, cls, \"gt.yml\")\n",
        "    split_dir = os.path.join(base_dir, cls, \"Split\")\n",
        "    os.makedirs(split_dir, exist_ok=True)\n",
        "\n",
        "    # Load all RGB image indices\n",
        "    rgb_paths = sorted(glob.glob(os.path.join(rgb_dir, \"*.png\")))\n",
        "    rgb_indices = {os.path.splitext(os.path.basename(p))[0] for p in rgb_paths}\n",
        "\n",
        "    # Load ground-truth frame indices from gt.yml\n",
        "    if not os.path.exists(gt_path):\n",
        "        print(f\"  gt.yml missing for class {cls}, skipping.\")\n",
        "        continue\n",
        "    with open(gt_path, \"r\") as f:\n",
        "        gt_data = yaml.safe_load(f)\n",
        "    gt_indices = {f\"{int(k):06d}\" for k in gt_data.keys()}\n",
        "\n",
        "    # Intersect to find valid image IDs\n",
        "    valid_ids = list(rgb_indices & gt_indices)\n",
        "    if not valid_ids:\n",
        "        print(f\"  No valid frames found for class {cls}.\")\n",
        "        continue\n",
        "\n",
        "    # Shuffle and split\n",
        "    random.shuffle(valid_ids)\n",
        "    n = len(valid_ids)\n",
        "    train, val, test = valid_ids[:int(0.7*n)], valid_ids[int(0.7*n):int(0.9*n)], valid_ids[int(0.9*n):]\n",
        "\n",
        "    # Write split files\n",
        "    for split_name, split_ids in zip([\"train\", \"val\", \"test\"], [train, val, test]):\n",
        "        path = os.path.join(split_dir, f\"{split_name}.txt\")\n",
        "        with open(path, \"w\") as f:\n",
        "            f.write(\"\\n\".join(split_ids))\n",
        "        print(f\"  {split_name:<5}: {len(split_ids)} samples\")\n",
        "\n",
        "print(\"\\n Done splitting.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalization over Dataset"
      ],
      "metadata": {
        "id": "Dptivg10sXLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "ROOT      = \"/content/dataset/Linemod_preprocessed/data\"\n",
        "CLASSES   = ['02']\n",
        "OUT_PTS   = [f\"Out_pt{i}_dm\" for i in range(1,6)]\n",
        "MEAN_RGB  = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
        "STD_RGB   = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
        "DEPTH_MAX = 10.0\n",
        "\n",
        "def read_depth_dpt(path):\n",
        "    with open(path, \"rb\") as f:\n",
        "        h, w = np.fromfile(f, dtype=np.uint32, count=2)\n",
        "        data = np.fromfile(f, dtype=np.uint16, count=h*w)\n",
        "    return data.reshape(h, w).astype(np.float32)\n",
        "\n",
        "def sep():\n",
        "    print(\"─\" * 70)\n",
        "\n",
        "for cls in CLASSES:\n",
        "    cls_dir = os.path.join(ROOT, cls)\n",
        "    if not os.path.isdir(cls_dir):\n",
        "        sep()\n",
        "        print(f\" Class folder missing: {cls_dir}\")\n",
        "        sep()\n",
        "        continue\n",
        "\n",
        "    sep()\n",
        "    print(f\"  Class {cls}  \".center(70, \"─\"))\n",
        "    sep()\n",
        "\n",
        "    # .npy Check\n",
        "    print(f\"{'Folder':<20} │ {'Count':>5}\")\n",
        "    sep()\n",
        "    for out_dir in OUT_PTS:\n",
        "        dir_path = os.path.join(cls_dir, out_dir)\n",
        "        if not os.path.isdir(dir_path):\n",
        "            continue\n",
        "        files = [f for f in os.listdir(dir_path) if f.endswith(\".npy\")]\n",
        "        print(f\"  {out_dir:<16} │ {len(files):>5}\")\n",
        "        for fn in files:\n",
        "            path = os.path.join(dir_path, fn)\n",
        "            try:\n",
        "                if os.path.getsize(path) == 0:\n",
        "                    raise ValueError(\"empty file\")\n",
        "                arr = np.load(path).astype(np.float32)\n",
        "                m   = arr.max() if arr.max() > 0 else 1.0\n",
        "                np.save(path, arr / m)\n",
        "            except Exception as e:\n",
        "                print(f\"   [SKIP] {fn:<16} │ {str(e)}\")\n",
        "    sep()\n",
        "\n",
        "    # Check RGB/DEPTH/MASK/POSE for train/val/test\n",
        "    split_dir = os.path.join(cls_dir, \"Split\")\n",
        "    for split in (\"train.txt\", \"val.txt\", \"test.txt\"):\n",
        "        split_path = os.path.join(split_dir, split)\n",
        "        if not os.path.isfile(split_path):\n",
        "            print(f\"ℹ  No {split:<12} │ skipping\")\n",
        "            continue\n",
        "\n",
        "        ids = []\n",
        "        with open(split_path) as f:\n",
        "            for line in f:\n",
        "                s = line.strip()\n",
        "                if s.isdigit():\n",
        "                    ids.append(int(s))\n",
        "        print()\n",
        "        sep()\n",
        "        print(f\" Split: {split:<10} │ {len(ids):>5} images\")\n",
        "        sep()\n",
        "\n",
        "        # RGB\n",
        "        print(f\" {'RGB':<16} │ checking ...\")\n",
        "        for idx in ids:\n",
        "            try:\n",
        "                p = os.path.join(cls_dir, \"rgb\", f\"{idx:06d}.png\")\n",
        "                im = Image.open(p).convert(\"RGB\")\n",
        "                arr = (np.asarray(im, np.float32) / 255.0 - MEAN_RGB) / STD_RGB\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"    [SKIP rgb {idx:06d}] │ {str(e)}\")\n",
        "\n",
        "        # Depth\n",
        "        print(f\"  {'Depth':<16} │ checking ...\")\n",
        "        for idx in ids:\n",
        "            try:\n",
        "                p  = os.path.join(cls_dir, \"depth\", f\"{idx:06d}.dpt\")\n",
        "                dp = read_depth_dpt(p)\n",
        "                dp = np.clip(dp, 0, DEPTH_MAX) / DEPTH_MAX\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"    [SKIP depth {idx:06d}] │ {str(e)}\")\n",
        "\n",
        "        # Mask\n",
        "        print(f\"  {'Mask':<16} │ checking ...\")\n",
        "        for idx in ids:\n",
        "            try:\n",
        "                p  = os.path.join(cls_dir, \"mask\", f\"{idx:06d}.png\")\n",
        "                m  = np.asarray(Image.open(p).convert(\"L\"), np.uint8)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  [SKIP mask {idx:06d}] │ {str(e)}\")\n",
        "\n",
        "        # Pose\n",
        "        print(f\"  {'Pose':<16} │ normalizing ...\")\n",
        "        for idx in ids:\n",
        "            try:\n",
        "                p    = os.path.join(cls_dir, \"pose\", f\"pose{idx:06d}.npy\")\n",
        "                pose = np.load(p).astype(np.float32)\n",
        "                pose[:3,3] /= 1000.0\n",
        "                np.save(p, pose)\n",
        "            except Exception as e:\n",
        "                print(f\"   🚧 [SKIP pose {idx:06d}] │ {str(e)}\")\n",
        "        sep()\n",
        "\n",
        "    print(f\" Done class {cls}\".center(70))\n",
        "    sep()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FCouNlsqbwpJ",
        "outputId": "b9bc4493-05ed-4990-bc26-8c2c9763664d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "──────────────────────────────────────────────────────────────────────\n",
            "─────────────────────────────  Class 02  ─────────────────────────────\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Folder               │ Count\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "  Out_pt1_dm       │  1214\n",
            "  Out_pt2_dm       │  1214\n",
            "  Out_pt3_dm       │  1214\n",
            "  Out_pt4_dm       │  1214\n",
            "  Out_pt5_dm       │  1214\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            " Split: train.txt  │   849 images\n",
            "──────────────────────────────────────────────────────────────────────\n",
            " RGB              │ checking ...\n",
            "  Depth            │ checking ...\n",
            "  Mask             │ checking ...\n",
            "  Pose             │ normalizing ...\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            " Split: val.txt    │   243 images\n",
            "──────────────────────────────────────────────────────────────────────\n",
            " RGB              │ checking ...\n",
            "  Depth            │ checking ...\n",
            "  Mask             │ checking ...\n",
            "  Pose             │ normalizing ...\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            " Split: test.txt   │   122 images\n",
            "──────────────────────────────────────────────────────────────────────\n",
            " RGB              │ checking ...\n",
            "  Depth            │ checking ...\n",
            "  Mask             │ checking ...\n",
            "  Pose             │ normalizing ...\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "                             Done class 02                            \n",
            "──────────────────────────────────────────────────────────────────────\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train**"
      ],
      "metadata": {
        "id": "2KsgE4ZxQxhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colorama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-sn2AhNkYBw",
        "outputId": "3c6d4156-f3e9-4007-c4f9-6126e1fae5f2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama\n",
            "Successfully installed colorama-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Enhanced RCVPose: Deep Learning Model for 6D Pose Estimation\n",
        "\n",
        "This script is designed to train a 6D pose estimation model using RGB images, depth maps, masks, pose data, and radius maps (Outside9 points) for multiple objects. The code is fully documented in English and all data paths are explicitly set according to the provided folder structure.\n",
        "\n",
        "Folder Structure for Each Object (e.g., 01, 02, ...):\n",
        "- Out_pt1_dm to Out_pt9_dm: Folders containing .npy files for each radius map\n",
        "- depth: Contains .dpt files (depth images)\n",
        "- mask: Contains .png files (object masks)\n",
        "- pose: Contains pose files named as pose00000.npy, pose00001.npy, ...\n",
        "- rgb: Contains .png files (RGB images)\n",
        "- Split: Contains train.txt, val.txt, test.txt (lists of sample names)\n",
        "- Outside9.npy: Reference points for the object\n",
        "- gt.yml, info.yml: Metadata files\n",
        "- mesh.ply: 3D mesh model\n",
        "- test.txt, train.txt: Additional text files (if needed)\n",
        "\n",
        "All measurements are in millimeters.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.nn import functional as F\n",
        "import time\n",
        "from colorama import init, Fore, Style\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from PIL import Image\n",
        "from scipy.spatial.transform import Rotation as R\n",
        "\n",
        "# Initialize colorama for colored terminal output\n",
        "init()\n",
        "\n",
        "# =========================\n",
        "# Configuration Section\n",
        "# =========================\n",
        "CONFIG = {\n",
        "    # Base directory containing all object folders (01, 02, ...)\n",
        "    'BASE_DIR': '/content/dataset/Linemod_preprocessed/data',\n",
        "    # List of object IDs to train (edit this list to select which objects to train)\n",
        "    #'OBJECT_IDS': ['01', '02', '04', '05', '06', '08', '09', '10', '11', '12', '13', '14', '15'],\n",
        "    'OBJECT_IDS': ['01'],\n",
        "    # Subdirectory names inside each object folder\n",
        "    'RGB_DIR': 'rgb',              # RGB images (.png)\n",
        "    'DEPTH_DIR': 'depth',          # Depth maps (.dpt)\n",
        "    'MASK_DIR': 'mask',            # Object masks (.png)\n",
        "    'POSE_DIR': 'pose',            # Pose data (pose00000.npy, ...)\n",
        "    'RADIUS_BASE_DIR': '.',        # Base dir for Out_pt*_dm folders (relative to object folder)\n",
        "    'RADIUS_PREFIX': 'Out_pt',     # Prefix for radius map folders\n",
        "    'RADIUS_SUFFIX': '_dm',        # Suffix for radius map folders\n",
        "    'NUM_RADIUS_POINTS': 5,        # Number of radius map points\n",
        "    'SPLIT_DIR': 'Split',          # Directory containing split files\n",
        "    'TRAIN_SPLIT': 'train.txt',    # Training set filenames\n",
        "    'VAL_SPLIT': 'val.txt',        # Validation set filenames\n",
        "    'TEST_SPLIT': 'test.txt',      # Test set filenames\n",
        "    # Training parameters\n",
        "    'BATCH_SIZE': 8,               # Reduced batch size for memory efficiency\n",
        "    'NUM_WORKERS': 2,              # Reduced number of workers\n",
        "    'NUM_EPOCHS': 100,             # Number of training epochs\n",
        "    'LEARNING_RATE': 0.001,        # Initial learning rate\n",
        "    'ACCUMULATION_STEPS': 4,       # Number of mini-batches to accumulate gradients over\n",
        "    # Model saving\n",
        "    'MODELS_DIR': '/content/models',  # Directory to save trained models\n",
        "}\n",
        "\n",
        "# =========================\n",
        "# Logger for Training Output\n",
        "# =========================\n",
        "class TrainingLogger:\n",
        "    \"\"\"\n",
        "    Logger class for professional, colored, and well-separated output during training.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.start_time = time.time()\n",
        "        self.epoch_times = []\n",
        "        self.separator = \"=\" * 100\n",
        "        self.sub_separator = \"-\" * 100\n",
        "    def log_section(self, message):\n",
        "        print(f\"\\n{Fore.CYAN}{self.separator}{Style.RESET_ALL}\")\n",
        "        print(f\"{Fore.CYAN}🚀 {message}{Style.RESET_ALL}\")\n",
        "        print(f\"{Fore.CYAN}{self.separator}{Style.RESET_ALL}\\n\")\n",
        "    def log_subsection(self, message, no_lines=False):\n",
        "        if no_lines:\n",
        "            print(f\"{Fore.YELLOW}{message}{Style.RESET_ALL}\")\n",
        "        else:\n",
        "            print(f\"\\n{Fore.YELLOW}{self.sub_separator}{Style.RESET_ALL}\")\n",
        "            print(f\"{Fore.YELLOW}📌 {message}{Style.RESET_ALL}\")\n",
        "            print(f\"{Fore.YELLOW}{self.sub_separator}{Style.RESET_ALL}\\n\")\n",
        "    def log_info(self, message):\n",
        "        print(f\"{Fore.GREEN}ℹ️  {message}{Style.RESET_ALL}\")\n",
        "        print(f\"{Fore.GREEN}{'.' * 50}{Style.RESET_ALL}\")\n",
        "    def log_warning(self, message):\n",
        "        print(f\"\\n{Fore.YELLOW}⚠️  {message}{Style.RESET_ALL}\")\n",
        "        print(f\"{Fore.YELLOW}{'!' * 50}{Style.RESET_ALL}\\n\")\n",
        "    def log_error(self, message):\n",
        "        print(f\"\\n{Fore.RED}❌ {message}{Style.RESET_ALL}\")\n",
        "        print(f\"{Fore.RED}{'#' * 50}{Style.RESET_ALL}\\n\")\n",
        "    def log_success(self, message):\n",
        "        print(f\"\\n{Fore.GREEN}✅ {message}{Style.RESET_ALL}\")\n",
        "        print(f\"{Fore.GREEN}{'*' * 50}{Style.RESET_ALL}\\n\")\n",
        "    def log_metrics(self, metrics, phase=\"Training\", no_lines=False):\n",
        "        if no_lines:\n",
        "            print(f\"{Fore.MAGENTA}{'=' * 50}{Style.RESET_ALL}\")\n",
        "            print(f\"{Fore.MAGENTA}📊 {phase} Metrics:{Style.RESET_ALL}\")\n",
        "            print(f\"{Fore.MAGENTA}{'-' * 50}{Style.RESET_ALL}\")\n",
        "            for key, value in metrics.items():\n",
        "                print(f\"{Fore.MAGENTA}   {key}: {value:.4f}{Style.RESET_ALL}\")\n",
        "            print(f\"{Fore.MAGENTA}{'=' * 50}{Style.RESET_ALL}\\n\")\n",
        "        else:\n",
        "            print(f\"\\n{Fore.MAGENTA}{'=' * 50}{Style.RESET_ALL}\")\n",
        "            print(f\"{Fore.MAGENTA}📊 {phase} Metrics:{Style.RESET_ALL}\")\n",
        "            print(f\"{Fore.MAGENTA}{'-' * 50}{Style.RESET_ALL}\")\n",
        "            for key, value in metrics.items():\n",
        "                print(f\"{Fore.MAGENTA}   {key}: {value:.4f}{Style.RESET_ALL}\")\n",
        "            print(f\"{Fore.MAGENTA}{'=' * 50}{Style.RESET_ALL}\\n\")\n",
        "    def log_model_save(self, path):\n",
        "        print(f\"\\n{Fore.BLUE}{'=' * 50}{Style.RESET_ALL}\")\n",
        "        print(f\"{Fore.BLUE}💾 Model saved at: {path}{Style.RESET_ALL}\")\n",
        "        print(f\"{Fore.BLUE}{'=' * 50}{Style.RESET_ALL}\\n\")\n",
        "    def log_time(self, epoch, total_epochs):\n",
        "        elapsed = time.time() - self.start_time\n",
        "        avg_epoch_time = elapsed / (epoch + 1)\n",
        "        remaining_time = avg_epoch_time * (total_epochs - epoch - 1)\n",
        "        print(f\"\\n{Fore.CYAN}{'=' * 50}{Style.RESET_ALL}\")\n",
        "        print(f\"{Fore.CYAN}⏱️  Time Information:{Style.RESET_ALL}\")\n",
        "        print(f\"{Fore.CYAN}{'-' * 50}{Style.RESET_ALL}\")\n",
        "        print(f\"{Fore.CYAN}   Elapsed Time: {self.format_time(elapsed)}{Style.RESET_ALL}\")\n",
        "        print(f\"{Fore.CYAN}   Average Epoch Time: {self.format_time(avg_epoch_time)}{Style.RESET_ALL}\")\n",
        "        print(f\"{Fore.CYAN}   Estimated Remaining Time: {self.format_time(remaining_time)}{Style.RESET_ALL}\")\n",
        "        print(f\"{Fore.CYAN}{'=' * 50}{Style.RESET_ALL}\\n\")\n",
        "    def log_epoch_start(self, epoch, total_epochs):\n",
        "        print(f\"\\n{Fore.CYAN}{'#' * 100}{Style.RESET_ALL}\")\n",
        "        print(f\"{Fore.CYAN}🔄 Starting Epoch {epoch + 1}/{total_epochs}{Style.RESET_ALL}\")\n",
        "        print(f\"{Fore.CYAN}{'#' * 100}{Style.RESET_ALL}\\n\")\n",
        "    def log_epoch_end(self, epoch, total_epochs, metrics):\n",
        "        print(f\"\\n{Fore.CYAN}{'#' * 100}{Style.RESET_ALL}\")\n",
        "        print(f\"{Fore.CYAN}✅ Completed Epoch {epoch + 1}/{total_epochs}{Style.RESET_ALL}\")\n",
        "        self.log_metrics(metrics, f\"Epoch {epoch + 1} Summary\")\n",
        "        print(f\"{Fore.CYAN}{'#' * 100}{Style.RESET_ALL}\\n\")\n",
        "    @staticmethod\n",
        "    def format_time(seconds):\n",
        "        hours = int(seconds // 3600)\n",
        "        minutes = int((seconds % 3600) // 60)\n",
        "        seconds = int(seconds % 60)\n",
        "        return f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n",
        "\n",
        "def safe_read_depth(path):\n",
        "    \"\"\"\n",
        "    Safely read a depth image. If the file is .dpt, try to read as binary; otherwise, use OpenCV.\n",
        "    Returns a numpy array or raises a FileNotFoundError if not found or unreadable.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Depth image not found: {path}\")\n",
        "    if path.endswith('.dpt'):\n",
        "        try:\n",
        "            with open(path, 'rb') as f:\n",
        "                h, w = np.fromfile(f, dtype=np.uint32, count=2)\n",
        "                data = np.fromfile(f, dtype=np.uint16, count=h*w)\n",
        "                if data.size != h*w:\n",
        "                    raise ValueError(f\"Depth data size mismatch in file: {path}\")\n",
        "                return data.reshape(h, w).astype(np.float32)\n",
        "        except Exception as e:\n",
        "            raise IOError(f\"Failed to read .dpt file: {path}. Error: {e}\")\n",
        "    # For other formats, use OpenCV\n",
        "    depth_img = cv2.imread(path, cv2.IMREAD_ANYDEPTH)\n",
        "    if depth_img is None:\n",
        "        raise IOError(f\"Failed to read depth image: {path}\")\n",
        "    return depth_img.astype(np.float32)\n",
        "\n",
        "# =========================\n",
        "# Custom Dataset Definition\n",
        "# =========================\n",
        "class CustomDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset for loading RGB, depth, mask, pose, and radius map data for a single object.\n",
        "    Each sample consists of:\n",
        "      - RGB image (.png)\n",
        "      - Depth map (.dpt)\n",
        "      - Mask (.png)\n",
        "      - Pose (pose{base_name}.npy)\n",
        "      - Radius maps (9 .npy files, one from each Out_pt*_dm folder)\n",
        "    All measurements are in millimeters.\n",
        "    \"\"\"\n",
        "    def __init__(self, rgb_dir, depth_dir, mask_dir, pose_dir, radius_base_dir, transform_rgb=None, transform_depth=None, transform_mask=None):\n",
        "        self.rgb_dir = rgb_dir\n",
        "        self.depth_dir = depth_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.pose_dir = pose_dir\n",
        "        self.radius_base_dir = radius_base_dir\n",
        "        self.transform_rgb = transform_rgb\n",
        "        self.transform_depth = transform_depth\n",
        "        self.transform_mask = transform_mask\n",
        "        self.filenames = sorted([f for f in os.listdir(rgb_dir) if f.endswith('.png')])\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.filenames[idx]\n",
        "        base_name = filename.split('.')[0]  # e.g., '000000'\n",
        "        # Load RGB image (H, W, 3) in uint8\n",
        "        rgb_path = os.path.join(self.rgb_dir, f'{base_name}.png')\n",
        "        rgb_img = cv2.imread(rgb_path)\n",
        "        rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)\n",
        "        rgb_img = Image.fromarray(rgb_img)  # Convert to PIL.Image\n",
        "        # Load depth map safely (handles .dpt and other formats)\n",
        "        depth_path = os.path.join(self.depth_dir, f'{base_name}.dpt')\n",
        "        depth_img = safe_read_depth(depth_path)  # (H, W) in float32\n",
        "        depth_img = np.expand_dims(depth_img / 10000.0, axis=2)  # Normalize depth to [0, 1] and add channel\n",
        "        depth_img = (depth_img * 255).astype(np.uint8)  # Convert to uint8 for PIL compatibility\n",
        "        depth_img = Image.fromarray(depth_img.squeeze(), mode='L')  # Convert to PIL.Image\n",
        "        # Load mask (H, W) in uint8\n",
        "        mask_path = os.path.join(self.mask_dir, f'{base_name}.png')\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        mask = np.expand_dims(mask.astype(np.float32) / 255.0, axis=2)  # Normalize mask to [0, 1] and add channel\n",
        "        mask = (mask * 255).astype(np.uint8)\n",
        "        mask = Image.fromarray(mask.squeeze(), mode='L')\n",
        "        # Load pose (with 'pose' prefix)\n",
        "        pose_path = os.path.join(self.pose_dir, f'pose{base_name}.npy')\n",
        "        pose = np.load(pose_path)\n",
        "        # Convert pose to (7,) vector if needed\n",
        "        if pose.shape == (3, 4):\n",
        "            rot = R.from_matrix(pose[:, :3]).as_quat()  # (x, y, z, w)\n",
        "            trans = pose[:, 3]\n",
        "            pose_vec = np.concatenate([trans, rot])  # (3,) + (4,) = (7,)\n",
        "            pose = pose_vec\n",
        "        elif pose.shape == (4, 4):\n",
        "            rot = R.from_matrix(pose[:3, :3]).as_quat()\n",
        "            trans = pose[:3, 3]\n",
        "            pose_vec = np.concatenate([trans, rot])\n",
        "            pose = pose_vec\n",
        "        # If pose is already (7,), do nothing\n",
        "        # Load radius maps for all 9 points\n",
        "        radius_maps = []\n",
        "        for pt_idx in range(1, CONFIG['NUM_RADIUS_POINTS'] + 1):\n",
        "            radius_folder = f\"Out_pt{pt_idx}_dm\"\n",
        "            radius_path = os.path.join(self.radius_base_dir, radius_folder, f'{base_name}.npy')\n",
        "            radius_map = np.load(radius_path)\n",
        "            # --- FULL RESOLUTION: Do NOT resize radius maps ---\n",
        "            # radius_map = cv2.resize(radius_map, (256, 256), interpolation=cv2.INTER_LINEAR)  # <-- REMOVE or COMMENT OUT\n",
        "            radius_maps.append(radius_map)\n",
        "        radius_maps = np.array(radius_maps)\n",
        "        # Apply separate transforms for each type\n",
        "        if self.transform_rgb:\n",
        "            rgb_img = self.transform_rgb(rgb_img)\n",
        "        if self.transform_depth:\n",
        "            depth_img = self.transform_depth(depth_img)\n",
        "        if self.transform_mask:\n",
        "            mask = self.transform_mask(mask)\n",
        "        return {\n",
        "            'rgb': rgb_img.float(),\n",
        "            'depth': depth_img.float(),\n",
        "            'mask': mask.float(),\n",
        "            'pose': torch.FloatTensor(pose),\n",
        "            'radius_maps': torch.FloatTensor(radius_maps)\n",
        "        }\n",
        "\n",
        "class AdvancedAugmentation:\n",
        "    \"\"\"\n",
        "    Applies various transformations to input images to improve model robustness.\n",
        "    This helps the model learn better by seeing different variations of the same image.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # Set up color transformations\n",
        "        self.color_jitter = transforms.ColorJitter(\n",
        "            brightness=0.2,  # Random brightness adjustment\n",
        "            contrast=0.2,    # Random contrast adjustment\n",
        "            saturation=0.2,  # Random saturation adjustment\n",
        "            hue=0.1         # Random hue adjustment\n",
        "        )\n",
        "\n",
        "    def __call__(self, rgb, depth, mask):\n",
        "        # Apply color transformations to RGB image\n",
        "        rgb = self.color_jitter(rgb)\n",
        "\n",
        "        # Random rotation between -10 and 10 degrees\n",
        "        angle = np.random.uniform(-10, 10)\n",
        "        rgb = transforms.functional.rotate(rgb, angle)\n",
        "        depth = transforms.functional.rotate(depth, angle)\n",
        "        mask = transforms.functional.rotate(mask, angle)\n",
        "\n",
        "        # Random scaling between 0.9 and 1.1\n",
        "        scale = np.random.uniform(0.9, 1.1)\n",
        "        rgb = transforms.functional.resize(rgb, (int(rgb.shape[1] * scale), int(rgb.shape[2] * scale)))\n",
        "        depth = transforms.functional.resize(depth, (int(depth.shape[1] * scale), int(depth.shape[2] * scale)))\n",
        "        mask = transforms.functional.resize(mask, (int(mask.shape[1] * scale), int(mask.shape[2] * scale)))\n",
        "\n",
        "        # Add small random noise to depth image\n",
        "        depth_noise = torch.randn_like(depth) * 0.01\n",
        "        depth = depth + depth_noise\n",
        "\n",
        "        return rgb, depth, mask\n",
        "\n",
        "class AttentionModule(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention mechanism to focus on important parts of the image.\n",
        "    This helps the model pay more attention to relevant features.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super(AttentionModule, self).__init__()\n",
        "        # Create query, key, and value projections\n",
        "        self.query = nn.Conv2d(in_channels, in_channels // 8, 1)\n",
        "        self.key = nn.Conv2d(in_channels, in_channels // 8, 1)\n",
        "        self.value = nn.Conv2d(in_channels, in_channels, 1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))  # Learnable parameter\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, C, H, W = x.size()\n",
        "\n",
        "        # Generate attention map\n",
        "        query = self.query(x).view(batch_size, -1, H * W)\n",
        "        key = self.key(x).view(batch_size, -1, H * W)\n",
        "        value = self.value(x).view(batch_size, -1, H * W)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        attention = torch.bmm(query.permute(0, 2, 1), key)\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "\n",
        "        # Apply attention to values\n",
        "        out = torch.bmm(value, attention.permute(0, 2, 1))\n",
        "        out = out.view(batch_size, C, H, W)\n",
        "\n",
        "        return self.gamma * out + x\n",
        "\n",
        "class FeaturePyramidNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Feature Pyramid Network for multi-scale feature extraction.\n",
        "    Accepts a list of input channels for each feature map.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels_list):\n",
        "        super(FeaturePyramidNetwork, self).__init__()\n",
        "        # Create lateral connections for each input channel\n",
        "        self.lateral_convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_ch, 256, 1) for in_ch in in_channels_list\n",
        "        ])\n",
        "        # Create output convolutions\n",
        "        self.fpn_convs = nn.ModuleList([\n",
        "            nn.Conv2d(256, 256, 3, padding=1) for _ in in_channels_list\n",
        "        ])\n",
        "    def forward(self, features):\n",
        "        # features should be a list of feature maps from different layers\n",
        "        laterals = [conv(feature) for feature, conv in zip(features, self.lateral_convs)]\n",
        "        # Top-down pathway\n",
        "        for i in range(len(laterals)-1, 0, -1):\n",
        "            laterals[i-1] += F.interpolate(laterals[i], size=laterals[i-1].shape[-2:])\n",
        "        # Final convolutions\n",
        "        return [conv(lateral) for lateral, conv in zip(laterals, self.fpn_convs)]\n",
        "\n",
        "class EnhancedRCVPose(nn.Module):\n",
        "    \"\"\"\n",
        "    Main model for 6D pose estimation.\n",
        "    Combines RGB and depth information to predict object pose.\n",
        "    Uses FPN with correct feature extraction from ResNet.\n",
        "    Now uses ResNet50 (original, higher memory usage).\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(EnhancedRCVPose, self).__init__()\n",
        "        # Use ResNet50 as backbone (original)\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        self.rgb_layer1 = nn.Sequential(*list(resnet.children())[:5])   # Output: 256\n",
        "        self.rgb_layer2 = list(resnet.children())[5]                    # Output: 512\n",
        "        self.rgb_layer3 = list(resnet.children())[6]                    # Output: 1024\n",
        "        self.rgb_layer4 = list(resnet.children())[7]                    # Output: 2048\n",
        "\n",
        "        resnet_depth = models.resnet50(pretrained=True)\n",
        "        resnet_depth.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.depth_layer1 = nn.Sequential(*list(resnet_depth.children())[:5])\n",
        "        self.depth_layer2 = list(resnet_depth.children())[5]\n",
        "        self.depth_layer3 = list(resnet_depth.children())[6]\n",
        "        self.depth_layer4 = list(resnet_depth.children())[7]\n",
        "\n",
        "        self.rgb_fpn = FeaturePyramidNetwork([512, 1024, 2048])\n",
        "        self.depth_fpn = FeaturePyramidNetwork([512, 1024, 2048])\n",
        "\n",
        "        self.rgb_attention = AttentionModule(256)\n",
        "        self.depth_attention = AttentionModule(256)\n",
        "\n",
        "        # Fusion block\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=False)\n",
        "        )\n",
        "\n",
        "        # Pose head (global pooling + FC)\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.pose_head = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, 7)\n",
        "        )\n",
        "\n",
        "        # Outside9 head: output 9 full-resolution maps\n",
        "        self.outside9_head = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.Conv2d(128, 9, kernel_size=1)  # 9 channel output\n",
        "        )\n",
        "\n",
        "    def forward(self, rgb, depth):\n",
        "      x1 = self.rgb_layer1(rgb)\n",
        "      x2 = self.rgb_layer2(x1)\n",
        "      x3 = self.rgb_layer3(x2)\n",
        "      x4 = self.rgb_layer4(x3)\n",
        "      rgb_fpn_features = self.rgb_fpn([x2, x3, x4])\n",
        "\n",
        "      d1 = self.depth_layer1(depth)\n",
        "      d2 = self.depth_layer2(d1)\n",
        "      d3 = self.depth_layer3(d2)\n",
        "      d4 = self.depth_layer4(d3)\n",
        "      depth_fpn_features = self.depth_fpn([d2, d3, d4])\n",
        "\n",
        "      rgb_attended = self.rgb_attention(rgb_fpn_features[0])\n",
        "      depth_attended = self.depth_attention(depth_fpn_features[0])\n",
        "      combined = torch.cat([rgb_attended, depth_attended], dim=1)\n",
        "      fused = self.fusion(combined)  # [B, 256, H, W]\n",
        "\n",
        "      # Pose: global pooling + FC\n",
        "      pooled = self.global_pool(fused)  # [B, 256, 1, 1]\n",
        "      pose = self.pose_head(pooled.view(pooled.size(0), -1))\n",
        "\n",
        "      # 🔧 Normalize quaternion (4D) without in-place operation\n",
        "      quaternion = pose[:, 3:].clone()\n",
        "      pose = torch.cat([pose[:, :3], F.normalize(quaternion, dim=1)], dim=1)\n",
        "\n",
        "      # Outside9 prediction\n",
        "      outside9 = self.outside9_head(fused)  # [B, 9, h, w]\n",
        "      target_size = (rgb.shape[2], rgb.shape[3])  # (H, W)\n",
        "      outside9 = F.interpolate(outside9, size=target_size, mode='bilinear', align_corners=False)\n",
        "\n",
        "      return pose, outside9\n",
        "\n",
        "\n",
        "class GeometricLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Loss function that combines translation, rotation, and point correspondence errors.\n",
        "    All measurements are in millimeters.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(GeometricLoss, self).__init__()\n",
        "\n",
        "    def forward(self, pred_pose, target_pose, pred_outside9, target_outside9):\n",
        "      trans_loss = F.mse_loss(pred_pose[:, :3], target_pose[:, :3])\n",
        "      rot_dot = torch.sum(pred_pose[:, 3:] * target_pose[:, 3:], dim=1)\n",
        "      rot_loss = torch.mean(1.0 - torch.clamp(torch.abs(rot_dot), 0, 1))  # clamp to avoid NaN\n",
        "      points_loss = F.mse_loss(pred_outside9, target_outside9)\n",
        "      total_loss = trans_loss + 0.1 * rot_loss + 0.5 * points_loss\n",
        "      return total_loss, trans_loss, rot_loss, points_loss\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=100, learning_rate=0.001):\n",
        "    \"\"\"\n",
        "    Train the model with advanced features and professional logging.\n",
        "    Uses gradient accumulation and mixed precision training for memory efficiency.\n",
        "    \"\"\"\n",
        "    logger = TrainingLogger()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    logger.log_section(\"Initializing Training\")\n",
        "    logger.log_info(f\"Using device: {device}\")\n",
        "    model = model.to(device)\n",
        "    os.makedirs(CONFIG['MODELS_DIR'], exist_ok=True)\n",
        "    logger.log_success(f\"Created models directory at: {CONFIG['MODELS_DIR']}\")\n",
        "\n",
        "    criterion = GeometricLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=learning_rate,\n",
        "        epochs=num_epochs,\n",
        "        steps_per_epoch=len(train_loader)\n",
        "    )\n",
        "\n",
        "    # Initialize gradient scaler for mixed precision training\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    accumulation_steps = CONFIG.get('ACCUMULATION_STEPS', 4)\n",
        "    logger.log_section(\"Starting Training Loop\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        torch.cuda.empty_cache()  # Free up GPU memory at the start of each epoch\n",
        "        epoch_start_time = time.time()\n",
        "        # #### EPOCH HEADER ####\n",
        "        print(f\"\\n{'#'*12} Epoch {epoch+1}/{num_epochs} {'#'*12}\")\n",
        "        logger.log_epoch_start(epoch, num_epochs)\n",
        "        model.train()\n",
        "        train_losses = {'total': 0.0, 'trans': 0.0, 'rot': 0.0, 'points': 0.0}\n",
        "        # Training Phase (no line separators)\n",
        "        logger.log_subsection(\"Training Phase\", no_lines=True)\n",
        "        optimizer.zero_grad()\n",
        "        for i, batch in enumerate(tqdm(train_loader, desc=f'🔄 Training')):\n",
        "            rgb = batch['rgb'].to(device)\n",
        "            depth = batch['depth'].to(device)\n",
        "            pose_target = batch['pose'].to(device)\n",
        "            outside9_target = batch['radius_maps'].to(device)\n",
        "            with autocast():\n",
        "                pose_pred, outside9_pred = model(rgb, depth)\n",
        "                total_loss, trans_loss, rot_loss, points_loss = criterion(\n",
        "                    pose_pred, pose_target, outside9_pred, outside9_target\n",
        "                )\n",
        "            scaler.scale(total_loss / accumulation_steps).backward()\n",
        "            if (i + 1) % accumulation_steps == 0:\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "            train_losses['total'] += total_loss.item()\n",
        "            train_losses['trans'] += trans_loss.item()\n",
        "            train_losses['rot'] += rot_loss.item()\n",
        "            train_losses['points'] += points_loss.item()\n",
        "        if (i + 1) % accumulation_steps != 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "        for k in train_losses:\n",
        "            train_losses[k] /= len(train_loader)\n",
        "        logger.log_metrics(train_losses, \"Training\", no_lines=True)\n",
        "        # ----\n",
        "        print(\"----\")\n",
        "        model.eval()\n",
        "        val_losses = {'total': 0.0, 'trans': 0.0, 'rot': 0.0, 'points': 0.0}\n",
        "        # Validation Phase (no line separators)\n",
        "        logger.log_subsection(\"Validation Phase\", no_lines=True)\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=f'🔍 Validation'):\n",
        "                rgb = batch['rgb'].to(device)\n",
        "                depth = batch['depth'].to(device)\n",
        "                pose_target = batch['pose'].to(device)\n",
        "                outside9_target = batch['radius_maps'].to(device)\n",
        "                with autocast():\n",
        "                    pose_pred, outside9_pred = model(rgb, depth)\n",
        "                    total_loss, trans_loss, rot_loss, points_loss = criterion(\n",
        "                        pose_pred, pose_target, outside9_pred, outside9_target\n",
        "                    )\n",
        "                val_losses['total'] += total_loss.item()\n",
        "                val_losses['trans'] += trans_loss.item()\n",
        "                val_losses['rot'] += rot_loss.item()\n",
        "                val_losses['points'] += points_loss.item()\n",
        "        for k in val_losses:\n",
        "            val_losses[k] /= len(val_loader)\n",
        "        logger.log_metrics(val_losses, \"Validation\", no_lines=True)\n",
        "        # #### EPOCH FOOTER ####\n",
        "        print(f\"{'#'*12} End Epoch {epoch+1}/{num_epochs} {'#'*12}\\n\")\n",
        "\n",
        "        if val_losses['total'] < best_val_loss:\n",
        "            best_val_loss = val_losses['total']\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            save_path = os.path.join(CONFIG['MODELS_DIR'], f'best_model_{timestamp}.pth')\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_loss': val_losses['total'],\n",
        "                'val_metrics': val_losses,\n",
        "                'config': CONFIG\n",
        "            }, save_path)\n",
        "            logger.log_success(f\"New best model saved with validation loss: {val_losses['total']:.4f}\")\n",
        "            logger.log_model_save(save_path)\n",
        "\n",
        "            # Save backup to Google Drive\n",
        "            drive_save_path = os.path.join('/content/drive/MyDrive/models', f'best_model_{timestamp}.pth')\n",
        "            os.makedirs(os.path.dirname(drive_save_path), exist_ok=True)\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_loss': val_losses['total'],\n",
        "                'val_metrics': val_losses,\n",
        "                'config': CONFIG\n",
        "            }, drive_save_path)\n",
        "            logger.log_success(f\"Backup copy saved to Google Drive\")\n",
        "            logger.log_model_save(drive_save_path)\n",
        "\n",
        "        logger.log_time(epoch, num_epochs)\n",
        "        logger.log_epoch_end(epoch, num_epochs, {\n",
        "            'train_loss': train_losses['total'],\n",
        "            'val_loss': val_losses['total']\n",
        "        })\n",
        "\n",
        "    logger.log_section(\"Training Completed\")\n",
        "    logger.log_success(f\"Best validation loss achieved: {best_val_loss:.4f}\")\n",
        "\n",
        "def load_split_file(split_file):\n",
        "    \"\"\"\n",
        "    Load filenames from a split file.\n",
        "\n",
        "    Args:\n",
        "        split_file (str): Path to the split file (Train.txt, Test.txt, or Validation.txt)\n",
        "\n",
        "    Returns:\n",
        "        list: List of filenames (without extension)\n",
        "    \"\"\"\n",
        "    with open(split_file, 'r') as f:\n",
        "        filenames = [line.strip() for line in f.readlines()]\n",
        "    return filenames\n",
        "\n",
        "def load_pretrained_weights(model, checkpoint_path=\"/content/drive/MyDrive/models/best_model_20250529_150327.pth\"):\n",
        "    import os\n",
        "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "    pretrained_state = checkpoint['model_state_dict']\n",
        "    model_state = model.state_dict()\n",
        "    matched = {k: v for k, v in pretrained_state.items() if k in model_state and v.size() == model_state[k].size()}\n",
        "    model_state.update(matched)\n",
        "    model.load_state_dict(model_state)\n",
        "    print(f\" Loaded {len(matched)} pretrained layers from {checkpoint_path}\")\n",
        "    return model\n",
        "\n",
        "def main():\n",
        "    logger = TrainingLogger()\n",
        "    logger.log_section(\"Starting RCVPose Training for Selected Objects\")\n",
        "\n",
        "    # Use the list of object IDs specified in CONFIG['OBJECT_IDS']\n",
        "    object_ids = CONFIG['OBJECT_IDS']\n",
        "    base_data_dir = CONFIG['BASE_DIR']\n",
        "\n",
        "    for object_id in object_ids:\n",
        "        logger.log_section(f\"Training for Object {object_id}\")\n",
        "        object_dir = os.path.join(base_data_dir, object_id)\n",
        "\n",
        "        rgb_dir = os.path.join(object_dir, CONFIG['RGB_DIR'])\n",
        "        depth_dir = os.path.join(object_dir, CONFIG['DEPTH_DIR'])\n",
        "        mask_dir = os.path.join(object_dir, CONFIG['MASK_DIR'])\n",
        "        pose_dir = os.path.join(object_dir, CONFIG['POSE_DIR'])\n",
        "        radius_dir = os.path.join(object_dir, CONFIG['RADIUS_BASE_DIR'])\n",
        "        split_dir = os.path.join(object_dir, CONFIG['SPLIT_DIR'])\n",
        "\n",
        "        logger.log_info(\"Loading split files\")\n",
        "        # Load split files\n",
        "        train_filenames = load_split_file(os.path.join(split_dir, CONFIG['TRAIN_SPLIT']))\n",
        "        val_filenames = load_split_file(os.path.join(split_dir, CONFIG['VAL_SPLIT']))\n",
        "        test_filenames = load_split_file(os.path.join(split_dir, CONFIG['TEST_SPLIT']))\n",
        "\n",
        "        logger.log_success(f\"Loaded {len(train_filenames)} training samples for Object {object_id}\")\n",
        "        logger.log_success(f\"Loaded {len(val_filenames)} validation samples for Object {object_id}\")\n",
        "        logger.log_success(f\"Loaded {len(test_filenames)} test samples for Object {object_id}\")\n",
        "\n",
        "        logger.log_info(\"Setting up data transforms and augmentation\")\n",
        "        # Define transforms and augmentation WITHOUT resizing (full resolution)\n",
        "        transform_rgb = transforms.Compose([\n",
        "            # No resizing! Use full resolution\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        transform_depth = transforms.Compose([\n",
        "            # No resizing! Use full resolution\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "        transform_mask = transforms.Compose([\n",
        "            # No resizing! Use full resolution\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "        augmentation = AdvancedAugmentation()\n",
        "\n",
        "        logger.log_info(\"Creating datasets\")\n",
        "        # Create datasets\n",
        "        full_dataset = CustomDataset(rgb_dir, depth_dir, mask_dir, pose_dir, radius_dir,\n",
        "                                   transform_rgb=transform_rgb,\n",
        "                                   transform_depth=transform_depth,\n",
        "                                   transform_mask=transform_mask)\n",
        "\n",
        "        # Restore original train/val split\n",
        "        train_dataset = torch.utils.data.Subset(full_dataset,\n",
        "            [i for i, f in enumerate(full_dataset.filenames) if f.split('.')[0] in train_filenames])\n",
        "        val_dataset = torch.utils.data.Subset(full_dataset,\n",
        "            [i for i, f in enumerate(full_dataset.filenames) if f.split('.')[0] in val_filenames])\n",
        "\n",
        "        logger.log_info(\"Creating data loaders\")\n",
        "        # Create data loaders with optimized settings\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=CONFIG['BATCH_SIZE'],\n",
        "            shuffle=True,\n",
        "            num_workers=CONFIG['NUM_WORKERS'],\n",
        "            pin_memory=True,\n",
        "            persistent_workers=True,\n",
        "            prefetch_factor=2\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=CONFIG['BATCH_SIZE'],\n",
        "            shuffle=False,\n",
        "            num_workers=CONFIG['NUM_WORKERS'],\n",
        "            pin_memory=True,\n",
        "            persistent_workers=True,\n",
        "            prefetch_factor=2\n",
        "        )\n",
        "\n",
        "        logger.log_info(\"Initializing model\")\n",
        "        # Initialize model with ResNet18 backbone\n",
        "        model = EnhancedRCVPose()\n",
        "        model = load_pretrained_weights(model)\n",
        "\n",
        "        logger.log_section(f\"Starting Training Process for Object {object_id}\")\n",
        "        # Start training\n",
        "        train_model(model, train_loader, val_loader,\n",
        "                    num_epochs=CONFIG['NUM_EPOCHS'],\n",
        "                    learning_rate=CONFIG['LEARNING_RATE'])\n",
        "\n",
        "        logger.log_section(f\"Training Process Completed for Object {object_id}\")\n",
        "\n",
        "    logger.log_section(\"All Object Trainings Completed\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BPjhiytw3HkQ",
        "outputId": "40928d97-5a28-4dfb-b31b-9e0ced9d98c3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================================================================================================\n",
            "🚀 Starting RCVPose Training for Selected Objects\n",
            "====================================================================================================\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "🚀 Training for Object 02\n",
            "====================================================================================================\n",
            "\n",
            "ℹ️  Loading split files\n",
            "..................................................\n",
            "\n",
            "✅ Loaded 849 training samples for Object 02\n",
            "**************************************************\n",
            "\n",
            "\n",
            "✅ Loaded 243 validation samples for Object 02\n",
            "**************************************************\n",
            "\n",
            "\n",
            "✅ Loaded 122 test samples for Object 02\n",
            "**************************************************\n",
            "\n",
            "ℹ️  Setting up data transforms and augmentation\n",
            "..................................................\n",
            "ℹ️  Creating datasets\n",
            "..................................................\n",
            "ℹ️  Creating data loaders\n",
            "..................................................\n",
            "ℹ️  Initializing model\n",
            "..................................................\n",
            " Loaded 684 pretrained layers from /content/drive/MyDrive/models/best_model_20250529_150327.pth\n",
            "\n",
            "====================================================================================================\n",
            "🚀 Starting Training Process for Object 02\n",
            "====================================================================================================\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "🚀 Initializing Training\n",
            "====================================================================================================\n",
            "\n",
            "ℹ️  Using device: cuda\n",
            "..................................................\n",
            "\n",
            "✅ Created models directory at: /content/models\n",
            "**************************************************\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-e74b7580de0a>:498: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================================================================================================\n",
            "🚀 Starting Training Loop\n",
            "====================================================================================================\n",
            "\n",
            "\n",
            "############ Epoch 1/100 ############\n",
            "\n",
            "####################################################################################################\n",
            "🔄 Starting Epoch 1/100\n",
            "####################################################################################################\n",
            "\n",
            "Training Phase\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🔄 Training:   0%|          | 0/107 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\", line 171, in collate\n    {\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\", line 172, in <dictcomp>\n    key: collate(\n         ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\", line 272, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack expects each tensor to be equal size, but got [3, 475, 633] at entry 0 and [3, 525, 701] at entry 1\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-e74b7580de0a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-e74b7580de0a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_section\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting Training Process for Object {object_id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m         train_model(model, train_loader, val_loader,\n\u001b[0m\u001b[1;32m    700\u001b[0m                     \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NUM_EPOCHS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     learning_rate=CONFIG['LEARNING_RATE'])\n",
            "\u001b[0;32m<ipython-input-29-e74b7580de0a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_subsection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Phase\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'🔄 Training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m             \u001b[0mrgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rgb'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'depth'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1478\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\", line 171, in collate\n    {\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\", line 172, in <dictcomp>\n    key: collate(\n         ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\", line 272, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack expects each tensor to be equal size, but got [3, 475, 633] at entry 0 and [3, 525, 701] at entry 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Validation**"
      ],
      "metadata": {
        "id": "BbAqYcV9HLro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from scipy.spatial.transform import Rotation as R\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import torchvision.models as models\n",
        "\n",
        "# =========================\n",
        "# CONFIGURATION SECTION\n",
        "# =========================\n",
        "CONFIG = {\n",
        "    'BASE_DIR': '/content/dataset/Linemod_preprocessed/data',  # Root directory containing all object folders\n",
        "    'OBJECT_ID': '01',  # Object ID to validate (e.g., '01')\n",
        "    'BATCH_SIZE': 1,  # For validation, use batch size 1 for accurate metrics\n",
        "    'NUM_RADIUS_POINTS': 9,  # Number of radius map points\n",
        "    'DEVICE': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "    'MODELS_DIR': '/content/models',  # Directory containing saved models\n",
        "    'MESH_PATH': 'mesh.ply',  # Mesh file for ADD metric (optional)\n",
        "}\n",
        "\n",
        "# =========================\n",
        "# DEPTH READER FOR .dpt FILES\n",
        "# =========================\n",
        "def read_depth_dpt(path):\n",
        "    with open(path, \"rb\") as f:\n",
        "        h, w = np.fromfile(f, dtype=np.uint32, count=2)\n",
        "        data = np.fromfile(f, dtype=np.uint16, count=h*w)\n",
        "    return data.reshape(h, w).astype(np.float32)\n",
        "\n",
        "# =========================\n",
        "# MODEL ARCHITECTURE\n",
        "# =========================\n",
        "class AttentionModule(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(AttentionModule, self).__init__()\n",
        "        self.query = nn.Conv2d(in_channels, in_channels // 8, 1)\n",
        "        self.key = nn.Conv2d(in_channels, in_channels // 8, 1)\n",
        "        self.value = nn.Conv2d(in_channels, in_channels, 1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, C, H, W = x.size()\n",
        "        query = self.query(x).view(batch_size, -1, H * W)\n",
        "        key = self.key(x).view(batch_size, -1, H * W)\n",
        "        value = self.value(x).view(batch_size, -1, H * W)\n",
        "        attention = torch.bmm(query.permute(0, 2, 1), key)\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        out = torch.bmm(value, attention.permute(0, 2, 1))\n",
        "        out = out.view(batch_size, C, H, W)\n",
        "        return self.gamma * out + x\n",
        "\n",
        "class FeaturePyramidNetwork(nn.Module):\n",
        "    def __init__(self, in_channels_list):\n",
        "        super(FeaturePyramidNetwork, self).__init__()\n",
        "        self.lateral_convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_ch, 256, 1) for in_ch in in_channels_list\n",
        "        ])\n",
        "        self.fpn_convs = nn.ModuleList([\n",
        "            nn.Conv2d(256, 256, 3, padding=1) for _ in in_channels_list\n",
        "        ])\n",
        "\n",
        "    def forward(self, features):\n",
        "        laterals = [conv(feature) for feature, conv in zip(features, self.lateral_convs)]\n",
        "        for i in range(len(laterals)-1, 0, -1):\n",
        "            laterals[i-1] += F.interpolate(laterals[i], size=laterals[i-1].shape[-2:])\n",
        "        return [conv(lateral) for lateral, conv in zip(laterals, self.fpn_convs)]\n",
        "\n",
        "class EnhancedRCVPose(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedRCVPose, self).__init__()\n",
        "        # Use ResNet50 as backbone\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        self.rgb_layer1 = nn.Sequential(*list(resnet.children())[:5])   # Output: 256\n",
        "        self.rgb_layer2 = list(resnet.children())[5]                    # Output: 512\n",
        "        self.rgb_layer3 = list(resnet.children())[6]                    # Output: 1024\n",
        "        self.rgb_layer4 = list(resnet.children())[7]                    # Output: 2048\n",
        "\n",
        "        resnet_depth = models.resnet50(pretrained=True)\n",
        "        resnet_depth.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.depth_layer1 = nn.Sequential(*list(resnet_depth.children())[:5])\n",
        "        self.depth_layer2 = list(resnet_depth.children())[5]\n",
        "        self.depth_layer3 = list(resnet_depth.children())[6]\n",
        "        self.depth_layer4 = list(resnet_depth.children())[7]\n",
        "\n",
        "        self.rgb_fpn = FeaturePyramidNetwork([512, 1024, 2048])\n",
        "        self.depth_fpn = FeaturePyramidNetwork([512, 1024, 2048])\n",
        "        self.rgb_attention = AttentionModule(256)\n",
        "        self.depth_attention = AttentionModule(256)\n",
        "\n",
        "        # Fusion block\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=False)\n",
        "        )\n",
        "\n",
        "        # Pose head\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.pose_head = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, 7)\n",
        "        )\n",
        "\n",
        "        # Outside9 head\n",
        "        self.outside9_head = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.Conv2d(128, 9, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, rgb, depth):\n",
        "        x1 = self.rgb_layer1(rgb)\n",
        "        x2 = self.rgb_layer2(x1)\n",
        "        x3 = self.rgb_layer3(x2)\n",
        "        x4 = self.rgb_layer4(x3)\n",
        "        rgb_fpn_features = self.rgb_fpn([x2, x3, x4])\n",
        "\n",
        "        d1 = self.depth_layer1(depth)\n",
        "        d2 = self.depth_layer2(d1)\n",
        "        d3 = self.depth_layer3(d2)\n",
        "        d4 = self.depth_layer4(d3)\n",
        "        depth_fpn_features = self.depth_fpn([d2, d3, d4])\n",
        "\n",
        "        rgb_attended = self.rgb_attention(rgb_fpn_features[0])\n",
        "        depth_attended = self.depth_attention(depth_fpn_features[0])\n",
        "\n",
        "        combined = torch.cat([rgb_attended, depth_attended], dim=1)\n",
        "        fused = self.fusion(combined)\n",
        "\n",
        "        # Pose prediction\n",
        "        pooled = self.global_pool(fused)\n",
        "        pose = self.pose_head(pooled.view(pooled.size(0), -1))\n",
        "\n",
        "        # Outside9 prediction\n",
        "        outside9 = self.outside9_head(fused)\n",
        "        target_size = (rgb.shape[2], rgb.shape[3])\n",
        "        outside9 = F.interpolate(outside9, size=target_size, mode='bilinear', align_corners=False)\n",
        "\n",
        "        return pose, outside9\n",
        "\n",
        "# =========================\n",
        "# DATASET CLASS\n",
        "# =========================\n",
        "class ValidationDataset(Dataset):\n",
        "    def __init__(self, base_dir, object_id, num_radius_points=9):\n",
        "        self.base_dir = base_dir\n",
        "        self.object_id = object_id\n",
        "        self.num_radius_points = num_radius_points\n",
        "\n",
        "        # Set up paths\n",
        "        self.rgb_dir = os.path.join(base_dir, object_id, 'rgb')\n",
        "        self.depth_dir = os.path.join(base_dir, object_id, 'depth')\n",
        "        self.mask_dir = os.path.join(base_dir, object_id, 'mask')\n",
        "        self.pose_dir = os.path.join(base_dir, object_id, 'pose')\n",
        "        self.radius_base_dir = os.path.join(base_dir, object_id)\n",
        "\n",
        "        # Load validation split\n",
        "        split_file = os.path.join(base_dir, object_id, 'Split', 'val.txt')\n",
        "        with open(split_file, 'r') as f:\n",
        "            self.filenames = [line.strip() for line in f.readlines()]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            filename = self.filenames[idx]\n",
        "            base_name = filename.split('.')[0]\n",
        "            # Load RGB image\n",
        "            rgb_path = os.path.join(self.rgb_dir, f'{base_name}.png')\n",
        "            rgb_img = cv2.imread(rgb_path)\n",
        "            rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)\n",
        "            rgb_img = Image.fromarray(rgb_img)\n",
        "            rgb_img = torch.from_numpy(np.array(rgb_img)).float() / 255.0\n",
        "            rgb_img = rgb_img.permute(2, 0, 1)\n",
        "            # Load depth image (.dpt files need special reader)\n",
        "            depth_path = os.path.join(self.depth_dir, f'{base_name}.dpt')\n",
        "            if not os.path.exists(depth_path):\n",
        "                raise FileNotFoundError(f\"Depth file not found: {depth_path}\")\n",
        "            depth_img = read_depth_dpt(depth_path)\n",
        "            if depth_img is None:\n",
        "                raise ValueError(f\"read_depth_dpt failed to read depth file: {depth_path}\")\n",
        "            depth_img = torch.from_numpy(depth_img).float() / 10000.0\n",
        "            depth_img = depth_img.unsqueeze(0)\n",
        "            # Load pose\n",
        "            pose_path = os.path.join(self.pose_dir, f'pose{base_name}.npy')\n",
        "            pose = np.load(pose_path)\n",
        "            if pose.shape == (3, 4):\n",
        "                rot = R.from_matrix(pose[:, :3]).as_quat()\n",
        "                trans = pose[:, 3]\n",
        "                pose = np.concatenate([trans, rot])\n",
        "            elif pose.shape == (4, 4):\n",
        "                rot = R.from_matrix(pose[:3, :3]).as_quat()\n",
        "                trans = pose[:3, 3]\n",
        "                pose = np.concatenate([trans, rot])\n",
        "            # Load radius maps\n",
        "            radius_maps = []\n",
        "            for pt_idx in range(1, self.num_radius_points + 1):\n",
        "                radius_folder = f\"Out_pt{pt_idx}_dm\"\n",
        "                radius_path = os.path.join(self.radius_base_dir, radius_folder, f'{base_name}.npy')\n",
        "                radius_map = np.load(radius_path)\n",
        "                radius_maps.append(radius_map)\n",
        "            radius_maps = np.array(radius_maps)\n",
        "            return {\n",
        "                'rgb': rgb_img,\n",
        "                'depth': depth_img,\n",
        "                'pose': torch.FloatTensor(pose),\n",
        "                'radius_maps': torch.FloatTensor(radius_maps)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            # Raise a RuntimeError with sample name for skipping in validation\n",
        "            raise RuntimeError(f\"Sample {self.filenames[idx]} skipped due to error: {e}\")\n",
        "\n",
        "# =========================\n",
        "# METRIC FUNCTIONS\n",
        "# =========================\n",
        "def translation_rmse(pred, gt):\n",
        "    return np.sqrt(np.mean((pred[:3] - gt[:3]) ** 2))\n",
        "\n",
        "def rotation_error(pred, gt):\n",
        "    pred_quat = pred[3:]\n",
        "    gt_quat = gt[3:]\n",
        "    # Normalize quaternions\n",
        "    pred_quat = pred_quat / np.linalg.norm(pred_quat)\n",
        "    gt_quat = gt_quat / np.linalg.norm(gt_quat)\n",
        "    # Compute quaternion difference\n",
        "    diff = 1 - np.abs(np.dot(pred_quat, gt_quat))\n",
        "    return diff\n",
        "\n",
        "def add_metric(pred_pose, gt_pose, mesh_points, threshold=0.1):\n",
        "    if mesh_points is None:\n",
        "        return 0.0\n",
        "\n",
        "    # Convert quaternions to rotation matrices\n",
        "    pred_rot = R.from_quat(pred_pose[3:]).as_matrix()\n",
        "    gt_rot = R.from_quat(gt_pose[3:]).as_matrix()\n",
        "\n",
        "    # Transform mesh points\n",
        "    pred_points = np.dot(mesh_points, pred_rot.T) + pred_pose[:3]\n",
        "    gt_points = np.dot(mesh_points, gt_rot.T) + gt_pose[:3]\n",
        "\n",
        "    # Compute mean distance\n",
        "    mean_dist = np.mean(np.linalg.norm(pred_points - gt_points, axis=1))\n",
        "    return mean_dist\n",
        "\n",
        "def get_latest_model(models_dir):\n",
        "    model_files = glob.glob(os.path.join(models_dir, '*.pth'))\n",
        "    if not model_files:\n",
        "        raise FileNotFoundError(f\"No model files found in {models_dir}\")\n",
        "    return max(model_files, key=os.path.getctime)\n",
        "\n",
        "# =========================\n",
        "# VALIDATION PIPELINE\n",
        "# =========================\n",
        "def validate():\n",
        "    print(\"\\n================= RCVPose Validation (Colab) =================\")\n",
        "\n",
        "    # Find and load latest model\n",
        "    model_path = get_latest_model(CONFIG['MODELS_DIR'])\n",
        "    print(f\"Using latest model: {model_path}\")\n",
        "\n",
        "    model = EnhancedRCVPose().to(CONFIG['DEVICE'])\n",
        "    checkpoint = torch.load(model_path, map_location=CONFIG['DEVICE'])\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    # Load validation data\n",
        "    dataset = ValidationDataset(CONFIG['BASE_DIR'], CONFIG['OBJECT_ID'], CONFIG['NUM_RADIUS_POINTS'])\n",
        "\n",
        "    # Load mesh for ADD metric\n",
        "    mesh_path = os.path.join(CONFIG['BASE_DIR'], CONFIG['OBJECT_ID'], CONFIG['MESH_PATH'])\n",
        "    mesh_points = None\n",
        "    try:\n",
        "        if os.path.exists(mesh_path):\n",
        "            import open3d as o3d\n",
        "            mesh_points = np.asarray(o3d.io.read_point_cloud(mesh_path).points)\n",
        "    except ImportError:\n",
        "        print(\"open3d is not installed. Please run: !pip install open3d\")\n",
        "\n",
        "    # Metrics accumulators\n",
        "    trans_errors = []\n",
        "    rot_errors = []\n",
        "    points_errors = []\n",
        "    add_errors = []\n",
        "    add_success = 0\n",
        "\n",
        "    # Validation loop (skip samples with missing/corrupt files)\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(len(dataset)), desc=\"Validating\"):\n",
        "            try:\n",
        "                batch = dataset[i]\n",
        "            except RuntimeError as e:\n",
        "                print(e)\n",
        "                continue\n",
        "            # Prepare batch (single sample)\n",
        "            rgb = batch['rgb'].unsqueeze(0).to(CONFIG['DEVICE'])\n",
        "            depth = batch['depth'].unsqueeze(0).to(CONFIG['DEVICE'])\n",
        "            pose_gt = batch['pose'].unsqueeze(0).to(CONFIG['DEVICE'])\n",
        "            radius_maps_gt = batch['radius_maps'].unsqueeze(0).to(CONFIG['DEVICE'])\n",
        "            # Forward pass\n",
        "            pose_pred, radius_maps_pred = model(rgb, depth)\n",
        "            # Move predictions to CPU for metric computation\n",
        "            pose_pred = pose_pred.cpu().numpy()\n",
        "            pose_gt = pose_gt.cpu().numpy()\n",
        "            radius_maps_pred = radius_maps_pred.cpu().numpy()\n",
        "            radius_maps_gt = radius_maps_gt.cpu().numpy()\n",
        "            # Compute metrics\n",
        "            for j in range(len(pose_pred)):\n",
        "                trans_error = translation_rmse(pose_pred[j], pose_gt[j])\n",
        "                trans_errors.append(trans_error)\n",
        "                pose_pred[j][3:] /= np.linalg.norm(pose_pred[j][3:])\n",
        "                pose_gt[j][3:] /= np.linalg.norm(pose_gt[j][3:])\n",
        "                rot_error = rotation_error(pose_pred[j], pose_gt[j])\n",
        "                rot_errors.append(rot_error)\n",
        "                points_error = np.mean((radius_maps_pred[j] - radius_maps_gt[j]) ** 2)\n",
        "                points_errors.append(points_error)\n",
        "                if mesh_points is not None:\n",
        "                    add_error = add_metric(pose_pred[j], pose_gt[j], mesh_points)\n",
        "                    add_errors.append(add_error)\n",
        "                    if add_error < 0.1:\n",
        "                        add_success += 1\n",
        "\n",
        "    # Compute and print final metrics\n",
        "    print(\"\\nValidation Results:\")\n",
        "    print(f\"Translation RMSE: {np.mean(trans_errors):.4f} mm\")\n",
        "    print(f\"Rotation Error: {np.mean(rot_errors):.4f}\")\n",
        "    print(f\"Points MSE: {np.mean(points_errors):.4f}\")\n",
        "    if mesh_points is not None:\n",
        "        print(f\"ADD Metric: {np.mean(add_errors):.4f}\")\n",
        "        print(f\"ADD Success Rate: {add_success/len(trans_errors)*100:.2f}%\")\n",
        "\n",
        "    return {\n",
        "        'trans_rmse': np.mean(trans_errors),\n",
        "        'rot_error': np.mean(rot_errors),\n",
        "        'points_mse': np.mean(points_errors),\n",
        "        'add_metric': np.mean(add_errors) if mesh_points is not None else None,\n",
        "        'add_success_rate': add_success/len(trans_errors)*100 if mesh_points is not None else None\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    validate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoKBaCBCHLHI",
        "outputId": "8b901db5-1b43-4b2a-e75c-710533fe3049"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================= RCVPose Validation (Colab) =================\n",
            "Using latest model: /content/models/best_model_20250529_150327.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 247/247 [00:20<00:00, 12.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Results:\n",
            "Translation RMSE: 0.0797 mm\n",
            "Rotation Error: 0.2383\n",
            "Points MSE: 0.0017\n",
            "ADD Metric: 28.2495\n",
            "ADD Success Rate: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test**"
      ],
      "metadata": {
        "id": "q1BtA8hCbrYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from scipy.spatial.transform import Rotation as R\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import torchvision.models as models\n",
        "\n",
        "# =========================\n",
        "# CONFIGURATION SECTION\n",
        "# =========================\n",
        "CONFIG = {\n",
        "    'BASE_DIR': '/content/dataset/Linemod_preprocessed/data',  # Root directory containing all object folders\n",
        "    'OBJECT_ID': '01',  # Object ID to test (e.g., '01')\n",
        "    'BATCH_SIZE': 1,  # For test, use batch size 1 for accurate metrics\n",
        "    'NUM_RADIUS_POINTS': 9,  # Number of radius map points\n",
        "    'DEVICE': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "    'MODELS_DIR': '/content/models',  # Directory containing saved models\n",
        "    'MESH_PATH': 'mesh.ply',  # Mesh file for ADD metric (optional)\n",
        "}\n",
        "\n",
        "# =========================\n",
        "# DEPTH READER FOR .dpt FILES\n",
        "# =========================\n",
        "def read_depth_dpt(path):\n",
        "    with open(path, \"rb\") as f:\n",
        "        h, w = np.fromfile(f, dtype=np.uint32, count=2)\n",
        "        data = np.fromfile(f, dtype=np.uint16, count=h*w)\n",
        "    return data.reshape(h, w).astype(np.float32)\n",
        "\n",
        "# =========================\n",
        "# MODEL ARCHITECTURE\n",
        "# =========================\n",
        "class AttentionModule(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(AttentionModule, self).__init__()\n",
        "        self.query = nn.Conv2d(in_channels, in_channels // 8, 1)\n",
        "        self.key = nn.Conv2d(in_channels, in_channels // 8, 1)\n",
        "        self.value = nn.Conv2d(in_channels, in_channels, 1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "    def forward(self, x):\n",
        "        batch_size, C, H, W = x.size()\n",
        "        query = self.query(x).view(batch_size, -1, H * W)\n",
        "        key = self.key(x).view(batch_size, -1, H * W)\n",
        "        value = self.value(x).view(batch_size, -1, H * W)\n",
        "        attention = torch.bmm(query.permute(0, 2, 1), key)\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        out = torch.bmm(value, attention.permute(0, 2, 1))\n",
        "        out = out.view(batch_size, C, H, W)\n",
        "        return self.gamma * out + x\n",
        "\n",
        "class FeaturePyramidNetwork(nn.Module):\n",
        "    def __init__(self, in_channels_list):\n",
        "        super(FeaturePyramidNetwork, self).__init__()\n",
        "        self.lateral_convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_ch, 256, 1) for in_ch in in_channels_list\n",
        "        ])\n",
        "        self.fpn_convs = nn.ModuleList([\n",
        "            nn.Conv2d(256, 256, 3, padding=1) for _ in in_channels_list\n",
        "        ])\n",
        "    def forward(self, features):\n",
        "        laterals = [conv(feature) for feature, conv in zip(features, self.lateral_convs)]\n",
        "        for i in range(len(laterals)-1, 0, -1):\n",
        "            laterals[i-1] += F.interpolate(laterals[i], size=laterals[i-1].shape[-2:])\n",
        "        return [conv(lateral) for lateral, conv in zip(laterals, self.fpn_convs)]\n",
        "\n",
        "class EnhancedRCVPose(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedRCVPose, self).__init__()\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        self.rgb_layer1 = nn.Sequential(*list(resnet.children())[:5])\n",
        "        self.rgb_layer2 = list(resnet.children())[5]\n",
        "        self.rgb_layer3 = list(resnet.children())[6]\n",
        "        self.rgb_layer4 = list(resnet.children())[7]\n",
        "        resnet_depth = models.resnet50(pretrained=True)\n",
        "        resnet_depth.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.depth_layer1 = nn.Sequential(*list(resnet_depth.children())[:5])\n",
        "        self.depth_layer2 = list(resnet_depth.children())[5]\n",
        "        self.depth_layer3 = list(resnet_depth.children())[6]\n",
        "        self.depth_layer4 = list(resnet_depth.children())[7]\n",
        "        self.rgb_fpn = FeaturePyramidNetwork([512, 1024, 2048])\n",
        "        self.depth_fpn = FeaturePyramidNetwork([512, 1024, 2048])\n",
        "        self.rgb_attention = AttentionModule(256)\n",
        "        self.depth_attention = AttentionModule(256)\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.pose_head = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, 7)\n",
        "        )\n",
        "        self.outside9_head = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 9, kernel_size=1)\n",
        "        )\n",
        "    def forward(self, rgb, depth):\n",
        "        x1 = self.rgb_layer1(rgb)\n",
        "        x2 = self.rgb_layer2(x1)\n",
        "        x3 = self.rgb_layer3(x2)\n",
        "        x4 = self.rgb_layer4(x3)\n",
        "        rgb_fpn_features = self.rgb_fpn([x2, x3, x4])\n",
        "        d1 = self.depth_layer1(depth)\n",
        "        d2 = self.depth_layer2(d1)\n",
        "        d3 = self.depth_layer3(d2)\n",
        "        d4 = self.depth_layer4(d3)\n",
        "        depth_fpn_features = self.depth_fpn([d2, d3, d4])\n",
        "        rgb_attended = self.rgb_attention(rgb_fpn_features[0])\n",
        "        depth_attended = self.depth_attention(depth_fpn_features[0])\n",
        "        combined = torch.cat([rgb_attended, depth_attended], dim=1)\n",
        "        fused = self.fusion(combined)\n",
        "        pooled = self.global_pool(fused)\n",
        "        pose = self.pose_head(pooled.view(pooled.size(0), -1))\n",
        "        outside9 = self.outside9_head(fused)\n",
        "        target_size = (rgb.shape[2], rgb.shape[3])\n",
        "        outside9 = F.interpolate(outside9, size=target_size, mode='bilinear', align_corners=False)\n",
        "        return pose, outside9\n",
        "\n",
        "# =========================\n",
        "# DATASET CLASS\n",
        "# =========================\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, base_dir, object_id, num_radius_points=9):\n",
        "        self.base_dir = base_dir\n",
        "        self.object_id = object_id\n",
        "        self.num_radius_points = num_radius_points\n",
        "        self.rgb_dir = os.path.join(base_dir, object_id, 'rgb')\n",
        "        self.depth_dir = os.path.join(base_dir, object_id, 'depth')\n",
        "        self.mask_dir = os.path.join(base_dir, object_id, 'mask')\n",
        "        self.pose_dir = os.path.join(base_dir, object_id, 'pose')\n",
        "        self.radius_base_dir = os.path.join(base_dir, object_id)\n",
        "        split_file = os.path.join(base_dir, object_id, 'Split', 'test.txt')\n",
        "        with open(split_file, 'r') as f:\n",
        "            self.filenames = [line.strip() for line in f.readlines()]\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            filename = self.filenames[idx]\n",
        "            base_name = filename.split('.')[0]\n",
        "            rgb_path = os.path.join(self.rgb_dir, f'{base_name}.png')\n",
        "            rgb_img = cv2.imread(rgb_path)\n",
        "            rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)\n",
        "            rgb_img = Image.fromarray(rgb_img)\n",
        "            rgb_img = torch.from_numpy(np.array(rgb_img)).float() / 255.0\n",
        "            rgb_img = rgb_img.permute(2, 0, 1)\n",
        "            depth_path = os.path.join(self.depth_dir, f'{base_name}.dpt')\n",
        "            if not os.path.exists(depth_path):\n",
        "                raise FileNotFoundError(f\"Depth file not found: {depth_path}\")\n",
        "            depth_img = read_depth_dpt(depth_path)\n",
        "            if depth_img is None:\n",
        "                raise ValueError(f\"read_depth_dpt failed to read depth file: {depth_path}\")\n",
        "            depth_img = torch.from_numpy(depth_img).float() / 10000.0\n",
        "            depth_img = depth_img.unsqueeze(0)\n",
        "            pose_path = os.path.join(self.pose_dir, f'pose{base_name}.npy')\n",
        "            pose = np.load(pose_path)\n",
        "            if pose.shape == (3, 4):\n",
        "                rot = R.from_matrix(pose[:, :3]).as_quat()\n",
        "                trans = pose[:, 3]\n",
        "                pose = np.concatenate([trans, rot])\n",
        "            elif pose.shape == (4, 4):\n",
        "                rot = R.from_matrix(pose[:3, :3]).as_quat()\n",
        "                trans = pose[:3, 3]\n",
        "                pose = np.concatenate([trans, rot])\n",
        "            radius_maps = []\n",
        "            for pt_idx in range(1, self.num_radius_points + 1):\n",
        "                radius_folder = f\"Out_pt{pt_idx}_dm\"\n",
        "                radius_path = os.path.join(self.radius_base_dir, radius_folder, f'{base_name}.npy')\n",
        "                radius_map = np.load(radius_path)\n",
        "                radius_maps.append(radius_map)\n",
        "            radius_maps = np.array(radius_maps)\n",
        "            return {\n",
        "                'rgb': rgb_img,\n",
        "                'depth': depth_img,\n",
        "                'pose': torch.FloatTensor(pose),\n",
        "                'radius_maps': torch.FloatTensor(radius_maps)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Sample {self.filenames[idx]} skipped due to error: {e}\")\n",
        "\n",
        "# =========================\n",
        "# METRIC FUNCTIONS\n",
        "# =========================\n",
        "def translation_rmse(pred, gt):\n",
        "    return np.sqrt(np.mean((pred[:3] - gt[:3]) ** 2))\n",
        "\n",
        "def rotation_error(pred, gt):\n",
        "    pred_quat = pred[3:]\n",
        "    gt_quat = gt[3:]\n",
        "    pred_quat = pred_quat / np.linalg.norm(pred_quat)\n",
        "    gt_quat = gt_quat / np.linalg.norm(gt_quat)\n",
        "    diff = 1 - np.abs(np.dot(pred_quat, gt_quat))\n",
        "    return diff\n",
        "\n",
        "def add_metric(pred_pose, gt_pose, mesh_points, threshold=0.1):\n",
        "    if mesh_points is None:\n",
        "        return 0.0\n",
        "    pred_rot = R.from_quat(pred_pose[3:]).as_matrix()\n",
        "    gt_rot = R.from_quat(gt_pose[3:]).as_matrix()\n",
        "    pred_points = np.dot(mesh_points, pred_rot.T) + pred_pose[:3]\n",
        "    gt_points = np.dot(mesh_points, gt_rot.T) + gt_pose[:3]\n",
        "    mean_dist = np.mean(np.linalg.norm(pred_points - gt_points, axis=1))\n",
        "    return mean_dist\n",
        "\n",
        "def get_latest_model(models_dir):\n",
        "    model_files = glob.glob(os.path.join(models_dir, '*.pth'))\n",
        "    if not model_files:\n",
        "        raise FileNotFoundError(f\"No model files found in {models_dir}\")\n",
        "    return max(model_files, key=os.path.getctime)\n",
        "\n",
        "# =========================\n",
        "# TEST PIPELINE\n",
        "# =========================\n",
        "def test():\n",
        "    print(\"\\n================= RCVPose Test (Colab) =================\")\n",
        "    model_path = get_latest_model(CONFIG['MODELS_DIR'])\n",
        "    print(f\"Using latest model: {model_path}\")\n",
        "    model = EnhancedRCVPose().to(CONFIG['DEVICE'])\n",
        "    checkpoint = torch.load(model_path, map_location=CONFIG['DEVICE'])\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "    dataset = TestDataset(CONFIG['BASE_DIR'], CONFIG['OBJECT_ID'], CONFIG['NUM_RADIUS_POINTS'])\n",
        "    mesh_path = os.path.join(CONFIG['BASE_DIR'], CONFIG['OBJECT_ID'], CONFIG['MESH_PATH'])\n",
        "    mesh_points = None\n",
        "    try:\n",
        "        if os.path.exists(mesh_path):\n",
        "            import open3d as o3d\n",
        "            mesh_points = np.asarray(o3d.io.read_point_cloud(mesh_path).points)\n",
        "    except ImportError:\n",
        "        print(\"open3d is not installed. Please run: !pip install open3d\")\n",
        "    trans_errors = []\n",
        "    rot_errors = []\n",
        "    points_errors = []\n",
        "    add_errors = []\n",
        "    add_success = 0\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(len(dataset)), desc=\"Testing\"):\n",
        "            try:\n",
        "                batch = dataset[i]\n",
        "            except RuntimeError as e:\n",
        "                print(e)\n",
        "                continue\n",
        "            rgb = batch['rgb'].unsqueeze(0).to(CONFIG['DEVICE'])\n",
        "            depth = batch['depth'].unsqueeze(0).to(CONFIG['DEVICE'])\n",
        "            pose_gt = batch['pose'].unsqueeze(0).to(CONFIG['DEVICE'])\n",
        "            radius_maps_gt = batch['radius_maps'].unsqueeze(0).to(CONFIG['DEVICE'])\n",
        "            pose_pred, radius_maps_pred = model(rgb, depth)\n",
        "            pose_pred = pose_pred.cpu().numpy()\n",
        "            pose_gt = pose_gt.cpu().numpy()\n",
        "            radius_maps_pred = radius_maps_pred.cpu().numpy()\n",
        "            radius_maps_gt = radius_maps_gt.cpu().numpy()\n",
        "            for j in range(len(pose_pred)):\n",
        "                trans_error = translation_rmse(pose_pred[j], pose_gt[j])\n",
        "                trans_errors.append(trans_error)\n",
        "                rot_error = rotation_error(pose_pred[j], pose_gt[j])\n",
        "                rot_errors.append(rot_error)\n",
        "                points_error = np.mean((radius_maps_pred[j] - radius_maps_gt[j]) ** 2)\n",
        "                points_errors.append(points_error)\n",
        "                if mesh_points is not None:\n",
        "                    add_error = add_metric(pose_pred[j], pose_gt[j], mesh_points)\n",
        "                    add_errors.append(add_error)\n",
        "                    if add_error < 0.1:\n",
        "                        add_success += 1\n",
        "    print(\"\\nTest Results:\")\n",
        "    print(f\"Translation RMSE: {np.mean(trans_errors):.4f} mm\")\n",
        "    print(f\"Rotation Error: {np.mean(rot_errors):.4f}\")\n",
        "    print(f\"Points MSE: {np.mean(points_errors):.4f}\")\n",
        "    if mesh_points is not None:\n",
        "        print(f\"ADD Metric: {np.mean(add_errors):.4f}\")\n",
        "        print(f\"ADD Success Rate: {add_success/len(trans_errors)*100:.2f}%\")\n",
        "    return {\n",
        "        'trans_rmse': np.mean(trans_errors),\n",
        "        'rot_error': np.mean(rot_errors),\n",
        "        'points_mse': np.mean(points_errors),\n",
        "        'add_metric': np.mean(add_errors) if mesh_points is not None else None,\n",
        "        'add_success_rate': add_success/len(trans_errors)*100 if mesh_points is not None else None\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4Mhrqi_bu5q",
        "outputId": "143192de-04ce-4d6c-8595-d51bd1ce9854"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================= RCVPose Test (Colab) =================\n",
            "Using latest model: /content/models/best_model_20250529_150327.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 124/124 [00:10<00:00, 11.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Results:\n",
            "Translation RMSE: 0.0758 mm\n",
            "Rotation Error: 0.2034\n",
            "Points MSE: 0.0018\n",
            "ADD Metric: 26.4648\n",
            "ADD Success Rate: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yar8tY6XPh1"
      },
      "source": [
        "# **All data are in millimeter scale.**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}